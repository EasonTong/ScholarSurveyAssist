{"data": [{"arxiv": {"page": "https://arxiv.org/abs/2202.08926", "id": "2202.08926", "pdf": "https://arxiv.org/pdf/2202.08926", "other": "https://arxiv.org/format/2202.08926"}, "title": "On Guiding Visual Attention with Language Specification", "author_info": ["Suzanne Petryk", "Lisa Dunlap", "Keyan Nasseri", "Joseph Gonzalez", "Trevor Darrell", "Anna Rohrbach"], "summary": "While real world challenges typically define visual categories with language words or phrases, most visual classification methods define categories with numerical indices. However, the language specification of the classes provides an especially useful prior for biased and noisy datasets, where it can help disambiguate what features are task-relevant. Recently, large-scale multimodal models have been shown to recognize a wide variety of high-level concepts from a language specification even without additional image training data, but they are often unable to distinguish classes for more fine-grained tasks. CNNs, in contrast, can extract subtle image features that are required for fine-grained discrimination, but will overfit to any bias or noise in datasets. Our insight is to use high-level language specification as advice for constraining the classification evidence to task-relevant features, instead of distractors. To do this, we ground task-relevant words or phrases with attention maps from a pretrained large-scale model. We then use this grounding to supervise a classifier's spatial attention away from distracting context. We show that supervising spatial attention in this way improves performance on classification tasks with biased and noisy data, including about 3-15% worst-group accuracy improvements and 41-45% relative improvements on fairness metrics.", "comment": " Comments: 14 pages, 9 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03543", "id": "2202.03543", "pdf": "https://arxiv.org/pdf/2202.03543", "other": "https://arxiv.org/format/2202.03543"}, "title": "Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling", "author_info": ["Puyuan Peng", "David Harwath"], "summary": "In this paper, we describe our submissions to the ZeroSpeech 2021 Challenge and SUPERB benchmark. Our submissions are based on the recently proposed FaST-VGS model, which is a Transformer-based model that learns to associate raw speech waveforms with semantically related images, all without the use of any transcriptions of the speech. Additionally, we introduce a novel extension of this model, FaST-VGS+, which is learned in a multi-task fashion with a masked language modeling objective in addition to the visual grounding objective. On ZeroSpeech 2021, we show that our models perform competitively on the ABX task, outperform all other concurrent submissions on the Syntactic and Semantic tasks, and nearly match the best system on the Lexical task. On the SUPERB benchmark, we show that our models also achieve strong performance, in some cases even outperforming the popular wav2vec2.0 model.", "comment": " Comments: SAS workshop at AAAI2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.12723", "id": "2201.12723", "pdf": "https://arxiv.org/pdf/2201.12723", "other": "https://arxiv.org/format/2201.12723"}, "title": "VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training", "author_info": ["Ziyang Luo", "Yadong Xi", "Rongsheng Zhang", "Jing Ma"], "summary": "Vision-and-language pre-trained models (VLMs) have achieved tremendous success in the cross-modal area, but most of them require a large amount of parallel image-caption data for pre-training. Collating such data is expensive and labor-intensive. In this work, we focus on reducing such need for generative vision-and-language pre-training (G-VLP) by taking advantage of the visual pre-trained model (CLIP-ViT) as encoder and language pre-trained model (GPT2) as decoder. Unfortunately, GPT2 lacks a necessary cross-attention module, which hinders the direct connection of CLIP-ViT and GPT2. To remedy such defects, we conduct extensive experiments to empirically investigate how to design and pre-train our model. Based on our experimental results, we propose a novel G-VLP framework, Visual Conditioned GPT (VC-GPT), and pre-train it with a small-scale image-caption corpus (Visual Genome, only 110k distinct images). Evaluating on the image captioning downstream tasks (MSCOCO and Flickr30k Captioning), VC-GPT achieves either the best or the second-best performance across all evaluation metrics over the previous works which consume around 30 times more distinct images during cross-modal pre-training.", "comment": " Comments: Work in progress "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11114", "id": "2201.11114", "pdf": "https://arxiv.org/pdf/2201.11114", "other": "https://arxiv.org/format/2201.11114"}, "title": "Natural Language Descriptions of Deep Visual Features", "author_info": ["Evan Hernandez", "Sarah Schwettmann", "David Bau", "Teona Bagashvili", "Antonio Torralba", "Jacob Andreas"], "summary": "Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual-information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to protected categories like race and gender in models trained on datasets intended to obscure these features. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels.", "comment": " Comments: To be published as a conference paper at ICLR 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.06742", "id": "2201.06742", "pdf": "https://arxiv.org/pdf/2201.06742", "other": "https://arxiv.org/format/2201.06742"}, "title": "Scalable Vega: Optimizing Declarative Visualization Languages", "author_info": ["Junran Yang", "Hyekang Kevin Joo", "Sai Yerramreddy", "Siyao Li", "Dominik Moritz", "Leilani Battle"], "summary": "While many visualization specification languages are user-friendly, they tend to have one critical drawback: they are designed for small data on the client-side and, as a result, perform poorly at scale. We propose a system that takes declarative visualization specifications as input and automatically optimizes the resulting visualization execution plans by offloading computational-intensive operations to a separate database management system (DBMS). Our demo will enable users to write their own Vega specifications (or import existing ones), view the optimized plans from our system, and even modify these plans and compare their performance via a dedicated performance dashboard.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.05299", "id": "2201.05299", "pdf": "https://arxiv.org/pdf/2201.05299", "other": "https://arxiv.org/format/2201.05299"}, "title": "A Thousand Words Are Worth More Than a Picture: Natural Language-Centric Outside-Knowledge Visual Question Answering", "author_info": ["Feng Gao", "Qing Ping", "Govind Thattai", "Aishwarya Reganti", "Ying Nian Wu", "Prem Natarajan"], "summary": "Outside-knowledge visual question answering (OK-VQA) requires the agent to comprehend the image, make use of relevant knowledge from the entire web, and digest all the information to answer the question. Most previous works address the problem by first fusing the image and question in the multi-modal space, which is inflexible for further fusion with a vast amount of external knowledge. In this paper, we call for a paradigm shift for the OK-VQA task, which transforms the image into plain text, so that we can enable knowledge passage retrieval, and generative question-answering in the natural language space. This paradigm takes advantage of the sheer volume of gigantic knowledge bases and the richness of pre-trained language models. A Transform-Retrieve-Generate framework (TRiG) framework is proposed, which can be plug-and-played with alternative image-to-text models and textual knowledge bases. Experimental results show that our TRiG framework outperforms all state-of-the-art supervised methods by at least 11.1% absolute margin.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.01819", "id": "2201.01819", "pdf": "https://arxiv.org/pdf/2201.01819", "other": "https://arxiv.org/format/2201.01819"}, "title": "Formal Analysis of Art: Proxy Learning of Visual Concepts from Style Through Language Models", "author_info": ["Diana Kim", "Ahmed Elgammal", "Marian Mazzone"], "summary": "We present a machine learning system that can quantify fine art paintings with a set of visual elements and principles of art. This formal analysis is fundamental for understanding art, but developing such a system is challenging. Paintings have high visual complexities, but it is also difficult to collect enough training data with direct labels. To resolve these practical limitations, we introduce a novel mechanism, called proxy learning, which learns visual concepts in paintings though their general relation to styles. This framework does not require any visual annotation, but only uses style labels and a general relationship between visual concepts and style. In this paper, we propose a novel proxy model and reformulate four pre-existing methods in the context of proxy learning. Through quantitative and qualitative comparison, we evaluate these methods and compare their effectiveness in quantifying the artistic visual concepts, where the general relationship is estimated by language models; GloVe or BERT. The language modeling is a practical and scalable solution requiring no labeling, but it is inevitably imperfect. We demonstrate how the new proxy model is robust to the imperfection, while the other models are sensitively affected by it.", "comment": " ACM Class:           I.2.6; I.2.7; I.2.10; J.5                "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.13985", "id": "2112.13985", "pdf": "https://arxiv.org/pdf/2112.13985", "other": "https://arxiv.org/format/2112.13985"}, "title": "LatteGAN: Visually Guided Language Attention for Multi-Turn Text-Conditioned Image Manipulation", "author_info": ["Shoya Matsumori", "Yuki Abe", "Kosuke Shingyouchi", "Komei Sugiura", "Michita Imai"], "summary": "Text-guided image manipulation tasks have recently gained attention in the vision-and-language community. While most of the prior studies focused on single-turn manipulation, our goal in this paper is to address the more challenging multi-turn image manipulation (MTIM) task. Previous models for this task successfully generate images iteratively, given a sequence of instructions and a previously generated image. However, this approach suffers from under-generation and a lack of generated quality of the objects that are described in the instructions, which consequently degrades the overall performance. To overcome these problems, we present a novel architecture called a Visually Guided Language Attention GAN (LatteGAN). Here, we address the limitations of the previous approaches by introducing a Visually Guided Language Attention (Latte) module, which extracts fine-grained text representations for the generator, and a Text-Conditioned U-Net discriminator architecture, which discriminates both the global and local representations of fake or real images. Extensive experiments on two distinct MTIM datasets, CoDraw and i-CLEVR, demonstrate the state-of-the-art performance of the proposed model.", "comment": " Journal ref:         IEEE Access, 9, 160521-160532 (2021)       "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.12926", "id": "2112.12926", "pdf": "https://arxiv.org/pdf/2112.12926", "other": "https://arxiv.org/format/2112.12926"}, "title": "nvBench: A Large-Scale Synthesized Dataset for Cross-Domain Natural Language to Visualization Task", "author_info": ["Yuyu Luo", "Jiawei Tang", "Guoliang Li"], "summary": "NL2VIS - which translates natural language (NL) queries to corresponding visualizations (VIS) - has attracted more and more attention both in commercial visualization vendors and academic researchers. In the last few years, the advanced deep learning-based models have achieved human-like abilities in many natural language processing (NLP) tasks, which clearly tells us that the deep learning-based technique is a good choice to push the field of NL2VIS. However, a big balk is the lack of benchmarks with lots of (NL, VIS) pairs. We present nvBench, the first large-scale NL2VIS benchmark, containing 25,750 (NL, VIS) pairs from 750 tables over 105 domains, synthesized from (NL, SQL) benchmarks to support cross-domain NL2VIS task. The quality of nvBench has been extensively validated by 23 experts and 300+ crowd workers. Deep learning-based models training using nvBench demonstrate that nvBench can push the field of NL2VIS.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.11691", "id": "2112.11691", "pdf": "https://arxiv.org/pdf/2112.11691", "other": "https://arxiv.org/format/2112.11691"}, "title": "CLEVR3D: Compositional Language and Elementary Visual Reasoning for Question Answering in 3D Real-World Scenes", "author_info": ["Xu Yan", "Zhihao Yuan", "Yuhao Du", "Yinghong Liao", "Yao Guo", "Zhen Li", "Shuguang Cui"], "summary": "3D scene understanding is a relatively emerging research field. In this paper, we introduce the Visual Question Answering task in 3D real-world scenes (VQA-3D), which aims to answer all possible questions given a 3D scene. To tackle this problem, the first VQA-3D dataset, namely CLEVR3D, is proposed, which contains 60K questions in 1,129 real-world scenes. Specifically, we develop a question engine leveraging 3D scene graph structures to generate diverse reasoning questions, covering the questions of objects' attributes (i.e., size, color, and material) and their spatial relationships. Built upon this dataset, we further design the first VQA-3D baseline model, TransVQA3D. The TransVQA3D model adopts well-designed Transformer architectures to achieve superior VQA-3D performance, compared with the pure language baseline and previous 3D reasoning methods directly applied to 3D scenarios. Experimental results verify that taking VQA-3D as an auxiliary task can boost the performance of 3D scene understanding, including scene graph analysis for the node-wise classification and whole-graph recognition.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.04478", "id": "2112.04478", "pdf": "https://arxiv.org/pdf/2112.04478", "other": "https://arxiv.org/format/2112.04478"}, "title": "Prompting Visual-Language Models for Efficient Video Understanding", "author_info": ["Chen Ju", "Tengda Han", "Kunhao Zheng", "Ya Zhang", "Weidi Xie"], "summary": "Visual-language pre-training has shown great success for learning joint visual-textual representations from large-scale web data, demonstrating remarkable ability for zero-shot generalisation. This paper presents a simple method to efficiently adapt one pre-trained visual-language model to novel tasks with minimal training, and here, we consider video understanding tasks. Specifically, we propose to optimise a few random vectors, termed as continuous prompt vectors, that convert the novel tasks into the same format as the pre-training objectives. In addition, to bridge the gap between static images and videos, temporal information is encoded with lightweight Transformers stacking on top of frame-wise visual features. Experimentally, we conduct extensive ablation studies to analyse the critical components and necessities. On 9 public benchmarks of action recognition, action localisation, and text-video retrieval, across closed-set, few-shot, open-set scenarios, we achieve competitive or state-of-the-art performance to existing methods, despite training significantly fewer parameters.", "comment": " Comments: Project page: https://ju-chen.github.io/efficient-prompt/ "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.02399", "id": "2112.02399", "pdf": "https://arxiv.org/pdf/2112.02399", "other": "https://arxiv.org/format/2112.02399"}, "title": "VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts", "author_info": ["Renrui Zhang", "Longtian Qiu", "Wei Zhang", "Ziyao Zeng"], "summary": "Contrastive Vision-Language Pre-training (CLIP) has drown increasing attention recently for its transferable visual representation learning. Supervised by large-scale image-text pairs, CLIP is able to align paired images and texts and thus conduct zero-shot recognition in open-vocabulary scenarios. However, there exists semantic gap between the specific application and generally pre-trained knowledge, which makes the matching sub-optimal on downstream tasks. In this paper, we propose VT-CLIP to enhance vision-language modeling via visual-guided texts. Specifically, we guide the text feature to adaptively explore informative regions on the image and aggregate the visual feature by cross-attention machanism. In this way, the visual-guided text become more semantically correlated with the image, which greatly benefits the matching process. In few-shot settings, we evaluate our VT-CLIP on 11 well-known classification datasets and experiment extensive ablation studies to demonstrate the effectiveness of VT-CLIP. The code will be released soon.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.13654", "id": "2111.13654", "pdf": "https://arxiv.org/pdf/2111.13654", "other": "https://arxiv.org/format/2111.13654"}, "title": "Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs", "author_info": ["Peter Hase", "Mona Diab", "Asli Celikyilmaz", "Xian Li", "Zornitsa Kozareva", "Veselin Stoyanov", "Mohit Bansal", "Srinivasan Iyer"], "summary": "Do language models have beliefs about the world? Dennett (1995) famously argues that even thermostats have beliefs, on the view that a belief is simply an informational state decoupled from any motivational state. In this paper, we discuss approaches to detecting when models have beliefs about the world, and we improve on methods for updating model beliefs to be more truthful, with a focus on methods based on learned optimizers or hypernetworks. Our main contributions include: (1) new metrics for evaluating belief-updating methods that focus on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing model updates (SLAG) that improves the performance of learned optimizers, and (3) the introduction of the belief graph, which is a new form of interface with language models that shows the interdependencies between model beliefs. Our experiments suggest that models possess belief-like qualities to only a limited extent, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work. Code is available at https://github.com/peterbhase/SLAG-Belief-Updating", "comment": " Comments: 19 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.12681", "id": "2111.12681", "pdf": "https://arxiv.org/pdf/2111.12681", "other": "https://arxiv.org/format/2111.12681"}, "title": "VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling", "author_info": ["Tsu-Jui Fu", "Linjie Li", "Zhe Gan", "Kevin Lin", "William Yang Wang", "Lijuan Wang", "Zicheng Liu"], "summary": "A great challenge in video-language (VidL) modeling lies in the disconnection between fixed video representations extracted from image/video understanding models and downstream VidL data. Recent studies try to mitigate this disconnection via end-to-end training. To make it computationally feasible, prior works tend to \"imagify\" video inputs, i.e., a handful of sparsely sampled frames are fed into a 2D CNN, followed by a simple mean-pooling or concatenation to obtain the overall video representations. Although achieving promising results, such simple approaches may lose temporal information that is essential for performing downstream VidL tasks. In this work, we present VIOLET, a fully end-to-end VIdeO-LanguagE Transformer, which adopts a video transformer to explicitly model the temporal dynamics of video inputs. Further, unlike previous studies that found pre-training tasks on video inputs (e.g., masked frame modeling) not very effective, we design a new pre-training task, Masked Visual-token Modeling (MVM), for better video modeling. Specifically, the original video frame patches are \"tokenized\" into discrete visual tokens, and the goal is to recover the original visual tokens based on the masked patches. Comprehensive analysis demonstrates the effectiveness of both explicit temporal modeling via video transformer and MVM. As a result, VIOLET achieves new state-of-the-art performance on 5 video question answering tasks and 4 text-to-video retrieval tasks.", "comment": " Comments: Code is available at https://github.com/tsujuifu/pytorch_violet "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.08531", "id": "2111.08531", "pdf": "https://arxiv.org/pdf/2111.08531", "other": "https://arxiv.org/format/2111.08531"}, "title": "Language bias in Visual Question Answering: A Survey and Taxonomy", "author_info": ["Desen Yuan"], "summary": "Visual question answering (VQA) is a challenging task, which has attracted more and more attention in the field of computer vision and natural language processing. However, the current visual question answering has the problem of language bias, which reduces the robustness of the model and has an adverse impact on the practical application of visual question answering. In this paper, we conduct a comprehensive review and analysis of this field for the first time, and classify the existing methods according to three categories, including enhancing visual information, weakening language priors, data enhancement and training strategies. At the same time, the relevant representative methods are introduced, summarized and analyzed in turn. The causes of language bias are revealed and classified. Secondly, this paper introduces the datasets mainly used for testing, and reports the experimental results of various existing methods. Finally, we discuss the possible future research directions in this field.", "comment": " Comments: 10 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.08276", "id": "2111.08276", "pdf": "https://arxiv.org/pdf/2111.08276", "other": "https://arxiv.org/format/2111.08276"}, "title": "Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts", "author_info": ["Yan Zeng", "Xinsong Zhang", "Hang Li"], "summary": "Most existing methods in vision language pre-training rely on object-centric features extracted through object detection, and make fine-grained alignments between the extracted features and texts. We argue that object detection may not be necessary for vision language pre-training. To this end, we propose a new method called X-VLM to perform `multi-grained vision language pre-training.' The key of learning multi-grained alignments is to locate visual concepts in the image given the associated texts, and in the meantime align the texts with the visual concepts, where the alignments are in multi-granularity. Experimental results show that X-VLM effectively leverages the learned alignments to many downstream vision language tasks and consistently outperforms state-of-the-art methods.", "comment": " Comments: 15 pages, 5 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.01582", "id": "2111.01582", "pdf": "https://arxiv.org/pdf/2111.01582", "other": "https://arxiv.org/format/2111.01582"}, "title": "LMdiff: A Visual Diff Tool to Compare Language Models", "author_info": ["Hendrik Strobelt", "Benjamin Hoover", "Arvind Satyanarayan", "Sebastian Gehrmann"], "summary": "While different language models are ubiquitous in NLP, it is hard to contrast their outputs and identify which contexts one can handle better than the other. To address this question, we introduce LMdiff, a tool that visually compares probability distributions of two models that differ, e.g., through finetuning, distillation, or simply training with different parameter sizes. LMdiff allows the generation of hypotheses about model behavior by investigating text instances token by token and further assists in choosing these interesting text instances by identifying the most interesting phrases from large corpora. We showcase the applicability of LMdiff for hypothesis generation across multiple case studies. A demo is available at http://lmdiff.net .", "comment": " Comments: EMNLP 2021 Demo Paper "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.15358", "id": "2110.15358", "pdf": "https://arxiv.org/pdf/2110.15358", "other": "https://arxiv.org/format/2110.15358"}, "title": "Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language", "author_info": ["Mingyu Ding", "Zhenfang Chen", "Tao Du", "Ping Luo", "Joshua B. Tenenbaum", "Chuang Gan"], "summary": "In this work, we propose a unified framework, called Visual Reasoning with Differ-entiable Physics (VRDP), that can jointly learn visual concepts and infer physics models of objects and their interactions from videos and language. This is achieved by seamlessly integrating three components: a visual perception module, a concept learner, and a differentiable physics engine. The visual perception module parses each video frame into object-centric trajectories and represents them as latent scene representations. The concept learner grounds visual concepts (e.g., color, shape, and material) from these object-centric representations based on the language, thus providing prior knowledge for the physics engine. The differentiable physics model, implemented as an impulse-based differentiable rigid-body simulator, performs differentiable physical simulation based on the grounded concepts to infer physical properties, such as mass, restitution, and velocity, by fitting the simulated trajectories into the video observations. Consequently, these learned concepts and physical models can explain what we have seen and imagine what is about to happen in future and counterfactual scenarios. Integrating differentiable physics into the dynamic reasoning framework offers several appealing benefits. More accurate dynamics prediction in learned physics models enables state-of-the-art performance on both synthetic and real-world benchmarks while still maintaining high transparency and interpretability; most notably, VRDP improves the accuracy of predictive and counterfactual questions by 4.5% and 11.5% compared to its best counterpart. VRDP is also highly data-efficient: physical parameters can be optimized from very few videos, and even a single video can be sufficient. Finally, with all physical parameters inferred, VRDP can quickly learn new concepts from a few examples.", "comment": " Comments: NeurIPS 2021. Project page: http://vrdp.csail.mit.edu/ "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.13214", "id": "2110.13214", "pdf": "https://arxiv.org/pdf/2110.13214", "other": "https://arxiv.org/format/2110.13214"}, "title": "IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning", "author_info": ["Pan Lu", "Liang Qiu", "Jiaqi Chen", "Tony Xia", "Yizhou Zhao", "Wei Zhang", "Zhou Yu", "Xiaodan Liang", "Song-Chun Zhu"], "summary": "Current visual question answering (VQA) tasks mainly consider answering human-annotated questions for natural images. However, aside from natural images, abstract diagrams with semantic richness are still understudied in visual understanding and reasoning research. In this work, we introduce a new challenge of Icon Question Answering (IconQA) with the goal of answering a question in an icon image context. We release IconQA, a large-scale dataset that consists of 107,439 questions and three sub-tasks: multi-image-choice, multi-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by real-world diagram word problems that highlight the importance of abstract diagram understanding and comprehensive cognitive reasoning. Thus, IconQA requires not only perception skills like object recognition and text understanding, but also diverse cognitive reasoning skills, such as geometric reasoning, commonsense reasoning, and arithmetic reasoning. To facilitate potential IconQA models to learn semantic representations for icon images, we further release an icon dataset Icon645 which contains 645,687 colored icons on 377 classes. We conduct extensive user studies and blind experiments and reproduce a wide range of advanced VQA methods to benchmark the IconQA task. Also, we develop a strong IconQA baseline Patch-TRM that applies a pyramid cross-modal Transformer with input diagram embeddings pre-trained on the icon dataset. IconQA and Icon645 are available at https://iconqa.github.io.", "comment": " Comments: Accepted to NeurIPS 2021 Track on Datasets and Benchmarks, 27 pages, 18 figures. Data and code are available at https://iconqa.github.io "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.08797", "id": "2110.08797", "pdf": "https://arxiv.org/pdf/2110.08797", "other": "https://arxiv.org/format/2110.08797"}, "title": "Towards Language-guided Visual Recognition via Dynamic Convolutions", "author_info": ["Gen Luo", "Yiyi Zhou", "Xiaoshuai Sun", "Xinghao Ding", "Yongjian Wu", "Feiyue Huang", "Yue Gao", "Rongrong Ji"], "summary": "In this paper, we are committed to establishing an unified and end-to-end multi-modal network via exploring the language-guided visual recognition. To approach this target, we first propose a novel multi-modal convolution module called Language-dependent Convolution (LaConv). Its convolution kernels are dynamically generated based on natural language information, which can help extract differentiated visual features for different multi-modal examples. Based on the LaConv module, we further build the first fully language-driven convolution network, termed as LaConvNet, which can unify the visual recognition and multi-modal reasoning in one forward structure. To validate LaConv and LaConvNet, we conduct extensive experiments on four benchmark datasets of two vision-and-language tasks, i.e., visual question answering (VQA) and referring expression comprehension (REC). The experimental results not only shows the performance gains of LaConv compared to the existing multi-modal modules, but also witness the merits of LaConvNet as an unified network, including compact network, high generalization ability and excellent performance, e.g., +4.7% on RefCOCO+.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.08518", "id": "2110.08518", "pdf": "https://arxiv.org/pdf/2110.08518", "other": "https://arxiv.org/format/2110.08518"}, "title": "MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding", "author_info": ["Junlong Li", "Yiheng Xu", "Lei Cui", "Furu Wei"], "summary": "Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document Understanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a large number of digital documents where the layout information is not fixed and needs to be interactively and dynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this paper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as HTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the pre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding tasks. The pre-trained model and code will be publicly available at https://aka.ms/markuplm.", "comment": " Comments: Work in Progress "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.04406", "id": "2110.04406", "pdf": "https://arxiv.org/pdf/2110.04406", "other": "https://arxiv.org/format/2110.04406"}, "title": "Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content", "author_info": ["Alan Lundgard", "Arvind Satyanarayan"], "summary": "Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization.", "comment": " Journal ref:         IEEE Trans. Visualization & Comp. Graphics (Proc. InfoVis), 2022       "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.00680", "id": "2110.00680", "pdf": "https://arxiv.org/pdf/2110.00680", "other": "https://arxiv.org/format/2110.00680"}, "title": "Collecting and Characterizing Natural Language Utterances for Specifying Data Visualizations", "author_info": ["Arjun Srinivasan", "Nikhila Nyapathy", "Bongshin Lee", "Steven M. Drucker", "John Stasko"], "summary": "Natural language interfaces (NLIs) for data visualization are becoming increasingly popular both in academic research and in commercial software. Yet, there is a lack of empirical understanding of how people specify visualizations through natural language. To bridge this gap, we conducted an online study with 102 participants. We showed participants a series of ten visualizations for a given dataset and asked them to provide utterances they would pose to generate the displayed charts. The curated list of utterances generated from the study is provided below. This corpus of utterances can be used to evaluate existing NLIs for data visualization as well as for creating new systems and models to generate visualizations from natural language utterances.", "comment": " Comments: Paper appeared at the 2021 ACM Conference on Conference on Human Factors in Computing Systems (CHI 2021), 10 pages (5 figures, 3 tables) "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.13238", "id": "2109.13238", "pdf": "https://arxiv.org/pdf/2109.13238"}, "title": "Visually Grounded Reasoning across Languages and Cultures", "author_info": ["Fangyu Liu", "Emanuele Bugliarello", "Edoardo Maria Ponti", "Siva Reddy", "Nigel Collier", "Desmond Elliott"], "summary": "The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for {M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems.", "comment": " Comments: EMNLP 2021; Fangyu and Emanuele contributed equally; MaRVL website: https://marvl-challenge.github.io "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.08029", "id": "2109.08029", "pdf": "https://arxiv.org/pdf/2109.08029", "other": "https://arxiv.org/format/2109.08029"}, "title": "Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering", "author_info": ["Ander Salaberria", "Gorka Azkune", "Oier Lopez de Lacalle", "Aitor Soroa", "Eneko Agirre"], "summary": "Integrating outside knowledge for reasoning in visio-linguistic tasks such as visual question answering (VQA) is an open problem. Given that pretrained language models have been shown to include world knowledge, we propose to use a unimodal (text-only) train and inference procedure based on automatic off-the-shelf captioning of images and pretrained language models. Our results on a visual question answering task which requires external knowledge (OK-VQA) show that our text-only model outperforms pretrained multimodal (image-text) models of comparable number of parameters. In contrast, our model is less effective in a standard VQA task (VQA 2.0) confirming that our text-only method is specially effective for tasks requiring external knowledge. In addition, we show that our unimodal model is complementary to multimodal models in both OK-VQA and VQA 2.0, and yield the best result to date in OK-VQA among systems not using external knowledge graphs, and comparable to systems that do use them. Our qualitative analysis on OK-VQA reveals that automatic captions often fail to capture relevant information in the images, which seems to be balanced by the better inference ability of the text-only language models. Our work opens up possibilities to further improve inference in visio-linguistic tasks.", "comment": " Comments: Under review. 11 pages with 3 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.03506", "id": "2109.03506", "pdf": "https://arxiv.org/pdf/2109.03506", "other": "https://arxiv.org/format/2109.03506"}, "title": "Towards Natural Language Interfaces for Data Visualization: A Survey", "author_info": ["Leixian Shen", "Enya Shen", "Yuyu Luo", "Xiaocong Yang", "Xuming Hu", "Xiongshuai Zhang", "Zhiwei Tai", "Jianmin Wang"], "summary": "Utilizing Visualization-oriented Natural Language Interfaces (V-NLI) as a complementary input modality to direct manipulation for visual analytics can provide an engaging user experience. It enables users to focus on their tasks rather than having to worry about how to operate visualization tools on the interface. In the past two decades, leveraging advanced natural language processing technologies, numerous V-NLI systems have been developed in academic research and commercial software, especially in recent years. In this article, we conduct a comprehensive review of the existing V-NLIs. In order to classify each paper, we develop categorical dimensions based on a classic information visualization pipeline with the extension of a V-NLI layer. The following seven stages are used: query interpretation, data transformation, visual mapping, view transformation, human interaction, dialogue management, and presentation. Finally, we also shed light on several promising directions for future work in the V-NLI community.", "comment": " Comments: 20 pages, 15 figures, accepted by IEEE TVCG "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.11544", "id": "2108.11544", "pdf": "https://arxiv.org/pdf/2108.11544", "other": "https://arxiv.org/format/2108.11544"}, "title": "Visual-and-Language Navigation: A Survey and Taxonomy", "author_info": ["Wansen Wu", "Tao Chang", "Xinmeng Li"], "summary": "An agent that can understand natural-language instruction and carry out corresponding actions in the visual world is one of the long-term challenges of Artificial Intelligent (AI). Due to multifarious instructions from humans, it requires the agent can link natural language to vision and action in unstructured, previously unseen environments. If the instruction given by human is a navigation task, this challenge is called Visual-and-Language Navigation (VLN). It is a booming multi-disciplinary field of increasing importance and with extraordinary practicality. Instead of focusing on the details of specific methods, this paper provides a comprehensive survey on VLN tasks and makes a classification carefully according the different characteristics of language instructions in these tasks. According to when the instructions are given, the tasks can be divided into single-turn and multi-turn. For single-turn tasks, we further divided them into goal-orientation and route-orientation based on whether the instructions contain a route. For multi-turn tasks, we divided them into imperative task and interactive task based on whether the agent responses to the instructions. This taxonomy enable researchers to better grasp the key point of a specific task and identify directions for future research.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2108.10904", "id": "2108.10904", "pdf": "https://arxiv.org/pdf/2108.10904", "other": "https://arxiv.org/format/2108.10904"}, "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision", "author_info": ["Zirui Wang", "Jiahui Yu", "Adams Wei Yu", "Zihang Dai", "Yulia Tsvetkov", "Yuan Cao"], "summary": "With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE (+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2108.09661", "id": "2108.09661", "pdf": "https://arxiv.org/pdf/2108.09661", "other": "https://arxiv.org/format/2108.09661"}, "title": "From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network", "author_info": ["Yuxin Wang", "Hongtao Xie", "Shancheng Fang", "Jing Wang", "Shenggao Zhu", "Yongdong Zhang"], "summary": "In this paper, we abandon the dominant complex language model and rethink the linguistic learning process in the scene text recognition. Different from previous methods considering the visual and linguistic information in two separate structures, we propose a Visual Language Modeling Network (VisionLAN), which views the visual and linguistic information as a union by directly enduing the vision model with language capability. Specially, we introduce the text recognition of character-wise occluded feature maps in the training stage. Such operation guides the vision model to use not only the visual texture of characters, but also the linguistic information in visual context for recognition when the visual cues are confused (e.g. occlusion, noise, etc.). As the linguistic information is acquired along with visual features without the need of extra language model, VisionLAN significantly improves the speed by 39% and adaptively considers the linguistic information to enhance the visual features for accurate recognition. Furthermore, an Occlusion Scene Text (OST) dataset is proposed to evaluate the performance on the case of missing character-wise visual cues. The state of-the-art results on several benchmarks prove our effectiveness. Code and dataset are available at https://github.com/wangyuxin87/VisionLAN.", "comment": " Comments: Accept by ICCV2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.05067", "id": "2108.05067", "pdf": "https://arxiv.org/pdf/2108.05067"}, "title": "Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning", "author_info": ["Guangyi Liu", "Yinghong Liao", "Fuyu Wang", "Bin Zhang", "Lu Zhang", "Xiaodan Liang", "Xiang Wan", "Shaolin Li", "Zhen Li", "Shuixing Zhang", "Shuguang Cui"], "summary": "Medical imaging technologies, including computed tomography (CT) or chest X-Ray (CXR), are largely employed to facilitate the diagnosis of the COVID-19. Since manual report writing is usually too time-consuming, a more intelligent auxiliary medical system that could generate medical reports automatically and immediately is urgently needed. In this article, we propose to use the medical visual language BERT (Medical-VLBERT) model to identify the abnormality on the COVID-19 scans and generate the medical report automatically based on the detected lesion regions. To produce more accurate medical reports and minimize the visual-and-linguistic differences, this model adopts an alternate learning strategy with two procedures that are knowledge pretraining and transferring. To be more precise, the knowledge pretraining procedure is to memorize the knowledge from medical texts, while the transferring procedure is to utilize the acquired knowledge for professional medical sentences generations through observations of medical images. In practice, for automatic medical report generation on the COVID-19 cases, we constructed a dataset of 368 medical findings in Chinese and 1104 chest CT scans from The First Affiliated Hospital of Jinan University, Guangzhou, China, and The Fifth Affiliated Hospital of Sun Yat-sen University, Zhuhai, China. Besides, to alleviate the insufficiency of the COVID-19 training samples, our model was first trained on the large-scale Chinese CX-CHR dataset and then transferred to the COVID-19 CT dataset for further fine-tuning. The experimental results showed that Medical-VLBERT achieved state-of-the-art performances on terminology prediction and report generation with the Chinese COVID-19 CT dataset and the CX-CHR dataset. The Chinese COVID-19 CT dataset is available at https://covid19ct.github.io/.", "comment": " Comments: Accepted by IEEE Transactions on Neural Networks and Learning Systems "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.04927", "id": "2108.04927", "pdf": "https://arxiv.org/pdf/2108.04927", "other": "https://arxiv.org/format/2108.04927"}, "title": "Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion", "author_info": ["Alessandro Suglia", "Qiaozi Gao", "Jesse Thomason", "Govind Thattai", "Gaurav Sukhatme"], "summary": "Language-guided robots performing home and office tasks must navigate in and interact with the world. Grounding language instructions against visual observations and actions to take in an environment is an open challenge. We present Embodied BERT (EmBERT), a transformer-based model which can attend to high-dimensional, multi-modal inputs across long temporal horizons for language-conditioned task completion. Additionally, we bridge the gap between successful object-centric navigation models used for non-interactive agents and the language-guided visual task completion benchmark, ALFRED, by introducing object navigation targets for EmBERT training. We achieve competitive performance on the ALFRED benchmark, and EmBERT marks the first transformer-based model to successfully handle the long-horizon, dense, multi-modal histories of ALFRED, and the first ALFRED model to utilize object-centric navigation targets.", "comment": " Comments: Accepted at Novel Ideas in Learning-to-Learn through Interaction (NILLI) workshop @ EMNLP 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.12021", "id": "2107.12021", "pdf": "https://arxiv.org/pdf/2107.12021", "other": "https://arxiv.org/format/2107.12021"}, "title": "Language Models as Zero-shot Visual Semantic Learners", "author_info": ["Yue Jiao", "Jonathon Hare", "Adam Pr\u00fcgel-Bennett"], "summary": "Visual Semantic Embedding (VSE) models, which map images into a rich semantic embedding space, have been a milestone in object recognition and zero-shot learning. Current approaches to VSE heavily rely on static word em-bedding techniques. In this work, we propose a Visual Se-mantic Embedding Probe (VSEP) designed to probe the semantic information of contextualized word embeddings in visual semantic understanding tasks. We show that the knowledge encoded in transformer language models can be exploited for tasks requiring visual semantic understanding.The VSEP with contextual representations can distinguish word-level object representations in complicated scenes as a compositional zero-shot learner. We further introduce a zero-shot setting with VSEPs to evaluate a model's ability to associate a novel word with a novel visual category. We find that contextual representations in language mod-els outperform static word embeddings, when the compositional chain of object is short. We notice that current visual semantic embedding models lack a mutual exclusivity bias which limits their performance.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2107.07212", "id": "2107.07212", "pdf": "https://arxiv.org/pdf/2107.07212", "other": "https://arxiv.org/format/2107.07212"}, "title": "Duplicated Code Pattern Mining in Visual Programming Languages", "author_info": ["Miguel Terra-Neves", "Jo\u00e3o Nadkarni", "Miguel Ventura", "Pedro Resende", "Hugo Veiga", "Ant\u00f3nio Alegria"], "summary": "Visual Programming Languages (VPLs), coupled with the high-level abstractions that are commonplace in visual programming environments, enable users with less technical knowledge to become proficient programmers. However, the lower skill floor required by VPLs also entails that programmers are more likely to not adhere to best practices of software development, producing systems with high technical debt, and thus poor maintainability. Duplicated code is one important example of such technical debt. In fact, we observed that the amount of duplication in the OutSystems VPL code bases can reach as high as 39%.", "comment": " Comments: Shorter version accepted for publication at FSE 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.06546", "id": "2107.06546", "pdf": "https://arxiv.org/pdf/2107.06546", "other": "https://arxiv.org/format/2107.06546"}, "title": "ZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded Language Modelling track, 2021 edition", "author_info": ["Afra Alishahi", "Grzegorz Chrupa\u0142a", "Alejandrina Cristia", "Emmanuel Dupoux", "Bertrand Higy", "Marvin Lavechin", "Okko R\u00e4s\u00e4nen", "Chen Yu"], "summary": "We present the visually-grounded language modelling track that was introduced in the Zero-Resource Speech challenge, 2021 edition, 2nd round. We motivate the new track and discuss participation rules in detail. We also present the two baseline systems that were developed for this track.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2107.03438", "id": "2107.03438", "pdf": "https://arxiv.org/pdf/2107.03438", "other": "https://arxiv.org/format/2107.03438"}, "title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding", "author_info": ["Junha Roh", "Karthik Desingh", "Ali Farhadi", "Dieter Fox"], "summary": "For robots to understand human instructions and perform meaningful tasks in the near future, it is important to develop learned models that comprehend referential language to identify common objects in real-world 3D scenes. In this paper, we introduce a spatial-language model for a 3D visual grounding problem. Specifically, given a reconstructed 3D scene in the form of point clouds with 3D bounding boxes of potential object candidates, and a language utterance referring to a target object in the scene, our model successfully identifies the target object from a set of potential candidates. Specifically, LanguageRefer uses a transformer-based architecture that combines spatial embedding from bounding boxes with fine-tuned language embeddings from DistilBert to predict the target object. We show that it performs competitively on visio-linguistic datasets proposed by ReferIt3D. Further, we analyze its spatial reasoning task performance decoupled from perception noise, the accuracy of view-dependent utterances, and viewpoint annotations for potential robotics applications.", "comment": " Comments: 11 pages, 3 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2106.14127", "id": "2106.14127", "pdf": "https://arxiv.org/pdf/2106.14127", "other": "https://arxiv.org/format/2106.14127"}, "title": "Visual Conceptual Blending with Large-scale Language and Vision Models", "author_info": ["Songwei Ge", "Devi Parikh"], "summary": "We ask the question: to what extent can recent large-scale language and image generation models blend visual concepts? Given an arbitrary object, we identify a relevant object and generate a single-sentence description of the blend of the two using a language model. We then generate a visual depiction of the blend using a text-based image generation model. Quantitative and qualitative evaluations demonstrate the superiority of language models over classical methods for conceptual blending, and of recent large-scale image generation models over prior models for the visual depiction.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2106.13488", "id": "2106.13488", "pdf": "https://arxiv.org/pdf/2106.13488", "other": "https://arxiv.org/format/2106.13488"}, "title": "Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training", "author_info": ["Hongwei Xue", "Yupan Huang", "Bei Liu", "Houwen Peng", "Jianlong Fu", "Houqiang Li", "Jiebo Luo"], "summary": "Vision-Language Pre-training (VLP) aims to learn multi-modal representations from image-text pairs and serves for downstream vision-language tasks in a fine-tuning fashion. The dominant VLP models adopt a CNN-Transformer architecture, which embeds images with a CNN, and then aligns images and text with a Transformer. Visual relationship between visual contents plays an important role in image understanding and is the basic for inter-modal alignment learning. However, CNNs have limitations in visual relation learning due to local receptive field's weakness in modeling long-range dependencies. Thus the two objectives of learning visual relation and inter-modal alignment are encapsulated in the same Transformer network. Such design might restrict the inter-modal alignment learning in the Transformer by ignoring the specialized characteristic of each objective. To tackle this, we propose a fully Transformer visual embedding for VLP to better learn visual relation and further promote inter-modal alignment. Specifically, we propose a metric named Inter-Modality Flow (IMF) to measure the interaction between vision and language modalities (i.e., inter-modality). We also design a novel masking optimization mechanism named Masked Feature Regression (MFR) in Transformer to further promote the inter-modality learning. To the best of our knowledge, this is the first study to explore the benefit of Transformer for visual feature learning in VLP. We verify our method on a wide range of vision-language tasks, including Image-Text Retrieval, Visual Question Answering (VQA), Visual Entailment and Visual Reasoning. Our approach not only outperforms the state-of-the-art VLP performance, but also shows benefits on the IMF metric.", "comment": " Comments: Accepted by NeurIPS 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2106.01804", "id": "2106.01804", "pdf": "https://arxiv.org/pdf/2106.01804", "other": "https://arxiv.org/format/2106.01804"}, "title": "E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning", "author_info": ["Haiyang Xu", "Ming Yan", "Chenliang Li", "Bin Bi", "Songfang Huang", "Wenming Xiao", "Fei Huang"], "summary": "Vision-language pre-training (VLP) on large-scale image-text pairs has achieved huge success for the cross-modal downstream tasks. The most existing pre-training methods mainly adopt a two-step training procedure, which firstly employs a pre-trained object detector to extract region-based visual features, then concatenates the image representation and text embedding as the input of Transformer to train. However, these methods face problems of using task-specific visual representation of the specific object detector for generic cross-modal understanding, and the computation inefficiency of two-stage pipeline. In this paper, we propose the first end-to-end vision-language pre-trained model for both V+L understanding and generation, namely E2E-VLP, where we build a unified Transformer framework to jointly learn visual representation, and semantic alignments between image and text. We incorporate the tasks of object detection and image captioning into pre-training with a unified Transformer encoder-decoder architecture for enhancing visual learning. An extensive set of experiments have been conducted on well-established vision-language downstream tasks to demonstrate the effectiveness of this novel VLP paradigm.", "comment": " Comments: ACL2021 main conference "}, {"arxiv": {"page": "https://arxiv.org/abs/2105.14300", "id": "2105.14300", "pdf": "https://arxiv.org/pdf/2105.14300", "other": "https://arxiv.org/format/2105.14300"}, "title": "LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering", "author_info": ["Zujie Liang", "Haifeng Hu", "Jiaying Zhu"], "summary": "Most existing Visual Question Answering (VQA) systems tend to overly rely on language bias and hence fail to reason from the visual clue. To address this issue, we propose a novel Language-Prior Feedback (LPF) objective function, to re-balance the proportion of each answer's loss value in the total VQA loss. The LPF firstly calculates a modulating factor to determine the language bias using a question-only branch. Then, the LPF assigns a self-adaptive weight to each training sample in the training process. With this reweighting mechanism, the LPF ensures that the total VQA loss can be reshaped to a more balanced form. By this means, the samples that require certain visual information to predict will be efficiently used during training. Our method is simple to implement, model-agnostic, and end-to-end trainable. We conduct extensive experiments and the results show that the LPF (1) brings a significant improvement over various VQA models, (2) achieves competitive performance on the bias-sensitive VQA-CP v2 benchmark.", "comment": " Comments: Accepted by ACM SIGIR 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2105.12202", "id": "2105.12202", "pdf": "https://arxiv.org/pdf/2105.12202", "other": "https://arxiv.org/format/2105.12202"}, "title": "Context-Sensitive Visualization of Deep Learning Natural Language Processing Models", "author_info": ["Andrew Dunn", "Diana Inkpen", "R\u0103zvan Andonie"], "summary": "The introduction of Transformer neural networks has changed the landscape of Natural Language Processing (NLP) during the last years. So far, none of the visualization systems has yet managed to examine all the facets of the Transformers. This gave us the motivation of the current work. We propose a new NLP Transformer context-sensitive visualization method that leverages existing NLP tools to find the most significant groups of tokens (words) that have the greatest effect on the output, thus preserving some context from the original text. First, we use a sentence-level dependency parser to highlight promising word groups. The dependency parser creates a tree of relationships between the words in the sentence. Next, we systematically remove adjacent and non-adjacent tuples of \\emph{n} tokens from the input text, producing several new texts with those tokens missing. The resulting texts are then passed to a pre-trained BERT model. The classification output is compared with that of the full text, and the difference in the activation strength is recorded. The modified texts that produce the largest difference in the target classification output neuron are selected, and the combination of removed words are then considered to be the most influential on the model's output. Finally, the most influential word combinations are visualized in a heatmap.", "comment": " Comments: 9 pages, 10 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2105.05636", "id": "2105.05636", "pdf": "https://arxiv.org/pdf/2105.05636", "other": "https://arxiv.org/format/2105.05636"}, "title": "VL-NMS: Breaking Proposal Bottlenecks in Two-Stage Visual-Language Matching", "author_info": ["Wenbo Ma", "Long Chen", "Hanwang Zhang", "Jian Shao", "Yueting Zhuang", "Jun Xiao"], "summary": "The prevailing framework for matching multimodal inputs is based on a two-stage process: 1) detecting proposals with an object detector and 2) matching text queries with proposals. Existing two-stage solutions mostly focus on the matching step. In this paper, we argue that these methods overlook an obvious \\emph{mismatch} between the roles of proposals in the two stages: they generate proposals solely based on the detection confidence (i.e., query-agnostic), hoping that the proposals contain all instances mentioned in the text query (i.e., query-aware). Due to this mismatch, chances are that proposals relevant to the text query are suppressed during the filtering process, which in turn bounds the matching performance. To this end, we propose VL-NMS, which is the first method to yield query-aware proposals at the first stage. VL-NMS regards all mentioned instances as critical objects, and introduces a lightweight module to predict a score for aligning each proposal with a critical object. These scores can guide the NMS operation to filter out proposals irrelevant to the text query, increasing the recall of critical objects, resulting in a significantly improved matching performance. Since VL-NMS is agnostic to the matching step, it can be easily integrated into any state-of-the-art two-stage matching methods. We validate the effectiveness of VL-NMS on two multimodal matching tasks, namely referring expression grounding and image-text matching. Extensive ablation studies on several baselines and benchmarks consistently demonstrate the superiority of VL-NMS.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2104.13225", "id": "2104.13225", "pdf": "https://arxiv.org/pdf/2104.13225", "other": "https://arxiv.org/format/2104.13225"}, "title": "Visually grounded models of spoken language: A survey of datasets, architectures and evaluation techniques", "author_info": ["Grzegorz Chrupa\u0142a"], "summary": "This survey provides an overview of the evolution of visually grounded models of spoken language over the last 20 years. Such models are inspired by the observation that when children pick up a language, they rely on a wide range of indirect and noisy clues, crucially including signals from the visual modality co-occurring with spoken utterances. Several fields have made important contributions to this approach to modeling or mimicking the process of learning language: Machine Learning, Natural Language and Speech Processing, Computer Vision and Cognitive Science. The current paper brings together these contributions in order to provide a useful introduction and overview for practitioners in all these areas. We discuss the central research questions addressed, the timeline of developments, and the datasets which enabled much of this work. We then summarize the main modeling architectures and offer an exhaustive overview of the evaluation metrics and analysis techniques.", "comment": " Journal ref:         Journal of Artificial Intelligence Research 73 (2022) 673-707       "}, {"arxiv": {"page": "https://arxiv.org/abs/2104.10283", "id": "2104.10283", "pdf": "https://arxiv.org/pdf/2104.10283", "other": "https://arxiv.org/format/2104.10283"}, "title": "GraghVQA: Language-Guided Graph Neural Networks for Graph-based Visual Question Answering", "author_info": ["Weixin Liang", "Yanhao Jiang", "Zixuan Liu"], "summary": "Images are more than a collection of objects or attributes -- they represent a web of relationships among interconnected objects. Scene Graph has emerged as a new modality for a structured graphical representation of images. Scene Graph encodes objects as nodes connected via pairwise relations as edges. To support question answering on scene graphs, we propose GraphVQA, a language-guided graph neural network framework that translates and executes a natural language question as multiple iterations of message passing among graph nodes. We explore the design space of GraphVQA framework, and discuss the trade-off of different design choices. Our experiments on GQA dataset show that GraphVQA outperforms the state-of-the-art model by a large margin (88.43% vs. 94.78%).", "comment": " Comments: NAACL 2021 MAI-Workshop. Code available at https://github.com/codexxxl/GraphVQA "}, {"arxiv": {"page": "https://arxiv.org/abs/2104.08066", "id": "2104.08066", "pdf": "https://arxiv.org/pdf/2104.08066", "other": "https://arxiv.org/format/2104.08066"}, "title": "Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models", "author_info": ["Taichi Iki", "Akiko Aizawa"], "summary": "A method for creating a vision-and-language (V&L) model is to extend a language model through structural modifications and V&L pre-training. Such an extension aims to make a V&L model inherit the capability of natural language understanding (NLU) from the original language model. To see how well this is achieved, we propose to evaluate V&L models using an NLU benchmark (GLUE). We compare five V&L models, including single-stream and dual-stream models, trained with the same pre-training. Dual-stream models, with their higher modality independence achieved by approximately doubling the number of parameters, are expected to preserve the NLU capability better. Our main finding is that the dual-stream scores are not much different than the single-stream scores, contrary to expectation. Further analysis shows that pre-training causes the performance drop in NLU tasks with few exceptions. These results suggest that adopting a single-stream structure and devising the pre-training could be an effective method for improving the maintenance of language knowledge in V&L extensions.", "comment": " Comments: to appear at EMNLP 2021. camera-ready version "}, {"arxiv": {"page": "https://arxiv.org/abs/2104.02330", "id": "2104.02330", "pdf": "https://arxiv.org/pdf/2104.02330", "other": "https://arxiv.org/format/2104.02330"}, "title": "Visual Alignment Constraint for Continuous Sign Language Recognition", "author_info": ["Yuecong Min", "Aiming Hao", "Xiujuan Chai", "Xilin Chen"], "summary": "Vision-based Continuous Sign Language Recognition (CSLR) aims to recognize unsegmented signs from image streams. Overfitting is one of the most critical problems in CSLR training, and previous works show that the iterative training scheme can partially solve this problem while also costing more training time. In this study, we revisit the iterative training scheme in recent CSLR works and realize that sufficient training of the feature extractor is critical to solving the overfitting problem. Therefore, we propose a Visual Alignment Constraint (VAC) to enhance the feature extractor with alignment supervision. Specifically, the proposed VAC comprises two auxiliary losses: one focuses on visual features only, and the other enforces prediction alignment between the feature extractor and the alignment module. Moreover, we propose two metrics to reflect overfitting by measuring the prediction inconsistency between the feature extractor and the alignment module. Experimental results on two challenging CSLR datasets show that the proposed VAC makes CSLR networks end-to-end trainable and achieves competitive performance.", "comment": " Comments: Accpted to ICCV 2021, code is available at: https://github.com/Blueprintf/VAC_CSLR "}, {"arxiv": {"page": "https://arxiv.org/abs/2103.13942", "id": "2103.13942", "pdf": "https://arxiv.org/pdf/2103.13942", "other": "https://arxiv.org/format/2103.13942"}, "title": "Visual Grounding Strategies for Text-Only Natural Language Processing", "author_info": ["Damien Sileo"], "summary": "Visual grounding is a promising path toward more robust and accurate Natural Language Processing (NLP) models. Many multimodal extensions of BERT (e.g., VideoBERT, LXMERT, VL-BERT) allow a joint modeling of texts and images that lead to state-of-the-art results on multimodal tasks such as Visual Question Answering. Here, we leverage multimodal modeling for purely textual tasks (language modeling and classification) with the expectation that the multimodal pretraining provides a grounding that can improve text processing accuracy. We propose possible strategies in this respect. A first type of strategy, referred to as {\\it transferred grounding} consists in applying multimodal models to text-only tasks using a placeholder to replace image input. The second one, which we call {\\it associative grounding}, harnesses image retrieval to match texts with related images during both pretraining and text-only downstream tasks. We draw further distinctions into both strategies and then compare them according to their impact on language modeling and commonsense-related downstream tasks, showing improvement over text-only baselines.", "comment": " Comments: Accepted at LANTERN2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2103.00020", "id": "2103.00020", "pdf": "https://arxiv.org/pdf/2103.00020", "other": "https://arxiv.org/format/2103.00020"}, "title": "Learning Transferable Visual Models From Natural Language Supervision", "author_info": ["Alec Radford", "Jong Wook Kim", "Chris Hallacy", "Aditya Ramesh", "Gabriel Goh", "Sandhini Agarwal", "Girish Sastry", "Amanda Askell", "Pamela Mishkin", "Jack Clark", "Gretchen Krueger", "Ilya Sutskever"], "summary": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2102.05918", "id": "2102.05918", "pdf": "https://arxiv.org/pdf/2102.05918", "other": "https://arxiv.org/format/2102.05918"}, "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision", "author_info": ["Chao Jia", "Yinfei Yang", "Ye Xia", "Yi-Ting Chen", "Zarana Parekh", "Hieu Pham", "Quoc V. Le", "Yunhsuan Sung", "Zhen Li", "Tom Duerig"], "summary": "Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.", "comment": " Journal ref:         International Conference on Machine Learning 2021       "}, {"arxiv": {"page": "https://arxiv.org/abs/2102.04811", "id": "2102.04811", "pdf": "https://arxiv.org/pdf/2102.04811", "other": "https://arxiv.org/format/2102.04811"}, "title": "Broader terms curriculum mapping: Using natural language processing and visual-supported communication to create representative program planning experiences", "author_info": ["Rog\u00e9rio Duarte", "\u00c2ngela Lacerda Nobre", "Fernando Pimentel", "Marc Jacquinet"], "summary": "Accreditation bodies call for curriculum development processes open to all stakeholders, reflecting viewpoints of students, industry, university faculty and society. However, communication difficulties between faculty and non-faculty groups leave unexplored an immense collaboration potential. Using classification of learning objectives, natural language processing, and data visualization, this paper presents a method to deliver program plan representations that are universal, self-explanatory, and empowering. A simple example shows how the method contributes to representative program planning experiences and a case study is used to confirm the method's accuracy and utility.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2101.07396", "id": "2101.07396", "pdf": "https://arxiv.org/pdf/2101.07396", "other": "https://arxiv.org/format/2101.07396"}, "title": "ArtEmis: Affective Language for Visual Art", "author_info": ["Panos Achlioptas", "Maks Ovsjanikov", "Kilichbek Haydarov", "Mohamed Elhoseiny", "Leonidas Guibas"], "summary": "We present a novel large-scale dataset and accompanying machine learning models aimed at providing a detailed understanding of the interplay between visual content, its emotional effect, and explanations for the latter in language. In contrast to most existing annotation datasets in computer vision, we focus on the affective experience triggered by visual artworks and ask the annotators to indicate the dominant emotion they feel for a given image and, crucially, to also provide a grounded verbal explanation for their emotion choice. As we demonstrate below, this leads to a rich set of signals for both the objective content and the affective impact of an image, creating associations with abstract concepts (e.g., \"freedom\" or \"love\"), or references that go beyond what is directly visible, including visual similes and metaphors, or subjective references to personal experiences. We focus on visual art (e.g., paintings, artistic photographs) as it is a prime example of imagery created to elicit emotional responses from its viewers. Our dataset, termed ArtEmis, contains 439K emotion attributions and explanations from humans, on 81K artworks from WikiArt. Building on this data, we train and demonstrate a series of captioning systems capable of expressing and explaining emotions from visual stimuli. Remarkably, the captions produced by these systems often succeed in reflecting the semantic and abstract content of the image, going well beyond systems trained on existing datasets. The collected dataset and developed methods are available at https://artemisdataset.org.", "comment": " Comments: https://artemisdataset.org "}, {"arxiv": {"page": "https://arxiv.org/abs/2101.00529", "id": "2101.00529", "pdf": "https://arxiv.org/pdf/2101.00529", "other": "https://arxiv.org/format/2101.00529"}, "title": "VinVL: Revisiting Visual Representations in Vision-Language Models", "author_info": ["Pengchuan Zhang", "Xiujun Li", "Xiaowei Hu", "Jianwei Yang", "Lei Zhang", "Lijuan Wang", "Yejin Choi", "Jianfeng Gao"], "summary": "This paper presents a detailed study of improving visual representations for vision language (VL) tasks and develops an improved object detection model to provide object-centric representations of images. Compared to the most widely used \\emph{bottom-up and top-down} model \\cite{anderson2018bottom}, the new model is bigger, better-designed for VL tasks, and pre-trained on much larger training corpora that combine multiple public annotated object detection datasets. Therefore, it can generate representations of a richer collection of visual objects and concepts. While previous VL research focuses mainly on improving the vision-language fusion model and leaves the object detection model improvement untouched, we show that visual features matter significantly in VL models. In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model \\oscar \\cite{li2020oscar}, and utilize an improved approach \\short\\ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks. Our results show that the new visual features significantly improve the performance across all VL tasks, creating new state-of-the-art results on seven public benchmarks. We will release the new object detection model to public.", "comment": " Journal ref:         CVPR 2021       "}, {"arxiv": {"page": "https://arxiv.org/abs/2012.11528", "id": "2012.11528", "pdf": "https://arxiv.org/pdf/2012.11528", "other": "https://arxiv.org/format/2012.11528"}, "title": "Overcoming Language Priors with Self-supervised Learning for Visual Question Answering", "author_info": ["Xi Zhu", "Zhendong Mao", "Chunxiao Liu", "Peng Zhang", "Bin Wang", "Yongdong Zhang"], "summary": "Most Visual Question Answering (VQA) models suffer from the language prior problem, which is caused by inherent data biases. Specifically, VQA models tend to answer questions (e.g., what color is the banana?) based on the high-frequency answers (e.g., yellow) ignoring image contents. Existing approaches tackle this problem by creating delicate models or introducing additional visual annotations to reduce question dependency while strengthening image dependency. However, they are still subject to the language prior problem since the data biases have not been even alleviated. In this paper, we introduce a self-supervised learning framework to solve this problem. Concretely, we first automatically generate labeled data to balance the biased data, and propose a self-supervised auxiliary task to utilize the balanced data to assist the base VQA model to overcome language priors. Our method can compensate for the data biases by generating balanced data without introducing external annotations. Experimental results show that our method can significantly outperform the state-of-the-art, improving the overall accuracy from 49.50% to 57.59% on the most commonly used benchmark VQA-CP v2. In other words, we can increase the performance of annotation-based methods by 16% without using external annotations.", "comment": " Comments: Accepted by IJCAI 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2012.11134", "id": "2012.11134", "pdf": "https://arxiv.org/pdf/2012.11134", "other": "https://arxiv.org/format/2012.11134"}, "title": "Learning content and context with language bias for Visual Question Answering", "author_info": ["Chao Yang", "Su Feng", "Dongsheng Li", "Huawei Shen", "Guoqing Wang", "Bin Jiang"], "summary": "Visual Question Answering (VQA) is a challenging multimodal task to answer questions about an image. Many works concentrate on how to reduce language bias which makes models answer questions ignoring visual content and language context. However, reducing language bias also weakens the ability of VQA models to learn context prior. To address this issue, we propose a novel learning strategy named CCB, which forces VQA models to answer questions relying on Content and Context with language Bias. Specifically, CCB establishes Content and Context branches on top of a base VQA model and forces them to focus on local key content and global effective context respectively. Moreover, a joint loss function is proposed to reduce the importance of biased samples and retain their beneficial influence on answering questions. Experiments show that CCB outperforms the state-of-the-art methods in terms of accuracy on VQA-CP v2.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2012.09486", "id": "2012.09486", "pdf": "https://arxiv.org/pdf/2012.09486", "other": "https://arxiv.org/format/2012.09486"}, "title": "ReferentialGym: A Nomenclature and Framework for Language Emergence & Grounding in (Visual) Referential Games", "author_info": ["Kevin Denamgana\u00ef", "James Alfred Walker"], "summary": "Natural languages are powerful tools wielded by human beings to communicate information and co-operate towards common goals. Their values lie in some main properties like compositionality, hierarchy and recurrent syntax, which computational linguists have been researching the emergence of in artificial languages induced by language games. Only relatively recently, the AI community has started to investigate language emergence and grounding working towards better human-machine interfaces. For instance, interactive/conversational AI assistants that are able to relate their vision to the ongoing conversation.", "comment": " Comments: Accepted at 4th NeurIPS Workshop on Emergent Communication (EmeCom @ NeurIPS 2020) "}, {"arxiv": {"page": "https://arxiv.org/abs/2012.05446", "id": "2012.05446", "pdf": "https://arxiv.org/pdf/2012.05446", "other": "https://arxiv.org/format/2012.05446"}, "title": "Visual Perception Generalization for Vision-and-Language Navigation via Meta-Learning", "author_info": ["Ting Wang", "Zongkai Wu", "Donglin Wang"], "summary": "Vision-and-language navigation (VLN) is a challenging task that requires an agent to navigate in real-world environments by understanding natural language instructions and visual information received in real-time. Prior works have implemented VLN tasks on continuous environments or physical robots, all of which use a fixed camera configuration due to the limitations of datasets, such as 1.5 meters height, 90 degrees horizontal field of view (HFOV), etc. However, real-life robots with different purposes have multiple camera configurations, and the huge gap in visual information makes it difficult to directly transfer the learned navigation model between various robots. In this paper, we propose a visual perception generalization strategy based on meta-learning, which enables the agent to fast adapt to a new camera configuration with a few shots. In the training phase, we first locate the generalization problem to the visual perception module, and then compare two meta-learning algorithms for better generalization in seen and unseen environments. One of them uses the Model-Agnostic Meta-Learning (MAML) algorithm that requires a few shot adaptation, and the other refers to a metric-based meta-learning method with a feature-wise affine transformation layer. The experiment results show that our strategy successfully adapts the learned navigation model to a new camera configuration, and the two algorithms show their advantages in seen and unseen environments respectively.", "comment": " Comments: 8 pages, 4 figures, preprinted version "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.09304", "id": "2010.09304", "pdf": "https://arxiv.org/pdf/2010.09304", "other": "https://arxiv.org/format/2010.09304"}, "title": "Language and Visual Entity Relationship Graph for Agent Navigation", "author_info": ["Yicong Hong", "Cristian Rodriguez-Opazo", "Yuankai Qi", "Qi Wu", "Stephen Gould"], "summary": "Vision-and-Language Navigation (VLN) requires an agent to navigate in a real-world environment following natural language instructions. From both the textual and visual perspectives, we find that the relationships among the scene, its objects,and directional clues are essential for the agent to interpret complex instructions and correctly perceive the environment. To capture and utilize the relationships, we propose a novel Language and Visual Entity Relationship Graph for modelling the inter-modal relationships between text and vision, and the intra-modal relationships among visual entities. We propose a message passing algorithm for propagating information between language elements and visual entities in the graph, which we then combine to determine the next action to take. Experiments show that by taking advantage of the relationships we are able to improve over state-of-the-art. On the Room-to-Room (R2R) benchmark, our method achieves the new best performance on the test unseen split with success rate weighted by path length (SPL) of 52%. On the Room-for-Room (R4R) dataset, our method significantly improves the previous best from 13% to 34% on the success weighted by normalized dynamic time warping (SDTW). Code is available at: https://github.com/YicongHong/Entity-Graph-VLN.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2010.07526", "id": "2010.07526", "pdf": "https://arxiv.org/pdf/2010.07526", "other": "https://arxiv.org/format/2010.07526"}, "title": "Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs", "author_info": ["Ana Marasovi\u0107", "Chandra Bhagavatula", "Jae Sung Park", "Ronan Le Bras", "Noah A. Smith", "Yejin Choi"], "summary": "Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first study focused on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. The key challenge of accurate rationalization is comprehensive image understanding at all levels: not just their explicit content at the pixel level, but their contextual contents at the semantic and pragmatic levels. We present Rationale^VT Transformer, an integrated model that learns to generate free-text rationales by combining pretrained language models with object recognition, grounded visual semantic frames, and visual commonsense graphs. Our experiments show that the base pretrained language model benefits from visual adaptation and that free-text rationalization is a promising research direction to complement model interpretability for complex visual-textual reasoning tasks.", "comment": " Comments: Accepted to Findings of EMNLP "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.06775", "id": "2010.06775", "pdf": "https://arxiv.org/pdf/2010.06775", "other": "https://arxiv.org/format/2010.06775"}, "title": "Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision", "author_info": ["Hao Tan", "Mohit Bansal"], "summary": "Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper. We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora. Therefore, we develop a technique named \"vokenization\" that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call \"vokens\"). The \"vokenizer\" is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora. Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG. Code and pre-trained models publicly available at https://github.com/airsplay/vokenization", "comment": " Comments: EMNLP 2020 (15 pages) "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.03644", "id": "2010.03644", "pdf": "https://arxiv.org/pdf/2010.03644", "other": "https://arxiv.org/format/2010.03644"}, "title": "Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations", "author_info": ["Wanrong Zhu", "Xin Eric Wang", "Pradyumna Narayana", "Kazoo Sone", "Sugato Basu", "William Yang Wang"], "summary": "A major challenge in visually grounded language generation is to build robust benchmark datasets and models that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and benchmarks are reliable. In this work, we set forth to design a set of experiments to understand an important but often ignored problem in visually grounded language generation: given that humans have different utilities and visual attention, how will the sample variance in multi-reference datasets affect the models' performance? Empirically, we study several multi-reference datasets and corresponding vision-and-language tasks. We show that it is of paramount importance to report variance in experiments; that human-generated references could vary drastically in different datasets/tasks, revealing the nature of each task; that metric-wise, CIDEr has shown systematically larger variances than others. Our evaluations on reference-per-instance shed light on the design of reliable datasets in the future.", "comment": " Comments: EMNLP 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.02806", "id": "2010.02806", "pdf": "https://arxiv.org/pdf/2010.02806", "other": "https://arxiv.org/format/2010.02806"}, "title": "Textual Supervision for Visually Grounded Spoken Language Understanding", "author_info": ["Bertrand Higy", "Desmond Elliott", "Grzegorz Chrupa\u0142a"], "summary": "Visually-grounded models of spoken language understanding extract semantic information directly from speech, without relying on transcriptions. This is useful for low-resource languages, where transcriptions can be expensive or impossible to obtain. Recent work showed that these models can be improved if transcriptions are available at training time. However, it is not clear how an end-to-end approach compares to a traditional pipeline-based approach when one has access to transcriptions. Comparing different strategies, we find that the pipeline approach works better when enough text is available. With low-resource languages in mind, we also show that translations can be effectively used in place of transcriptions but more data is needed to obtain similar results.", "comment": " Comments: Findings of EMNLP 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2009.14259", "id": "2009.14259", "pdf": "https://arxiv.org/pdf/2009.14259", "other": "https://arxiv.org/format/2009.14259"}, "title": "Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions", "author_info": ["Peter A. Jansen"], "summary": "The recently proposed ALFRED challenge task aims for a virtual robotic agent to complete complex multi-step everyday tasks in a virtual home environment from high-level natural language directives, such as \"put a hot piece of bread on a plate\". Currently, the best-performing models are able to complete less than 5% of these tasks successfully. In this work we focus on modeling the translation problem of converting natural language directives into detailed multi-step sequences of actions that accomplish those goals in the virtual environment. We empirically demonstrate that it is possible to generate gold multi-step plans from language directives alone without any visual input in 26% of unseen cases. When a small amount of visual information is incorporated, namely the starting location in the virtual environment, our best-performing GPT-2 model successfully generates gold command sequences in 58% of cases. Our results suggest that contextualized language models may provide strong visual semantic planning modules for grounded virtual agents.", "comment": " Comments: Accepted to Findings of EMNLP. V2: corrected typo Table 1; margins Table 3 "}, {"arxiv": {"page": "https://arxiv.org/abs/2009.10370", "id": "2009.10370", "pdf": "https://arxiv.org/pdf/2009.10370", "other": "https://arxiv.org/format/2009.10370"}, "title": "Visual Methods for Sign Language Recognition: A Modality-Based Review", "author_info": ["Bassem Seddik", "Najoua Essoukri Ben Amara"], "summary": "Sign language visual recognition from continuous multi-modal streams is still one of the most challenging fields.", "comment": " Comments: This survey paper is accepted as Springer book chapter, currently under edition "}, {"arxiv": {"page": "https://arxiv.org/abs/2009.08700", "id": "2009.08700", "pdf": "https://arxiv.org/pdf/2009.08700"}, "title": "A Visual Language for Composable Inductive Programming", "author_info": ["Edward McDaid", "Sarah McDaid"], "summary": "We present Zoea Visual which is a visual programming language based on the Zoea composable inductive programming language. Zoea Visual allows users to create software directly from a specification that resembles a set of functional test cases. Programming with Zoea Visual involves the definition of a data flow model of test case inputs, optional intermediate values, and outputs. Data elements are represented visually and can be combined to create structures of any complexity. Data flows between elements provide additional information that allows the Zoea compiler to generate larger programs in less time. This paper includes an overview of the language. The benefits of the approach and some possible future enhancements are also discussed.", "comment": " ACM Class:           D.1.2; D.1.7; D.2.3; D.2.6; D.3.4; F.3.1; I.2.2                "}, {"arxiv": {"page": "https://arxiv.org/abs/2009.01399", "id": "2009.01399", "pdf": "https://arxiv.org/pdf/2009.01399", "other": "https://arxiv.org/format/2009.01399"}, "title": "P6: A Declarative Language for Integrating Machine Learning in Visual Analytics", "author_info": ["Jianping Kelvin Li", "Kwan-Liu Ma"], "summary": "We present P6, a declarative language for building high performance visual analytics systems through its support for specifying and integrating machine learning and interactive visualization methods. As data analysis methods based on machine learning and artificial intelligence continue to advance, a visual analytics solution can leverage these methods for better exploiting large and complex data. However, integrating machine learning methods with interactive visual analysis is challenging. Existing declarative programming libraries and toolkits for visualization lack support for coupling machine learning methods. By providing a declarative language for visual analytics, P6 can empower more developers to create visual analytics applications that combine machine learning and visualization methods for data analysis and problem solving. Through a variety of example applications, we demonstrate P6's capabilities and show the benefits of using declarative specifications to build visual analytics systems. We also identify and discuss the research opportunities and challenges for declarative visual analytics.", "comment": " Comments: Accepted for presentation at IEEE VIS 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2008.10723", "id": "2008.10723", "pdf": "https://arxiv.org/pdf/2008.10723", "other": "https://arxiv.org/format/2008.10723"}, "title": "NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries", "author_info": ["Arpit Narechania", "Arjun Srinivasan", "John Stasko"], "summary": "Natural language interfaces (NLIs) have shown great promise for visual data analysis, allowing people to flexibly specify and interact with visualizations. However, developing visualization NLIs remains a challenging task, requiring low-level implementation of natural language processing (NLP) techniques as well as knowledge of visual analytic tasks and visualization design. We present NL4DV, a toolkit for natural language-driven data visualization. NL4DV is a Python package that takes as input a tabular dataset and a natural language query about that dataset. In response, the toolkit returns an analytic specification modeled as a JSON object containing data attributes, analytic tasks, and a list of Vega-Lite specifications relevant to the input query. In doing so, NL4DV aids visualization developers who may not have a background in NLP, enabling them to create new visualization NLIs or incorporate natural language input within their existing systems. We demonstrate NL4DV's usage and capabilities through four examples: 1) rendering visualizations using natural language in a Jupyter notebook, 2) developing a NLI to specify and edit Vega-Lite charts, 3) recreating data ambiguity widgets from the DataTone system, and 4) incorporating speech input to create a multimodal visualization system.", "comment": " Comments: 11 pages, 10 figures. Proceedings of IEEE VIS'2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2008.05122", "id": "2008.05122", "pdf": "https://arxiv.org/pdf/2008.05122", "other": "https://arxiv.org/format/2008.05122"}, "title": "The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models", "author_info": ["Ian Tenney", "James Wexler", "Jasmijn Bastings", "Tolga Bolukbasi", "Andy Coenen", "Sebastian Gehrmann", "Ellen Jiang", "Mahima Pushkarna", "Carey Radebaugh", "Emily Reif", "Ann Yuan"], "summary": "We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models--including classification, seq2seq, and structured prediction--and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2007.14626", "id": "2007.14626", "pdf": "https://arxiv.org/pdf/2007.14626", "other": "https://arxiv.org/format/2007.14626"}, "title": "Object-and-Action Aware Model for Visual Language Navigation", "author_info": ["Yuankai Qi", "Zizheng Pan", "Shengping Zhang", "Anton van den Hengel", "Qi Wu"], "summary": "Vision-and-Language Navigation (VLN) is unique in that it requires turning relatively general natural-language instructions into robot agent actions, on the basis of the visible environment. This requires to extract value from two very different types of natural-language information. The first is object description (e.g., 'table', 'door'), each presenting as a tip for the agent to determine the next action by finding the item visible in the environment, and the second is action specification (e.g., 'go straight', 'turn left') which allows the robot to directly predict the next movements without relying on visual perceptions. However, most existing methods pay few attention to distinguish these information from each other during instruction encoding and mix together the matching between textual object/action encoding and visual perception/orientation features of candidate viewpoints. In this paper, we propose an Object-and-Action Aware Model (OAAM) that processes these two different forms of natural language based instruction separately. This enables each process to match object-centered/action-centered instruction to their own counterpart visual perception/action orientation flexibly. However, one side-issue caused by above solution is that an object mentioned in instructions may be observed in the direction of two or more candidate viewpoints, thus the OAAM may not predict the viewpoint on the shortest path as the next action. To handle this problem, we design a simple but effective path loss to penalize trajectories deviating from the ground truth path. Experimental results demonstrate the effectiveness of the proposed model and path loss, and the superiority of their combination with a 50% SPL score on the R2R dataset and a 40% CLS score on the R4R dataset in unseen environments, outperforming the previous state-of-the-art.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2007.11668", "id": "2007.11668", "pdf": "https://arxiv.org/pdf/2007.11668", "other": "https://arxiv.org/format/2007.11668"}, "title": "Analogical Reasoning for Visually Grounded Language Acquisition", "author_info": ["Bo Wu", "Haoyu Qin", "Alireza Zareian", "Carl Vondrick", "Shih-Fu Chang"], "summary": "Children acquire language subconsciously by observing the surrounding world and listening to descriptions. They can discover the meaning of words even without explicit language knowledge, and generalize to novel compositions effortlessly. In this paper, we bring this ability to AI, by studying the task of Visually grounded Language Acquisition (VLA). We propose a multimodal transformer model augmented with a novel mechanism for analogical reasoning, which approximates novel compositions by learning semantic mapping and reasoning operations from previously seen compositions. Our proposed method, Analogical Reasoning Transformer Networks (ARTNet), is trained on raw multimedia data (video frames and transcripts), and after observing a set of compositions such as \"washing apple\" or \"cutting carrot\", it can generalize and recognize new compositions in new video frames, such as \"washing carrot\" or \"cutting apple\". To this end, ARTNet refers to relevant instances in the training data and uses their visual features and captions to establish analogies with the query image. Then it chooses the suitable verb and noun to create a new composition that describes the new image best. Extensive experiments on an instructional video dataset demonstrate that the proposed method achieves significantly better generalization capability and recognition accuracy compared to state-of-the-art transformer models.", "comment": " MSC Class:           68T07; 68T45; 68T50; 68T40; 68T27                              ACM Class:           I.2.10; I.2.6; I.2.7; I.2.9                "}, {"arxiv": {"page": "https://arxiv.org/abs/2007.08037", "id": "2007.08037", "pdf": "https://arxiv.org/pdf/2007.08037", "other": "https://arxiv.org/format/2007.08037"}, "title": "Active Visual Information Gathering for Vision-Language Navigation", "author_info": ["Hanqing Wang", "Wenguan Wang", "Tianmin Shu", "Wei Liang", "Jianbing Shen"], "summary": "Vision-language navigation (VLN) is the task of entailing an agent to carry out navigational instructions inside photo-realistic environments. One of the key challenges in VLN is how to conduct a robust navigation by mitigating the uncertainty caused by ambiguous instructions and insufficient observation of the environment. Agents trained by current approaches typically suffer from this and would consequently struggle to avoid random and inefficient actions at every step. In contrast, when humans face such a challenge, they can still maintain robust navigation by actively exploring the surroundings to gather more information and thus make more confident navigation decisions. This work draws inspiration from human navigation behavior and endows an agent with an active information gathering ability for a more intelligent vision-language navigation policy. To achieve this, we propose an end-to-end framework for learning an exploration policy that decides i) when and where to explore, ii) what information is worth gathering during exploration, and iii) how to adjust the navigation decision after the exploration. The experimental results show promising exploration strategies emerged from training, which leads to significant boost in navigation performance. On the R2R challenge leaderboard, our agent gets promising results all three VLN settings, i.e., single run, pre-exploration, and beam search.", "comment": " Comments: ECCV2020 (changed with improved perfromance on Pre-Explore and Beam Search settings); website: https://github.com/HanqingWangAI/Active_VLN "}, {"arxiv": {"page": "https://arxiv.org/abs/2007.06198", "id": "2007.06198", "pdf": "https://arxiv.org/pdf/2007.06198", "other": "https://arxiv.org/format/2007.06198"}, "title": "Reducing Language Biases in Visual Question Answering with Visually-Grounded Question Encoder", "author_info": ["Gouthaman KV", "Anurag Mittal"], "summary": "Recent studies have shown that current VQA models are heavily biased on the language priors in the train set to answer the question, irrespective of the image. E.g., overwhelmingly answer \"what sport is\" as \"tennis\" or \"what color banana\" as \"yellow.\" This behavior restricts them from real-world application scenarios. In this work, we propose a novel model-agnostic question encoder, Visually-Grounded Question Encoder (VGQE), for VQA that reduces this effect. VGQE utilizes both visual and language modalities equally while encoding the question. Hence the question representation itself gets sufficient visual-grounding, and thus reduces the dependency of the model on the language priors. We demonstrate the effect of VGQE on three recent VQA models and achieve state-of-the-art results on the bias-sensitive split of the VQAv2 dataset; VQA-CPv2. Further, unlike the existing bias-reduction techniques, on the standard VQAv2 benchmark, our approach does not drop the accuracy; instead, it improves the performance.", "comment": " Comments: ECCV 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2006.09199", "id": "2006.09199", "pdf": "https://arxiv.org/pdf/2006.09199", "other": "https://arxiv.org/format/2006.09199"}, "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos", "author_info": ["Andrew Rouditchenko", "Angie Boggust", "David Harwath", "Brian Chen", "Dhiraj Joshi", "Samuel Thomas", "Kartik Audhkhasi", "Hilde Kuehne", "Rameswar Panda", "Rogerio Feris", "Brian Kingsbury", "Michael Picheny", "Antonio Torralba", "James Glass"], "summary": "Current methods for learning visually grounded language from videos often rely on text annotation, such as human generated captions or machine generated automatic speech recognition (ASR) transcripts. In this work, we introduce the Audio-Video Language Network (AVLnet), a self-supervised network that learns a shared audio-visual embedding space directly from raw video inputs. To circumvent the need for text annotation, we learn audio-visual representations from randomly segmented video clips and their raw audio waveforms. We train AVLnet on HowTo100M, a large corpus of publicly available instructional videos, and evaluate on image retrieval and video retrieval tasks, achieving state-of-the-art performance. We perform analysis of AVLnet's learned representations, showing our model utilizes speech and natural sounds to learn audio-visual concepts. Further, we propose a tri-modal model that jointly processes raw audio, video, and text captions from videos to learn a multi-modal semantic embedding space useful for text-video retrieval. Our code, data, and trained models will be released at avlnet.csail.mit.edu", "comment": " Comments: A version of this work has been accepted to Interspeech 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2006.06105", "id": "2006.06105", "pdf": "https://arxiv.org/pdf/2006.06105", "other": "https://arxiv.org/format/2006.06105"}, "title": "PeopleMap: Visualization Tool for Mapping Out Researchers using Natural Language Processing", "author_info": ["Jon Saad-Falcon", "Omar Shaikh", "Zijie J. Wang", "Austin P. Wright", "Sasha Richardson", "Duen Horng Chau"], "summary": "Discovering research expertise at institutions can be a difficult task. Manually curated university directories easily become out of date and they often lack the information necessary for understanding a researcher's interests and past work, making it harder to explore the diversity of research at an institution and identify research talents. This results in lost opportunities for both internal and external entities to discover new connections and nurture research collaboration. To solve this problem, we have developed PeopleMap, the first interactive, open-source, web-based tool that visually \"maps out\" researchers based on their research interests and publications by leveraging embeddings generated by natural language processing (NLP) techniques. PeopleMap provides a new engaging way for institutions to summarize their research talents and for people to discover new connections. The platform is developed with ease-of-use and sustainability in mind. Using only researchers' Google Scholar profiles as input, PeopleMap can be readily adopted by any institution using its publicly-accessible repository and detailed documentation.", "comment": " Comments: 7 pages, 3 figures, submission to the 29th ACM International Conference on Information and Knowledge Management (CIKM '20), October 19-23, 2020, Galway, Ireland "}, {"arxiv": {"page": "https://arxiv.org/abs/2006.01131", "id": "2006.01131", "pdf": "https://arxiv.org/pdf/2006.01131", "other": "https://arxiv.org/format/2006.01131"}, "title": "NLP Scholar: An Interactive Visual Explorer for Natural Language Processing Literature", "author_info": ["Saif M. Mohammad"], "summary": "As part of the NLP Scholar project, we created a single unified dataset of NLP papers and their meta-information (including citation numbers), by extracting and aligning information from the ACL Anthology and Google Scholar. In this paper, we describe several interconnected interactive visualizations (dashboards) that present various aspects of the data. Clicking on an item within a visualization or entering query terms in the search boxes filters the data in all visualizations in the dashboard. This allows users to search for papers in the area of their interest, published within specific time periods, published by specified authors, etc. The interactive visualizations presented here, and the associated dataset of papers mapped to citations, have additional uses as well including understanding how the field is growing (both overall and across sub-areas), as well as quantifying the impact of different types of papers on subsequent publications.", "comment": " Journal ref:         Proceedings of the 58th Annual Meeting of the Association of Computational Linguistics (ACL-2020), July 2020       "}, {"arxiv": {"page": "https://arxiv.org/abs/2005.11017", "id": "2005.11017", "pdf": "https://arxiv.org/pdf/2005.11017", "other": "https://arxiv.org/format/2005.11017"}, "title": "Robust Layout-aware IE for Visually Rich Documents with Pre-trained Language Models", "author_info": ["Mengxi Wei", "Yifan He", "Qiong Zhang"], "summary": "Many business documents processed in modern NLP and IR pipelines are visually rich: in addition to text, their semantics can also be captured by visual traits such as layout, format, and fonts. We study the problem of information extraction from visually rich documents (VRDs) and present a model that combines the power of large pre-trained language models and graph neural networks to efficiently encode both textual and visual information in business documents. We further introduce new fine-tuning objectives to improve in-domain unsupervised fine-tuning to better utilize large amount of unlabeled in-domain data. We experiment on real world invoice and resume data sets and show that the proposed method outperforms strong text-based RoBERTa baselines by 6.3% absolute F1 on invoices and 4.7% absolute F1 on resumes. When evaluated in a few-shot setting, our method requires up to 30x less annotation data than the baseline to achieve the same level of performance at ~90% F1.", "comment": " Comments: 10 pages, to appear in SIGIR 2020 Industry Track "}, {"arxiv": {"page": "https://arxiv.org/abs/2005.07327", "id": "2005.07327", "pdf": "https://arxiv.org/pdf/2005.07327", "other": "https://arxiv.org/format/2005.07327"}, "title": "ViTAA: Visual-Textual Attributes Alignment in Person Search by Natural Language", "author_info": ["Zhe Wang", "Zhiyuan Fang", "Jun Wang", "Yezhou Yang"], "summary": "Person search by natural language aims at retrieving a specific person in a large-scale image pool that matches the given textual descriptions. While most of the current methods treat the task as a holistic visual and textual feature matching one, we approach it from an attribute-aligning perspective that allows grounding specific attribute phrases to the corresponding visual regions. We achieve success as well as the performance boosting by a robust feature learning that the referred identity can be accurately bundled by multiple attribute visual cues. To be concrete, our Visual-Textual Attribute Alignment model (dubbed as ViTAA) learns to disentangle the feature space of a person into subspaces corresponding to attributes using a light auxiliary attribute segmentation computing branch. It then aligns these visual features with the textual attributes parsed from the sentences by using a novel contrastive learning loss. Upon that, we validate our ViTAA framework through extensive experiments on tasks of person search by natural language and by attribute-phrase queries, on which our system achieves state-of-the-art performances. Code will be publicly available upon publication.", "comment": " Comments: ECCV2020, 18 pages, 6 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2005.03257", "id": "2005.03257", "pdf": "https://arxiv.org/pdf/2005.03257", "other": "https://arxiv.org/format/2005.03257"}, "title": "Quda: Natural Language Queries for Visual Data Analytics", "author_info": ["Siwei Fu", "Kai Xiong", "Xiaodong Ge", "Siliang Tang", "Wei Chen", "Yingcai Wu"], "summary": "The identification of analytic tasks from free text is critical for visualization-oriented natural language interfaces (V-NLIs) to suggest effective visualizations. However, it is challenging due to the ambiguity and complexity nature of human language. To address this challenge, we present a new dataset, called Quda, that aims to help V-NLIs recognize analytic tasks from free-form natural language by training and evaluating cutting-edge multi-label classification models. Our dataset contains 14,035 diverse user queries, and each is annotated with one or multiple analytic tasks. We achieve this goal by first gathering seed queries with data analysts and then employing extensive crowd force for paraphrase generation and validation. We demonstrate the usefulness of Quda through three applications. This work is the first attempt to construct a large-scale corpus for recognizing analytic tasks. With the release of Quda, we hope it will boost the research and development of V-NLIs in data analysis and visualization.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2005.00619", "id": "2005.00619", "pdf": "https://arxiv.org/pdf/2005.00619", "other": "https://arxiv.org/format/2005.00619"}, "title": "Probing Contextual Language Models for Common Ground with Visual Representations", "author_info": ["Gabriel Ilharco", "Rowan Zellers", "Ali Farhadi", "Hannaneh Hajishirzi"], "summary": "The success of large-scale contextual language models has attracted great interest in probing what is encoded in their representations. In this work, we consider a new question: to what extent contextual representations of concrete nouns are aligned with corresponding visual representations? We design a probing model that evaluates how effective are text-only representations in distinguishing between matching and non-matching visual representations. Our findings show that language representations alone provide a strong signal for retrieving image patches from the correct object categories. Moreover, they are effective in retrieving specific instances of image patches; textual context plays an important role in this process. Visually grounded language models slightly outperform text-only language models in instance retrieval, but greatly under-perform humans. We hope our analyses inspire future research in understanding and improving the visual capabilities of language models.", "comment": " Comments: Proceedings of the 2021 North American Chapter of the Association for Computational Linguistics (NAACL 2021) "}, {"arxiv": {"page": "https://arxiv.org/abs/2004.14603", "id": "2004.14603", "pdf": "https://arxiv.org/pdf/2004.14603", "other": "https://arxiv.org/format/2004.14603"}, "title": "Dynamic Language Binding in Relational Visual Reasoning", "author_info": ["Thao Minh Le", "Vuong Le", "Svetha Venkatesh", "Truyen Tran"], "summary": "We present Language-binding Object Graph Network, the first neural reasoning method with dynamic relational structures across both visual and textual domains with applications in visual question answering. Relaxing the common assumption made by current models that the object predicates pre-exist and stay static, passive to the reasoning process, we propose that these dynamic predicates expand across the domain borders to include pair-wise visual-linguistic object binding. In our method, these contextualized object links are actively found within each recurrent reasoning step without relying on external predicative priors. These dynamic structures reflect the conditional dual-domain object dependency given the evolving context of the reasoning through co-attention. Such discovered dynamic graphs facilitate multi-step knowledge combination and refinements that iteratively deduce the compact representation of the final answer. The effectiveness of this model is demonstrated on image question answering demonstrating favorable performance on major VQA datasets. Our method outperforms other methods in sophisticated question-answering tasks wherein multiple object relations are involved. The graph structure effectively assists the progress of training, and therefore the network learns efficiently compared to other reasoning models.", "comment": " Comments: Early version accepted by IJCAI20, Code available at https://github.com/thaolmk54/LOGNet-VQA "}, {"arxiv": {"page": "https://arxiv.org/abs/2004.14166", "id": "2004.14166", "pdf": "https://arxiv.org/pdf/2004.14166", "other": "https://arxiv.org/format/2004.14166"}, "title": "SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check", "author_info": ["Xingyi Cheng", "Weidi Xu", "Kunlong Chen", "Shaohua Jiang", "Feng Wang", "Taifeng Wang", "Wei Chu", "Yuan Qi"], "summary": "Chinese Spelling Check (CSC) is a task to detect and correct spelling errors in Chinese natural language. Existing methods have made attempts to incorporate the similarity knowledge between Chinese characters. However, they take the similarity knowledge as either an external input resource or just heuristic rules. This paper proposes to incorporate phonological and visual similarity knowledge into language models for CSC via a specialized graph convolutional network (SpellGCN). The model builds a graph over the characters, and SpellGCN is learned to map this graph into a set of inter-dependent character classifiers. These classifiers are applied to the representations extracted by another network, such as BERT, enabling the whole network to be end-to-end trainable. Experiments (The dataset and all code for this paper are available at https://github.com/ACL2020SpellGCN/SpellGCN) are conducted on three human-annotated datasets. Our method achieves superior performance against previous models by a large margin.", "comment": " Comments: Accepted by ACL2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2004.03744", "id": "2004.03744", "pdf": "https://arxiv.org/pdf/2004.03744", "other": "https://arxiv.org/format/2004.03744"}, "title": "e-SNLI-VE: Corrected Visual-Textual Entailment with Natural Language Explanations", "author_info": ["Virginie Do", "Oana-Maria Camburu", "Zeynep Akata", "Thomas Lukasiewicz"], "summary": "The recently proposed SNLI-VE corpus for recognising visual-textual entailment is a large, real-world dataset for fine-grained multimodal reasoning. However, the automatic way in which SNLI-VE has been assembled (via combining parts of two related datasets) gives rise to a large number of errors in the labels of this corpus. In this paper, we first present a data collection effort to correct the class with the highest error rate in SNLI-VE. Secondly, we re-evaluate an existing model on the corrected corpus, which we call SNLI-VE-2.0, and provide a quantitative comparison with its performance on the non-corrected corpus. Thirdly, we introduce e-SNLI-VE, which appends human-written natural language explanations to SNLI-VE-2.0. Finally, we train models that learn from these explanations at training time, and output such explanations at testing time.", "comment": " Journal ref:         IEEE CVPR Workshop on Fair, Data Efficient and Trusted Computer Vision, 2020       "}, {"arxiv": {"page": "https://arxiv.org/abs/2003.12739", "id": "2003.12739", "pdf": "https://arxiv.org/pdf/2003.12739", "other": "https://arxiv.org/format/2003.12739"}, "title": "Modulating Bottom-Up and Top-Down Visual Processing via Language-Conditional Filters", "author_info": ["\u0130lker Kesen", "Ozan Arkan Can", "Erkut Erdem", "Aykut Erdem", "Deniz Yuret"], "summary": "How to best integrate linguistic and perceptual processing in multi-modal tasks that involve language and vision is an important open problem. In this work, we argue that the common practice of using language in a top-down manner, to direct visual attention over high-level visual features, may not be optimal. We hypothesize that the use of language to also condition the bottom-up processing from pixels to high-level features can provide benefits to the overall performance. To support our claim, we propose a model for language-vision problems involving dense prediction, and perform experiments on two different multi-modal tasks: image segmentation from referring expressions and language-guided image colorization. We compare results where either one or both of the top-down and bottom-up visual branches are conditioned on language. Our experiments reveal that using language to control the filters for bottom-up visual processing in addition to top-down attention leads to better results on both tasks and achieves state-of-the-art performance. Our analysis of different word types in input expressions suggest that the bottom-up conditioning is especially helpful in the presence of low level visual concepts like color.", "comment": " Comments: 13 pages, 6 figures, currently under review for IEEE Transactions on Multimedia "}, {"arxiv": {"page": "https://arxiv.org/abs/2002.06701", "id": "2002.06701", "pdf": "https://arxiv.org/pdf/2002.06701", "other": "https://arxiv.org/format/2002.06701"}, "title": "Gaussian Smoothen Semantic Features (GSSF) -- Exploring the Linguistic Aspects of Visual Captioning in Indian Languages (Bengali) Using MSCOCO Framework", "author_info": ["Chiranjib Sur"], "summary": "In this work, we have introduced Gaussian Smoothen Semantic Features (GSSF) for Better Semantic Selection for Indian regional language-based image captioning and introduced a procedure where we used the existing translation and English crowd-sourced sentences for training. We have shown that this architecture is a promising alternative source, where there is a crunch in resources. Our main contribution of this work is the development of deep learning architectures for the Bengali language (is the fifth widely spoken language in the world) with a completely different grammar and language attributes. We have shown that these are working well for complex applications like language generation from image contexts and can diversify the representation through introducing constraints, more extensive features, and unique feature spaces. We also established that we could achieve absolute precision and diversity when we use smoothened semantic tensor with the traditional LSTM and feature decomposition networks. With better learning architecture, we succeeded in establishing an automated algorithm and assessment procedure that can help in the evaluation of competent applications without the requirement for expertise and human intervention.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2001.11604", "id": "2001.11604", "pdf": "https://arxiv.org/pdf/2001.11604", "other": "https://arxiv.org/format/2001.11604"}, "title": "Diva: A Declarative and Reactive Language for In-Situ Visualization", "author_info": ["Qi Wu", "Tyson Neuroth", "Oleg Igouchkine", "Konduri Aditya", "Jacqueline H. Chen", "Kwan-Liu Ma"], "summary": "The use of adaptive workflow management for in situ visualization and analysis has been a growing trend in large-scale scientific simulations. However, coordinating adaptive workflows with traditional procedural programming languages can be difficult because system flow is determined by unpredictable scientific phenomena, which often appear in an unknown order and can evade event handling. This makes the implementation of adaptive workflows tedious and error-prone. Recently, reactive and declarative programming paradigms have been recognized as well-suited solutions to similar problems in other domains. However, there is a dearth of research on adapting these approaches to in situ visualization and analysis. With this paper, we present a language design and runtime system for developing adaptive systems through a declarative and reactive programming paradigm. We illustrate how an adaptive workflow programming system is implemented using our approach and demonstrate it with a use case from a combustion simulation.", "comment": " Comments: 11 pages, 5 figures, 6 listings, 1 table, to be published in LDAV 2020. The article has gone through 2 major revisions: Emphasized contributions, features and examples. Addressed connections between DIVA and FRP. In sec. 3, we fixed a design flaw and addressed it in sec. 3.3-3.4. Re-designed sec. 5 with a more concrete example and benchmark results. Simplified the syntax of DIVA "}, {"arxiv": {"page": "https://arxiv.org/abs/1912.11872", "id": "1912.11872", "pdf": "https://arxiv.org/pdf/1912.11872", "other": "https://arxiv.org/format/1912.11872"}, "title": "Vision and Language: from Visual Perception to Content Creation", "author_info": ["Tao Mei", "Wei Zhang", "Ting Yao"], "summary": "Vision and language are two fundamental capabilities of human intelligence. Humans routinely perform tasks through the interactions between vision and language, supporting the uniquely human capacity to talk about what they see or hallucinate a picture on a natural-language description. The valid question of how language interacts with vision motivates us researchers to expand the horizons of computer vision area. In particular, \"vision to language\" is probably one of the most popular topics in the past five years, with a significant growth in both volume of publications and extensive applications, e.g., captioning, visual question answering, visual dialog, language navigation, etc. Such tasks boost visual perception with more comprehensive understanding and diverse linguistic representations. Going beyond the progresses made in \"vision to language,\" language can also contribute to vision understanding and offer new possibilities of visual content creation, i.e., \"language to vision.\" The process performs as a prism through which to create visual content conditioning on the language inputs. This paper reviews the recent advances along these two dimensions: \"vision to language\" and \"language to vision.\" More concretely, the former mainly focuses on the development of image/video captioning, as well as typical encoder-decoder structures and benchmarks, while the latter summarizes the technologies of visual content creation. The real-world deployment or services of vision and language are elaborated as well.", "comment": " Journal ref:         APSIPA Transactions on Signal and Information Processing 9 (2020) e11       "}, {"arxiv": {"page": "https://arxiv.org/abs/1912.02256", "id": "1912.02256", "pdf": "https://arxiv.org/pdf/1912.02256", "other": "https://arxiv.org/format/1912.02256"}, "title": "Compositional Temporal Visual Grounding of Natural Language Event Descriptions", "author_info": ["Jonathan C. Stroud", "Ryan McCaffrey", "Rada Mihalcea", "Jia Deng", "Olga Russakovsky"], "summary": "Temporal grounding entails establishing a correspondence between natural language event descriptions and their visual depictions. Compositional modeling becomes central: we first ground atomic descriptions \"girl eating an apple,\" \"batter hitting the ball\" to short video segments, and then establish the temporal relationships between the segments. This compositional structure enables models to recognize a wider variety of events not seen during training through recognizing their atomic sub-events. Explicit temporal modeling accounts for a wide variety of temporal relationships that can be expressed in language: e.g., in the description \"girl stands up from the table after eating an apple\" the visual ordering of the events is reversed, with first \"eating an apple\" followed by \"standing up from the table.\" We leverage these observations to develop a unified deep architecture, CTG-Net, to perform temporal grounding of natural language event descriptions to videos. We demonstrate that our system outperforms prior state-of-the-art methods on the DiDeMo, Tempo-TL, and Tempo-HL temporal grounding datasets.", "comment": " Comments: Project page: jonathancstroud.com/ctg "}, {"arxiv": {"page": "https://arxiv.org/abs/1912.00336", "id": "1912.00336", "pdf": "https://arxiv.org/pdf/1912.00336", "other": "https://arxiv.org/format/1912.00336"}, "title": "Semi-supervised Visual Feature Integration for Pre-trained Language Models", "author_info": ["Lisai Zhang", "Qingcai Chen", "Dongfang Li", "Buzhou Tang"], "summary": "Integrating visual features has been proved useful for natural language understanding tasks. Nevertheless, in most existing multimodal language models, the alignment of visual and textual data is expensive. In this paper, we propose a novel semi-supervised visual integration framework for pre-trained language models. In the framework, the visual features are obtained through a visualization and fusion mechanism. The uniqueness includes: 1) the integration is conducted via a semi-supervised approach, which does not require aligned images for every sentences 2) the visual features are integrated as an external component and can be directly used by pre-trained language models. To verify the efficacy of the proposed framework, we conduct the experiments on both natural language inference and reading comprehension tasks. The results demonstrate that our mechanism brings improvement to two strong baseline models. Considering that our framework only requires an image database, and no not requires further alignments, it provides an efficient and feasible way for multimodal language learning.", "comment": " Comments: 12 pages, 6 figures, 5 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/1911.05546", "id": "1911.05546", "pdf": "https://arxiv.org/pdf/1911.05546", "other": "https://arxiv.org/format/1911.05546"}, "title": "Avoiding hashing and encouraging visual semantics in referential emergent language games", "author_info": ["Daniela Mihai", "Jonathon Hare"], "summary": "There has been an increasing interest in the area of emergent communication between agents which learn to play referential signalling games with realistic images. In this work, we consider the signalling game setting of Havrylov and Titov and investigate the effect of the feature extractor's weights and of the task being solved on the visual semantics learned or captured by the models. We impose various augmentation to the input images and additional tasks in the game with the aim to induce visual representations which capture conceptual properties of images. Through our set of experiments, we demonstrate that communication systems which capture visual semantics can be learned in a completely self-supervised manner by playing the right types of game.", "comment": " Comments: 4 pages, presented at Emergent Communication: Towards Natural Language workshop (NeurIPS 2019) "}, {"arxiv": {"page": "https://arxiv.org/abs/1911.03738", "id": "1911.03738", "pdf": "https://arxiv.org/pdf/1911.03738", "other": "https://arxiv.org/format/1911.03738"}, "title": "On Architectures for Including Visual Information in Neural Language Models for Image Description", "author_info": ["Marc Tanti", "Albert Gatt", "Kenneth P. Camilleri"], "summary": "A neural language model can be conditioned into generating descriptions for images by providing visual information apart from the sentence prefix. This visual information can be included into the language model through different points of entry resulting in different neural architectures. We identify four main architectures which we call init-inject, pre-inject, par-inject, and merge.", "comment": " Comments: 145 pages, 41 figures, 15 tables, Doctoral thesis "}, {"arxiv": {"page": "https://arxiv.org/abs/1911.02683", "id": "1911.02683", "pdf": "https://arxiv.org/pdf/1911.02683", "other": "https://arxiv.org/format/1911.02683"}, "title": "Shaping Visual Representations with Language for Few-shot Classification", "author_info": ["Jesse Mu", "Percy Liang", "Noah Goodman"], "summary": "By describing the features and abstractions of our world, language is a crucial tool for human learning and a promising source of supervision for machine learning models. We use language to improve few-shot visual classification in the underexplored scenario where natural language task descriptions are available during training, but unavailable for novel tasks at test time. Existing models for this setting sample new descriptions at test time and use those to classify images. Instead, we propose language-shaped learning (LSL), an end-to-end model that regularizes visual representations to predict language. LSL is conceptually simpler, more data efficient, and outperforms baselines in two challenging few-shot domains.", "comment": " Comments: ACL 2020. Version 1 appeared at the NeurIPS 2019 Workshop on Visually Grounded Interaction and Language (ViGIL) "}, {"arxiv": {"page": "https://arxiv.org/abs/1910.04887", "id": "1910.04887", "pdf": "https://arxiv.org/pdf/1910.04887", "other": "https://arxiv.org/format/1910.04887"}, "title": "Visual Natural Language Query Auto-Completion for Estimating Instance Probabilities", "author_info": ["Samuel Sharpe", "Jin Yan", "Fan Wu", "Iddo Drori"], "summary": "We present a new task of query auto-completion for estimating instance probabilities. We complete a user query prefix conditioned upon an image. Given the complete query, we fine tune a BERT embedding for estimating probabilities of a broad set of instances. The resulting instance probabilities are used for selection while being agnostic to the segmentation or attention mechanism. Our results demonstrate that auto-completion using both language and vision performs better than using only language, and that fine tuning a BERT embedding allows to efficiently rank instances in the image. In the spirit of reproducible research we make our data, models, and code available.", "comment": " Journal ref:         CVPR Language and Vision Workshop, 2019       "}, {"arxiv": {"page": "https://arxiv.org/abs/1910.01210", "id": "1910.01210", "pdf": "https://arxiv.org/pdf/1910.01210", "other": "https://arxiv.org/format/1910.01210"}, "title": "Embodied Language Grounding with 3D Visual Feature Representations", "author_info": ["Mihir Prabhudesai", "Hsiao-Yu Fish Tung", "Syed Ashar Javed", "Maximilian Sieb", "Adam W. Harley", "Katerina Fragkiadaki"], "summary": "We propose associating language utterances to 3D visual abstractions of the scene they describe. The 3D visual abstractions are encoded as 3-dimensional visual feature maps. We infer these 3D visual scene feature maps from RGB images of the scene via view prediction: when the generated 3D scene feature map is neurally projected from a camera viewpoint, it should match the corresponding RGB image. We present generative models that condition on the dependency tree of an utterance and generate a corresponding visual 3D feature map as well as reason about its plausibility, and detector models that condition on both the dependency tree of an utterance and a related image and localize the object referents in the 3D feature map inferred from the image. Our model outperforms models of language and vision that associate language with 2D CNN activations or 2D images by a large margin in a variety of tasks, such as, classifying plausibility of utterances, detecting referential expressions, and supplying rewards for trajectory optimization of object placement policies from language instructions. We perform numerous ablations and show the improved performance of our detectors is due to its better generalization across camera viewpoints and lack of object interferences in the inferred 3D feature space, and the improved performance of our generators is due to their ability to spatially reason about objects and their configurations in 3D when mapping from language to scenes.", "comment": " Journal ref:         Conference on Computer Vision and Pattern Recognition. 2020, pp. 2220-2229       "}, {"arxiv": {"page": "https://arxiv.org/abs/1909.10407", "id": "1909.10407", "pdf": "https://arxiv.org/pdf/1909.10407", "other": "https://arxiv.org/format/1909.10407"}, "title": "CochleaNet: A Robust Language-independent Audio-Visual Model for Speech Enhancement", "author_info": ["Mandar Gogate", "Kia Dashtipour", "Ahsan Adeel", "Amir Hussain"], "summary": "Noisy situations cause huge problems for suffers of hearing loss as hearing aids often make the signal more audible but do not always restore the intelligibility. In noisy settings, humans routinely exploit the audio-visual (AV) nature of the speech to selectively suppress the background noise and to focus on the target speaker. In this paper, we present a causal, language, noise and speaker independent AV deep neural network (DNN) architecture for speech enhancement (SE). The model exploits the noisy acoustic cues and noise robust visual cues to focus on the desired speaker and improve the speech intelligibility. To evaluate the proposed SE framework a first of its kind AV binaural speech corpus, called ASPIRE, is recorded in real noisy environments including cafeteria and restaurant. We demonstrate superior performance of our approach in terms of objective measures and subjective listening tests over the state-of-the-art SE approaches as well as recent DNN based SE models. In addition, our work challenges a popular belief that a scarcity of multi-language large vocabulary AV corpus and wide variety of noises is a major bottleneck to build a robust language, speaker and noise independent SE systems. We show that a model trained on synthetic mixture of Grid corpus (with 33 speakers and a small English vocabulary) and ChiME 3 Noises (consisting of only bus, pedestrian, cafeteria, and street noises) generalise well not only on large vocabulary corpora but also on completely unrelated languages (such as Mandarin), wide variety of speakers and noises.", "comment": " Comments: 34 pages, 11 figures, Submitted to Information Fusion "}, {"arxiv": {"page": "https://arxiv.org/abs/1909.05365", "id": "1909.05365", "pdf": "https://arxiv.org/pdf/1909.05365", "other": "https://arxiv.org/format/1909.05365"}, "title": "Building Task-Oriented Visual Dialog Systems Through Alternative Optimization Between Dialog Policy and Language Generation", "author_info": ["Mingyang Zhou", "Josh Arnold", "Zhou Yu"], "summary": "Reinforcement learning (RL) is an effective approach to learn an optimal dialog policy for task-oriented visual dialog systems. A common practice is to apply RL on a neural sequence-to-sequence (seq2seq) framework with the action space being the output vocabulary in the decoder. However, it is difficult to design a reward function that can achieve a balance between learning an effective policy and generating a natural dialog response. This paper proposes a novel framework that alternatively trains a RL policy for image guessing and a supervised seq2seq model to improve dialog generation quality. We evaluate our framework on the GuessWhich task and the framework achieves the state-of-the-art performance in both task completion and dialog quality.", "comment": " Comments: updated with acknowledgement and minor typo fixes on tables "}, {"arxiv": {"page": "https://arxiv.org/abs/1909.04499", "id": "1909.04499", "pdf": "https://arxiv.org/pdf/1909.04499", "other": "https://arxiv.org/format/1909.04499"}, "title": "Countering Language Drift via Visual Grounding", "author_info": ["Jason Lee", "Kyunghyun Cho", "Douwe Kiela"], "summary": "Emergent multi-agent communication protocols are very different from natural language and not easily interpretable by humans. We find that agents that were initially pretrained to produce natural language can also experience detrimental language drift: when a non-linguistic reward is used in a goal-based task, e.g. some scalar success metric, the communication protocol may easily and radically diverge from natural language. We recast translation as a multi-agent communication game and examine auxiliary training constraints for their effectiveness in mitigating language drift. We show that a combination of syntactic (language model likelihood) and semantic (visual grounding) constraints gives the best communication performance, allowing pre-trained agents to retain English syntax while learning to accurately convey the intended meaning.", "comment": " Comments: Accepted to EMNLP 2019 "}, {"arxiv": {"page": "https://arxiv.org/abs/1908.01699", "id": "1908.01699", "pdf": "https://arxiv.org/pdf/1908.01699"}, "title": "Thoth: Improved Rapid Serial Visual Presentation using Natural Language Processing", "author_info": ["David Awad"], "summary": "Thoth is a tool designed to combine many different types of speed reading technology. The largest insight is using natural language parsing for more optimal rapid serial visual presentation and more effective reading information.", "comment": " Comments: 10 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/1908.00681", "id": "1908.00681", "pdf": "https://arxiv.org/pdf/1908.00681", "other": "https://arxiv.org/format/1908.00681"}, "title": "FlowSense: A Natural Language Interface for Visual Data Exploration within a Dataflow System", "author_info": ["Bowen Yu", "Claudio T. Silva"], "summary": "Dataflow visualization systems enable flexible visual data exploration by allowing the user to construct a dataflow diagram that composes query and visualization modules to specify system functionality. However learning dataflow diagram usage presents overhead that often discourages the user. In this work we design FlowSense, a natural language interface for dataflow visualization systems that utilizes state-of-the-art natural language processing techniques to assist dataflow diagram construction. FlowSense employs a semantic parser with special utterance tagging and special utterance placeholders to generalize to different datasets and dataflow diagrams. It explicitly presents recognized dataset and diagram special utterances to the user for dataflow context awareness. With FlowSense the user can expand and adjust dataflow diagrams more conveniently via plain English. We apply FlowSense to the VisFlow subset-flow visualization system to enhance its usability. We evaluate FlowSense by one case study with domain experts on a real-world data analysis problem and a formal user study.", "comment": " Comments: Published in IEEE Transactions on Visualization and Computer Graphics "}, {"arxiv": {"page": "https://arxiv.org/abs/1908.00277", "id": "1908.00277", "pdf": "https://arxiv.org/pdf/1908.00277", "other": "https://arxiv.org/format/1908.00277"}, "title": "A Natural-language-based Visual Query Approach of Uncertain Human Trajectories", "author_info": ["Zhaosong Huang", "Ye Zhao", "Wei Chen", "Shengjie Gao", "Kejie Yu", "Weixia Xu", "Mingjie Tang", "Minfeng Zhu", "Mingliang Xu"], "summary": "Visual querying is essential for interactively exploring massive trajectory data. However, the data uncertainty imposes profound challenges to fulfill advanced analytics requirements. On the one hand, many underlying data does not contain accurate geographic coordinates, e.g., positions of a mobile phone only refer to the regions (i.e., mobile cell stations) in which it resides, instead of accurate GPS coordinates. On the other hand, domain experts and general users prefer a natural way, such as using a natural language sentence, to access and analyze massive movement data. In this paper, we propose a visual analytics approach that can extract spatial-temporal constraints from a textual sentence and support an effective query method over uncertain mobile trajectory data. It is built up on encoding massive, spatially uncertain trajectories by the semantic information of the POIs and regions covered by them, and then storing the trajectory documents in text database with an effective indexing scheme. The visual interface facilitates query condition specification, situation-aware visualization, and semantic exploration of large trajectory data. Usage scenarios on real-world human mobility datasets demonstrate the effectiveness of our approach.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/1907.11751", "id": "1907.11751", "pdf": "https://arxiv.org/pdf/1907.11751", "other": "https://arxiv.org/format/1907.11751"}, "title": "Real-time Visual Object Tracking with Natural Language Description", "author_info": ["Qi Feng", "Vitaly Ablavsky", "Qinxun Bai", "Guorong Li", "Stan Sclaroff"], "summary": "In recent years, deep-learning-based visual object trackers have been studied thoroughly, but handling occlusions and/or rapid motion of the target remains challenging. In this work, we argue that conditioning on the natural language (NL) description of a target provides information for longer-term invariance, and thus helps cope with typical tracking challenges. However, deriving a formulation to combine the strengths of appearance-based tracking with the language modality is not straightforward. We propose a novel deep tracking-by-detection formulation that can take advantage of NL descriptions. Regions that are related to the given NL description are generated by a proposal network during the detection phase of the tracker. Our LSTM based tracker then predicts the update of the target from regions proposed by the NL based detection phase. In benchmarks, our method is competitive with state of the art trackers, while it outperforms all other trackers on targets with unambiguous and precise language annotations. It also beats the state-of-the-art NL tracker when initializing without a bounding box. Our method runs at over 30 fps on a single GPU.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/1907.08501", "id": "1907.08501", "pdf": "https://arxiv.org/pdf/1907.08501", "other": "https://arxiv.org/format/1907.08501"}, "title": "A Comparative Evaluation of Visual and Natural Language Question Answering Over Linked Data", "author_info": ["Gerhard Wohlgenannt", "Dmitry Mouromtsev", "Dmitry Pavlov", "Yury Emelyanov", "Alexey Morozov"], "summary": "With the growing number and size of Linked Data datasets, it is crucial to make the data accessible and useful for users without knowledge of formal query languages. Two approaches towards this goal are knowledge graph visualization and natural language interfaces. Here, we investigate specifically question answering (QA) over Linked Data by comparing a diagrammatic visual approach with existing natural language-based systems. Given a QA benchmark (QALD7), we evaluate a visual method which is based on iteratively creating diagrams until the answer is found, against four QA systems that have natural language queries as input. Besides other benefits, the visual approach provides higher performance, but also requires more manual input. The results indicate that the methods can be used complementary, and that such a combination has a large positive impact on QA performance, and also facilitates additional features such as data exploration.", "comment": " Comments: KEOD 2019 "}, {"arxiv": {"page": "https://arxiv.org/abs/1906.07689", "id": "1906.07689", "pdf": "https://arxiv.org/pdf/1906.07689", "other": "https://arxiv.org/format/1906.07689"}, "title": "Expressing Visual Relationships via Language", "author_info": ["Hao Tan", "Franck Dernoncourt", "Zhe Lin", "Trung Bui", "Mohit Bansal"], "summary": "Describing images with text is a fundamental problem in vision-language research. Current studies in this domain mostly focus on single image captioning. However, in various real applications (e.g., image editing, difference interpretation, and retrieval), generating relational captions for two images, can also be very useful. This important problem has not been explored mostly due to lack of datasets and effective models. To push forward the research in this direction, we first introduce a new language-guided image editing dataset that contains a large number of real image pairs with corresponding editing instructions. We then propose a new relational speaker model based on an encoder-decoder architecture with static relational attention and sequential multi-head attention. We also extend the model with dynamic relational attention, which calculates visual alignment while decoding. Our models are evaluated on our newly collected and two public datasets consisting of image pairs annotated with relationship sentences. Experimental results, based on both automatic and human evaluation, demonstrate that our model outperforms all baselines and existing methods on all the datasets.", "comment": " Comments: ACL 2019 (11 pages) "}]}