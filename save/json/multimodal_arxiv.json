{"data": [{"arxiv": {"page": "https://arxiv.org/abs/2202.10644", "id": "2202.10644", "pdf": "https://arxiv.org/pdf/2202.10644"}, "title": "Multiview Scattering Scanning Imaging Confocal Microscopy through a Multimode Fiber", "author_info": ["Sakshi Singh", "Simon Labouesse", "Rafael Piestun"], "summary": "Confocal and multiphoton microscopy are effective techniques to obtain high-contrast images of 2-D sections within bulk tissue. However, scattering limits their application to depths only up to ~1 millimeter. Multimode fibers make excellent ultrathin endoscopes that can penetrate deep inside the tissue with minimal damage. Here, we present Multiview Scattering Scanning Imaging Confocal (MUSSIC) Microscopy that enables high signal-to-noise ratio (SNR) imaging through a multimode fiber, hence combining the optical sectioning and resolution gain of confocal microscopy with the minimally invasive penetration capability of multimode fibers. The key advance presented here is the high SNR image reconstruction enabled by employing multiple coplanar virtual pinholes to capture multiple perspectives of the object, re-shifting them appropriately and combining them to obtain a high-contrast and high-resolution confocal image. We present the theory for the gain in contrast and resolution in MUSSIC microscopy and validate the concept through experimental results.", "comment": " Comments: 13 pages, 5 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10454", "id": "2202.10454", "pdf": "https://arxiv.org/pdf/2202.10454"}, "title": "A Novel Anomaly Detection Method for Multimodal WSN Data Flow via a Dynamic Graph Neural Network", "author_info": ["Qinghao Zhang", "Miao Ye", "Hongbing Qiu", "Yong Wang", "Xiaofang Deng"], "summary": "Anomaly detection is widely used to distinguish system anomalies by analyzing the temporal and spatial features of wireless sensor network (WSN) data streams; it is one of critical technique that ensures the reliability of WSNs. Currently, graph neural networks (GNNs) have become popular state-of-the-art methods for conducting anomaly detection on WSN data streams. However, the existing anomaly detection methods based on GNNs do not consider the temporal and spatial features of WSN data streams simultaneously, such as multi-node, multi-modal and multi-time features, seriously impacting their effectiveness. In this paper, a novel anomaly detection model is proposed for multimodal WSN data flows, where three GNNs are used to separately extract the temporal features of WSN data flows, the correlation features between different modes and the spatial features between sensor node positions. Specifically, first, the temporal features and modal correlation features extracted from each sensor node are fused into one vector representation, which is further aggregated with the spatial features, i.e., the spatial position relationships of the nodes; finally, the current time-series data of WSN nodes are predicted, and abnormal states are identified according to the fusion features. The simulation results obtained on a public dataset show that the proposed approach is able to significantly improve upon the existing methods in terms of its robustness, and its F1 score reaches 0.90, which is 14.2% higher than that of the graph convolution network (GCN) with long short-term memory (LSTM).", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10325", "id": "2202.10325", "pdf": "https://arxiv.org/pdf/2202.10325", "other": "https://arxiv.org/format/2202.10325"}, "title": "Reducing the Gibbs effect in multimodal medical imaging by the Fake Nodes Approach", "author_info": ["Davide Poggiali", "Diego Cecchin", "Stefano De Marchi"], "summary": "It is a common practice in multimodal medical imaging to undersample the anatomically-derived segmentation images to measure the mean activity of a co-acquired functional image. This practice avoids the resampling-related Gibbs effect that would occur in oversampling the functional image. As sides effect, waste of time and efforts are produced since the anatomical segmentation at full resolution is performed in many hours of computations or manual work. In this work we explain the commonly-used resampling methods and give errors bound in the cases of continuous and discontinuous signals. Then we propose a Fake Nodes scheme for image resampling designed to reduce the Gibbs effect when oversampling the functional image. This new approach is compared to the traditional counterpart in two significant experiments, both showing that Fake Nodes resampling gives smaller errors.", "comment": " MSC Class:           68U10; 65D05; 41A15                          "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09843", "id": "2202.09843", "pdf": "https://arxiv.org/pdf/2202.09843", "other": "https://arxiv.org/format/2202.09843"}, "title": "Multimode soliton collisions in graded-index optical fibers", "author_info": ["Yifan Sun", "Mario Zitelli", "Mario Ferraro", "Fabio Mangini", "Pedro Parra-Rivas", "Stefan Wabnitz"], "summary": "In this work, we unveil the unique complex dynamics of multimode soliton interactions in graded-index optical fibers through simulations and experiments. By generating two multimode solitons from the fission of an input femtosecond pulse, we examine the evolution of their Raman-induced red-shift when the input pulse energy grows larger. Remarkably, we find that the output red-shift of the trailing multimode soliton may be reduced, so that it accelerates until it collides with the leading multimode soliton. As a result of the inelastic collision, a significant energy transfer occurs between the two multimode solitons: the trailing soliton captures energy from the leading soliton, which ultimately enhances its red-shift, thus increasing temporal separation between the two multimode solitons.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09750", "id": "2202.09750", "pdf": "https://arxiv.org/pdf/2202.09750", "other": "https://arxiv.org/format/2202.09750"}, "title": "Enhancing Affective Representations of Music-Induced EEG through Multimodal Supervision and latent Domain Adaptation", "author_info": ["Kleanthis Avramidis", "Christos Garoufis", "Athanasia Zlatintsi", "Petros Maragos"], "summary": "The study of Music Cognition and neural responses to music has been invaluable in understanding human emotions. Brain signals, though, manifest a highly complex structure that makes processing and retrieving meaningful features challenging, particularly of abstract constructs like affect. Moreover, the performance of learning models is undermined by the limited amount of available neuronal data and their severe inter-subject variability. In this paper we extract efficient, personalized affective representations from EEG signals during music listening. To this end, we employ music signals as a supervisory modality to EEG, aiming to project their semantic correspondence onto a common representation space. We utilize a bi-modal framework by combining an LSTM-based attention model to process EEG and a pre-trained model for music tagging, along with a reverse domain discriminator to align the distributions of the two modalities, further constraining the learning process with emotion tags. The resulting framework can be utilized for emotion recognition both directly, by performing supervised predictions from either modality, and indirectly, by providing relevant music samples to EEG input queries. The experimental findings show the potential of enhancing neuronal data through stimulus information for recognition purposes and yield insights into the distribution and temporal variance of music-induced affective features.", "comment": " Comments: 5 pages, 3 figures, IEEE ICASSP 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09195", "id": "2202.09195", "pdf": "https://arxiv.org/pdf/2202.09195", "other": "https://arxiv.org/format/2202.09195"}, "title": "A Review on Methods and Applications in Multimodal Deep Learning", "author_info": ["Jabeen Summaira", "Xi Li", "Amin Muhammad Shoib", "Jabbar Abdul"], "summary": "Deep Learning has implemented a wide range of applications and has become increasingly popular in recent years. The goal of multimodal deep learning (MMDL) is to create models that can process and link information using various modalities. Despite the extensive development made for unimodal learning, it still cannot cover all the aspects of human learning. Multimodal learning helps to understand and analyze better when various senses are engaged in the processing of information. This paper focuses on multiple types of modalities, i.e., image, video, text, audio, body gestures, facial expressions, and physiological signals. Detailed analysis of the baseline approaches and an in-depth study of recent advancements during the last five years (2017 to 2021) in multimodal deep learning applications has been provided. A fine-grained taxonomy of various multimodal deep learning methods is proposed, elaborating on different applications in more depth. Lastly, main issues are highlighted separately for each domain, along with their possible future research directions.", "comment": " Journal ref:         ACM Transactions on Multimedia Computing, Communications, and Applications 2022       "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09099", "id": "2202.09099", "pdf": "https://arxiv.org/pdf/2202.09099", "other": "https://arxiv.org/format/2202.09099"}, "title": "AMS_ADRN at SemEval-2022 Task 5: A Suitable Image-text Multimodal Joint Modeling Method for Multi-task Misogyny Identification", "author_info": ["Da Li", "Ming Yi", "Yukai He"], "summary": "Women are influential online, especially in image-based social media such as Twitter and Instagram. However, many in the network environment contain gender discrimination and aggressive information, which magnify gender stereotypes and gender inequality. Therefore, the filtering of illegal content such as gender discrimination is essential to maintain a healthy social network environment. In this paper, we describe the system developed by our team for SemEval-2022 Task 5: Multimedia Automatic Misogyny Identification. More specifically, we introduce two novel system to analyze these posts: a multimodal multi-task learning architecture that combines Bertweet for text encoding with ResNet-18 for image representation, and a single-flow transformer structure which combines text embeddings from BERT-Embeddings and image embeddings from several different modules such as EfficientNet and ResNet. In this manner, we show that the information behind them can be properly revealed. Our approach achieves good performance on each of the two subtasks of the current competition, ranking 15th for Subtask A (0.746 macro F1-score), 11th for Subtask B (0.706 macro F1-score) while exceeding the official baseline results by high margins.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08974", "id": "2202.08974", "pdf": "https://arxiv.org/pdf/2202.08974", "other": "https://arxiv.org/format/2202.08974"}, "title": "Multimodal Emotion Recognition using Transfer Learning from Speaker Recognition and BERT-based models", "author_info": ["Sarala Padi", "Seyed Omid Sadjadi", "Dinesh Manocha", "Ram D. Sriram"], "summary": "Automatic emotion recognition plays a key role in computer-human interaction as it has the potential to enrich the next-generation artificial intelligence with emotional intelligence. It finds applications in customer and/or representative behavior analysis in call centers, gaming, personal assistants, and social robots, to mention a few. Therefore, there has been an increasing demand to develop robust automatic methods to analyze and recognize the various emotions. In this paper, we propose a neural network-based emotion recognition framework that uses a late fusion of transfer-learned and fine-tuned models from speech and text modalities. More specifically, we i) adapt a residual network (ResNet) based model trained on a large-scale speaker recognition task using transfer learning along with a spectrogram augmentation approach to recognize emotions from speech, and ii) use a fine-tuned bidirectional encoder representations from transformers (BERT) based model to represent and recognize emotions from the text. The proposed system then combines the ResNet and BERT-based model scores using a late fusion strategy to further improve the emotion recognition performance. The proposed multimodal solution addresses the data scarcity limitation in emotion recognition using transfer learning, data augmentation, and fine-tuning, thereby improving the generalization performance of the emotion recognition models. We evaluate the effectiveness of our proposed multimodal approach on the interactive emotional dyadic motion capture (IEMOCAP) dataset. Experimental results indicate that both audio and text-based models improve the emotion recognition performance and that the proposed multimodal solution achieves state-of-the-art results on the IEMOCAP benchmark.", "comment": " Comments: arXiv admin note: substantial text overlap with arXiv:2108.02510 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08967", "id": "2202.08967", "pdf": "https://arxiv.org/pdf/2202.08967", "other": "https://arxiv.org/format/2202.08967"}, "title": "Ensemble and Multimodal Approach for Forecasting Cryptocurrency Price", "author_info": ["Zeyd Boukhers", "Azeddine Bouabdallah", "Matthias Lohr", "Jan J\u00fcrjens"], "summary": "Since the birth of Bitcoin in 2009, cryptocurrencies have emerged to become a global phenomenon and an important decentralized financial asset. Due to this decentralization, the value of these digital currencies against fiat currencies is highly volatile over time. Therefore, forecasting the crypto-fiat currency exchange rate is an extremely challenging task. For reliable forecasting, this paper proposes a multimodal AdaBoost-LSTM ensemble approach that employs all modalities which derive price fluctuation such as social media sentiments, search volumes, blockchain information, and trading data. To better support investment decision making, the approach forecasts also the fluctuation distribution. The conducted extensive experiments demonstrated the effectiveness of relying on multimodalities instead of only trading data. Further experiments demonstrate the outperformance of the proposed approach compared to existing tools and methods with a 19.29% improvement.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08611", "id": "2202.08611", "pdf": "https://arxiv.org/pdf/2202.08611"}, "title": "Multimodal interference and bound in the continuum modes in indirectly-patterned hyperbolic cavities", "author_info": ["Hanan Herzig Sheinfux", "Lorenzo Orsini", "Minwoo Jung", "Iacopo Torre", "Matteo Ceccanti", "Rinu Maniyara", "David Barcons Ruiz", "Alexander H\u00f6tger", "Ricardo Bertini", "Sebasti\u00e1n Castilla", "Niels C. H. Hesp", "Eli Janzen", "Alexander Holleitner", "Valerio Pruneri", "James H. Edgar", "Gennady Shvets", "Frank H. L. Koppens"], "summary": "A conventional optical cavity supports modes which are confined because they are unable to leak out of the cavity. Bound state in continuum (BIC) cavities are an unconventional alternative, where light can leak out, but is confined by multimodal destructive interference. BICs are a general wave phenomenon, of particular interest to optics, but BICs and multimodal interference have never been demonstrated at the nanoscale. Here, we demonstrate the first nanophotonic cavities based on BIC-like multimodal interference. This novel confinement mechanism for deep sub-wavelength light shows orders of magnitude improvement in several confinement metrics. Specifically, we obtain cavity volumes below 100x100x3nm^3 with quality factors about 100, with extreme cases having 23x23x3nm^3 volumes or quality factors above 400. Key to our approach, is the use of pristine crystalline hyperbolic dispersion media (HyM) which can support large momentum excitations with relatively low losses. Making a HyM cavity is complicated by the additional modes that appear in a HyM. Ordinarily, these serve as additional channels for leakage, reducing cavity performance. But, in our experiments, we find a BIC-like cavity confinement enhancement effect, which is intimately related to the ray-like nature of HyM excitations. In fact, the quality factors of our cavities exceed the maximum that is possible in the absence of higher order modes. The alliance of HyM with BICs in our work yields a radically novel way to confine light and is expected to have far reaching consequences wherever strong optical confinement is utilized, from ultra-strong light-matter interactions, to mid-IR nonlinear optics and a range of sensing applications.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07360", "id": "2202.07360", "pdf": "https://arxiv.org/pdf/2202.07360", "other": "https://arxiv.org/format/2202.07360"}, "title": "Multimodal Driver Referencing: A Comparison of Pointing to Objects Inside and Outside the Vehicle", "author_info": ["Abdul Rafey Aftab", "Michael von der Beeck"], "summary": "Advanced in-cabin sensing technologies, especially vision based approaches, have tremendously progressed user interaction inside the vehicle, paving the way for new applications of natural user interaction. Just as humans use multiple modes to communicate with each other, we follow an approach which is characterized by simultaneously using multiple modalities to achieve natural human-machine interaction for a specific task: pointing to or glancing towards objects inside as well as outside the vehicle for deictic references. By tracking the movements of eye-gaze, head and finger, we design a multimodal fusion architecture using a deep neural network to precisely identify the driver's referencing intent. Additionally, we use a speech command as a trigger to separate each referencing event. We observe differences in driver behavior in the two pointing use cases (i.e. for inside and outside objects), especially when analyzing the preciseness of the three modalities eye, head, and finger. We conclude that there is no single modality that is solely optimal for all cases as each modality reveals certain limitations. Fusion of multiple modalities exploits the relevant characteristics of each modality, hence overcoming the case dependent limitations of each individual modality. Ultimately, we propose a method to identity whether the driver's referenced object lies inside or outside the vehicle, based on the predicted pointing direction.", "comment": " Journal ref:         27th International Conference on Intelligent User Interfaces (IUI '22), March 22--25, 2022, Helsinki, Finland       "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07247", "id": "2202.07247", "pdf": "https://arxiv.org/pdf/2202.07247", "other": "https://arxiv.org/format/2202.07247"}, "title": "CommerceMM: Large-Scale Commerce MultiModal Representation Learning with Omni Retrieval", "author_info": ["Licheng Yu", "Jun Chen", "Animesh Sinha", "Mengjiao MJ Wang", "Hugo Chen", "Tamara L. Berg", "Ning Zhang"], "summary": "We introduce CommerceMM - a multimodal model capable of providing a diverse and granular understanding of commerce topics associated to the given piece of content (image, text, image+text), and having the capability to generalize to a wide range of tasks, including Multimodal Categorization, Image-Text Retrieval, Query-to-Product Retrieval, Image-to-Product Retrieval, etc. We follow the pre-training + fine-tuning training regime and present 5 effective pre-training tasks on image-text pairs. To embrace more common and diverse commerce data with text-to-multimodal, image-to-multimodal, and multimodal-to-multimodal mapping, we propose another 9 novel cross-modal and cross-pair retrieval tasks, called Omni-Retrieval pre-training. The pre-training is conducted in an efficient manner with only two forward/backward updates for the combined 14 tasks. Extensive experiments and analysis show the effectiveness of each task. When combining all pre-training tasks, our model achieves state-of-the-art performance on 7 commerce-related downstream tasks after fine-tuning. Additionally, we propose a novel approach of modality randomization to dynamically adjust our model under different efficiency constraints.", "comment": " Comments: 10 pages, 7 figures. Commerce Multimodal Model towards Real Applications at Facebook "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06839", "id": "2202.06839", "pdf": "https://arxiv.org/pdf/2202.06839", "other": "https://arxiv.org/format/2202.06839"}, "title": "Close-up and Whispering: An Understanding of Multimodal and Parasocial Interactions in YouTube ASMR videos", "author_info": ["Shuo Niu", "Hugh S. Manon", "Ava Bartolome", "Nguyen B. Ha", "Keegan Veazey"], "summary": "ASMR (Autonomous Sensory Meridian Response) has grown to immense popularity on YouTube and drawn HCI designers' attention to its effects and applications in design. YouTube ASMR creators incorporate visual elements, sounds, motifs of touching and tasting, and other scenarios in multisensory video interactions to deliver enjoyable and relaxing experiences to their viewers. ASMRtists engage viewers by social, physical, and task attractions. Research has identified the benefits of ASMR in mental wellbeing. However, ASMR remains an understudied phenomenon in the HCI community, constraining designers' ability to incorporate ASMR in video-based designs. This work annotates and analyzes the interaction modalities and parasocial attractions of 2663 videos to identify unique experiences. YouTube comment sections are also analyzed to compare viewers' responses to different ASMR interactions. We find that ASMR videos are experiences of multimodal social connection, relaxing physical intimacy, and sensory-rich activity observation. Design implications are discussed to foster future ASMR-augmented video interactions.", "comment": " Comments: 4 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06507", "id": "2202.06507", "pdf": "https://arxiv.org/pdf/2202.06507", "other": "https://arxiv.org/format/2202.06507"}, "title": "EMGSE: Acoustic/EMG Fusion for Multimodal Speech Enhancement", "author_info": ["Kuan-Chen Wang", "Kai-Chun Liu", "Hsin-Min Wang", "Yu Tsao"], "summary": "Multimodal learning has been proven to be an effective method to improve speech enhancement (SE) performance, especially in challenging situations such as low signal-to-noise ratios, speech noise, or unseen noise types. In previous studies, several types of auxiliary data have been used to construct multimodal SE systems, such as lip images, electropalatography, or electromagnetic midsagittal articulography. In this paper, we propose a novel EMGSE framework for multimodal SE, which integrates audio and facial electromyography (EMG) signals. Facial EMG is a biological signal containing articulatory movement information, which can be measured in a non-invasive way. Experimental results show that the proposed EMGSE system can achieve better performance than the audio-only SE system. The benefits of fusing EMG signals with acoustic signals for SE are notable under challenging circumstances. Furthermore, this study reveals that cheek EMG is sufficient for SE.", "comment": " Comments: 5 pages, 4 figures, and 3 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06238", "id": "2202.06238", "pdf": "https://arxiv.org/pdf/2202.06238", "other": "https://arxiv.org/format/2202.06238"}, "title": "Multimodal Depression Classification Using Articulatory Coordination Features And Hierarchical Attention Based Text Embeddings", "author_info": ["Nadee Seneviratne", "Carol Espy-Wilson"], "summary": "Multimodal depression classification has gained immense popularity over the recent years. We develop a multimodal depression classification system using articulatory coordination features extracted from vocal tract variables and text transcriptions obtained from an automatic speech recognition tool that yields improvements of area under the receiver operating characteristics curve compared to uni-modal classifiers (7.5% and 13.7% for audio and text respectively). We show that in the case of limited training data, a segment-level classifier can first be trained to then obtain a session-wise prediction without hindering the performance, using a multi-stage convolutional recurrent neural network. A text model is trained using a Hierarchical Attention Network (HAN). The multimodal system is developed by combining embeddings from the session-level audio model and the HAN text model", "comment": " Comments: Accepted to ICASSP 2022. arXiv admin note: text overlap with arXiv:2104.04195 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06218", "id": "2202.06218", "pdf": "https://arxiv.org/pdf/2202.06218", "other": "https://arxiv.org/format/2202.06218"}, "title": "Emotion Based Hate Speech Detection using Multimodal Learning", "author_info": ["Aneri Rana", "Sonali Jha"], "summary": "In recent years, monitoring hate speech and offensive language on social media platforms has become paramount due to its widespread usage among all age groups, races, and ethnicities. Consequently, there have been substantial research efforts towards automated detection of such content using Natural Language Processing (NLP). While successfully filtering textual data, no research has focused on detecting hateful content in multimedia data. With increased ease of data storage and the exponential growth of social media platforms, multimedia content proliferates the internet as much as text data. Nevertheless, it escapes the automatic filtering systems. Hate speech and offensiveness can be detected in multimedia primarily via three modalities, i.e., visual, acoustic, and verbal. Our preliminary study concluded that the most essential features in classifying hate speech would be the speaker's emotional state and its influence on the spoken words, therefore limiting our current research to these modalities. This paper proposes the first multimodal deep learning framework to combine the auditory features representing emotion and the semantic features to detect hateful content. Our results demonstrate that incorporating emotional attributes leads to significant improvement over text-based models in detecting hateful multimedia content. This paper also presents a new Hate Speech Detection Video Dataset (HSDVD) collected for the purpose of multimodal learning as no such dataset exists today.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06076", "id": "2202.06076", "pdf": "https://arxiv.org/pdf/2202.06076", "other": "https://arxiv.org/format/2202.06076"}, "title": "Indication as Prior Knowledge for Multimodal Disease Classification in Chest Radiographs with Transformers", "author_info": ["Grzegorz Jacenk\u00f3w", "Alison Q. O'Neil", "Sotirios A. Tsaftaris"], "summary": "When a clinician refers a patient for an imaging exam, they include the reason (e.g. relevant patient history, suspected disease) in the scan request; this appears as the indication field in the radiology report. The interpretation and reporting of the image are substantially influenced by this request text, steering the radiologist to focus on particular aspects of the image. We use the indication field to drive better image classification, by taking a transformer network which is unimodally pre-trained on text (BERT) and fine-tuning it for multimodal classification of a dual image-text input. We evaluate the method on the MIMIC-CXR dataset, and present ablation studies to investigate the effect of the indication field on the classification performance. The experimental results show our approach achieves 87.8 average micro AUROC, outperforming the state-of-the-art methods for unimodal (84.4) and multimodal (86.0) classification. Our code is available at https://github.com/jacenkow/mmbt.", "comment": " Comments: Accepted at the IEEE International Symposium on Biomedical Imaging (ISBI) 2022 as an oral presentation "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05025", "id": "2202.05025", "pdf": "https://arxiv.org/pdf/2202.05025", "other": "https://arxiv.org/format/2202.05025"}, "title": "Two-colour spectrally multimode integrated SU(1,1) interferometer", "author_info": ["Alessandro Ferreri", "Polina R. Sharapova"], "summary": "Multimode integrated interferometers have great potential for both spectral engineering and metrological applications. However, material dispersion of integrated platforms constitutes an obstacle which limits the performance and precision of such interferometers. At the same time, two-colour non-linear interferometers present an important tool for metrological applications, when measurements in a certain frequency range are difficult. In this manuscript, we theoretically develop and investigate an integrated multimode two-colour SU(1,1) interferometer that operates in a supersensitive mode. By ensuring a proper design of the integrated platform, we suppress dispersion and thereby significantly increase the visibility of the interference pattern. We demonstrate that such an interferometer overcomes the classical phase sensitivity limit for wide parametric gain ranges, when up to 3\u2217104 photons are generated.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.04528", "id": "2202.04528", "pdf": "https://arxiv.org/pdf/2202.04528", "other": "https://arxiv.org/format/2202.04528"}, "title": "A Multimodal Canonical-Correlated Graph Neural Network for Energy-Efficient Speech Enhancement", "author_info": ["Leandro Aparecido Passos", "Jo\u00e3o Paulo Papa", "Amir Hussain", "Ahsan Adeel"], "summary": "This paper proposes a novel multimodal self-supervised architecture for energy-efficient AV speech enhancement by integrating graph neural networks with canonical correlation analysis (CCA-GNN). This builds on a state-of-the-art CCA-GNN that aims to learn representative embeddings by maximizing the correlation between pairs of augmented views of the same input while decorrelating disconnected features. The key idea of the conventional CCA-GNN involves discarding augmentation-variant information and preserving augmentation-invariant information whilst preventing capturing of redundant information. Our proposed AV CCA-GNN model is designed to deal with the challenging multimodal representation learning context. Specifically, our model improves contextual AV speech processing by maximizing canonical correlation from augmented views of the same channel, as well as canonical correlation from audio and visual embeddings. In addition, we propose a positional encoding of the nodes that considers a prior-frame sequence distance instead of a feature-space representation while computing the node's nearest neighbors. This serves to introduce temporal information in the embeddings through the neighborhood's connectivity. Experiments conducted with the benchmark ChiME3 dataset show that our proposed prior frame-based AV CCA-GNN reinforces better feature learning in the temporal context, leading to more energy-efficient speech reconstruction compared to state-of-the-art CCA-GNN and multi-layer perceptron models. The results demonstrate the potential of our proposed approach for exploitation in future assistive technology and energy-efficient multimodal devices.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.04426", "id": "2202.04426", "pdf": "https://arxiv.org/pdf/2202.04426", "other": "https://arxiv.org/format/2202.04426"}, "title": "Deep Feature Rotation for Multimodal Image Style Transfer", "author_info": ["Son Truong Nguyen", "Nguyen Quang Tuyen", "Nguyen Hong Phuc"], "summary": "Recently, style transfer is a research area that attracts a lot of attention, which transfers the style of an image onto a content target. Extensive research on style transfer has aimed at speeding up processing or generating high-quality stylized images. Most approaches only produce an output from a content and style image pair, while a few others use complex architectures and can only produce a certain number of outputs. In this paper, we propose a simple method for representing style features in many ways called Deep Feature Rotation (DFR), while not only producing diverse outputs but also still achieving effective stylization compared to more complex methods. Our approach is representative of the many ways of augmentation for intermediate feature embedding without consuming too much computational expense. We also analyze our method by visualizing output in different rotation weights. Our code is available at https://github.com/sonnguyen129/deep-feature-rotation.", "comment": " Comments: Accepted to NICS'21 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.04303", "id": "2202.04303", "pdf": "https://arxiv.org/pdf/2202.04303", "other": "https://arxiv.org/format/2202.04303"}, "title": "TinyM2Net: A Flexible System Algorithm Co-designed Multimodal Learning Framework for Tiny Devices", "author_info": ["Hasib-Al Rashid", "Pretom Roy Ovi", "Aryya Gangopadhyay", "Tinoosh Mohsenin"], "summary": "With the emergence of Artificial Intelligence (AI), new attention has been given to implement AI algorithms on resource constrained tiny devices to expand the application domain of IoT. Multimodal Learning has recently become very popular with the classification task due to its impressive performance for both image and audio event classification. This paper presents TinyM2Net -- a flexible system algorithm co-designed multimodal learning framework for resource constrained tiny devices. The framework was designed to be evaluated on two different case-studies: COVID-19 detection from multimodal audio recordings and battle field object detection from multimodal images and audios. In order to compress the model to implement on tiny devices, substantial network architecture optimization and mixed precision quantization were performed (mixed 8-bit and 4-bit). TinyM2Net shows that even a tiny multimodal learning model can improve the classification performance than that of any unimodal frameworks. The most compressed TinyM2Net achieves 88.4% COVID-19 detection accuracy (14.5% improvement from unimodal base model) and 96.8\\% battle field object detection accuracy (3.9% improvement from unimodal base model). Finally, we test our TinyM2Net models on a Raspberry Pi 4 to see how they perform when deployed to a resource constrained tiny device.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.04266", "id": "2202.04266", "pdf": "https://arxiv.org/pdf/2202.04266", "other": "https://arxiv.org/format/2202.04266"}, "title": "MMLN: Leveraging Domain Knowledge for Multimodal Diagnosis", "author_info": ["Haodi Zhang", "Chenyu Xu", "Peirou Liang", "Ke Duan", "Hao Ren", "Weibin Cheng", "Kaishun Wu"], "summary": "Recent studies show that deep learning models achieve good performance on medical imaging tasks such as diagnosis prediction. Among the models, multimodality has been an emerging trend, integrating different forms of data such as chest X-ray (CXR) images and electronic medical records (EMRs). However, most existing methods incorporate them in a model-free manner, which lacks theoretical support and ignores the intrinsic relations between different data sources. To address this problem, we propose a knowledge-driven and data-driven framework for lung disease diagnosis. By incorporating domain knowledge, machine learning models can reduce the dependence on labeled data and improve interpretability. We formulate diagnosis rules according to authoritative clinical medicine guidelines and learn the weights of rules from text data. Finally, a multimodal fusion consisting of text and image data is designed to infer the marginal probability of lung disease. We conduct experiments on a real-world dataset collected from a hospital. The results show that the proposed method outperforms the state-of-the-art multimodal baselines in terms of accuracy and interpretability.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03954", "id": "2202.03954", "pdf": "https://arxiv.org/pdf/2202.03954", "other": "https://arxiv.org/format/2202.03954"}, "title": "Social-DualCVAE: Multimodal Trajectory Forecasting Based on Social Interactions Pattern Aware and Dual Conditional Variational Auto-Encoder", "author_info": ["Jiashi Gao", "Xinming Shi", "James J. Q. Yu"], "summary": "Pedestrian trajectory forecasting is a fundamental task in multiple utility areas, such as self-driving, autonomous robots, and surveillance systems. The future trajectory forecasting is multi-modal, influenced by physical interaction with scene contexts and intricate social interactions among pedestrians. The mainly existing literature learns representations of social interactions by deep learning networks, while the explicit interaction patterns are not utilized. Different interaction patterns, such as following or collision avoiding, will generate different trends of next movement, thus, the awareness of social interaction patterns is important for trajectory forecasting. Moreover, the social interaction patterns are privacy concerned or lack of labels. To jointly address the above issues, we present a social-dual conditional variational auto-encoder (Social-DualCVAE) for multi-modal trajectory forecasting, which is based on a generative model conditioned not only on the past trajectories but also the unsupervised classification of interaction patterns. After generating the category distribution of the unlabeled social interaction patterns, DualCVAE, conditioned on the past trajectories and social interaction pattern, is proposed for multi-modal trajectory prediction by latent variables estimating. A variational bound is derived as the minimization objective during training. The proposed model is evaluated on widely used trajectory benchmarks and outperforms the prior state-of-the-art methods.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03775", "id": "2202.03775", "pdf": "https://arxiv.org/pdf/2202.03775", "other": "https://arxiv.org/format/2202.03775"}, "title": "Addressing Data Scarcity in Multimodal User State Recognition by Combining Semi-Supervised and Supervised Learning", "author_info": ["Hendric Vo\u00df", "Heiko Wersing", "Stefan Kopp"], "summary": "Detecting mental states of human users is crucial for the development of cooperative and intelligent robots, as it enables the robot to understand the user's intentions and desires. Despite their importance, it is difficult to obtain a large amount of high quality data for training automatic recognition algorithms as the time and effort required to collect and label such data is prohibitively high. In this paper we present a multimodal machine learning approach for detecting dis-/agreement and confusion states in a human-robot interaction environment, using just a small amount of manually annotated data. We collect a data set by conducting a human-robot interaction study and develop a novel preprocessing pipeline for our machine learning approach. By combining semi-supervised and supervised architectures, we are able to achieve an average F1-score of 81.1\\% for dis-/agreement detection with a small amount of labeled data and a large unlabeled data set, while simultaneously increasing the robustness of the model compared to the supervised approach.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03587", "id": "2202.03587", "pdf": "https://arxiv.org/pdf/2202.03587", "other": "https://arxiv.org/format/2202.03587"}, "title": "CALM: Contrastive Aligned Audio-Language Multirate and Multimodal Representations", "author_info": ["Vin Sachidananda", "Shao-Yen Tseng", "Erik Marchi", "Sachin Kajarekar", "Panayiotis Georgiou"], "summary": "Deriving multimodal representations of audio and lexical inputs is a central problem in Natural Language Understanding (NLU). In this paper, we present Contrastive Aligned Audio-Language Multirate and Multimodal Representations (CALM), an approach for learning multimodal representations using contrastive and multirate information inherent in audio and lexical inputs. The proposed model aligns acoustic and lexical information in the input embedding space of a pretrained language-only contextual embedding model. By aligning audio representations to pretrained language representations and utilizing contrastive information between acoustic inputs, CALM is able to bootstrap audio embedding competitive with existing audio representation models in only a few hours of training time. Operationally, audio spectrograms are processed using linearized patches through a Spectral Transformer (SpecTran) which is trained using a Contrastive Audio-Language Pretraining objective to align audio and language from similar queries. Subsequently, the derived acoustic and lexical tokens representations are input into a multimodal transformer to incorporate utterance level context and derive the proposed CALM representations. We show that these pretrained embeddings can subsequently be used in multimodal supervised tasks and demonstrate the benefits of the proposed pretraining steps in terms of the alignment of the two embedding spaces and the multirate nature of the pretraining. Our system shows 10-25\\% improvement over existing emotion recognition systems including state-of-the-art three-modality systems under various evaluation objectives.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03390", "id": "2202.03390", "pdf": "https://arxiv.org/pdf/2202.03390", "other": "https://arxiv.org/format/2202.03390"}, "title": "GMC -- Geometric Multimodal Contrastive Representation Learning", "author_info": ["Petra Poklukar", "Miguel Vasco", "Hang Yin", "Francisco S. Melo", "Ana Paiva", "Danica Kragic"], "summary": "Learning representations of multimodal data that are both informative and robust to missing modalities at test time remains a challenging problem due to the inherent heterogeneity of data obtained from different channels. To address it, we present a novel Geometric Multimodal Contrastive (GMC) representation learning method comprised of two main components: i) a two-level architecture consisting of modality-specific base encoder, allowing to process an arbitrary number of modalities to an intermediate representation of fixed dimensionality, and a shared projection head, mapping the intermediate representations to a latent representation space; ii) a multimodal contrastive loss function that encourages the geometric alignment of the learned representations. We experimentally demonstrate that GMC representations are semantically rich and achieve state-of-the-art performance with missing modality information on three different learning problems including prediction and reinforcement learning tasks.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03283", "id": "2202.03283", "pdf": "https://arxiv.org/pdf/2202.03283", "other": "https://arxiv.org/format/2202.03283"}, "title": "CZU-MHAD: A multimodal dataset for human action recognition utilizing a depth camera and 10 wearable inertial sensors", "author_info": ["Xin Chao", "Zhenjie Hou", "Yujian Mo"], "summary": "Human action recognition has been widely used in many fields of life, and many human action datasets have been published at the same time. However, most of the multi-modal databases have some shortcomings in the layout and number of sensors, which cannot fully represent the action features. Regarding the problems, this paper proposes a freely available dataset, named CZU-MHAD (Changzhou University: a comprehensive multi-modal human action dataset). It consists of 22 actions and three modals temporal synchronized data. These modals include depth videos and skeleton positions from a kinect v2 camera, and inertial signals from 10 wearable sensors. Compared with single modal sensors, multi-modal sensors can collect different modal data, so the use of multi-modal sensors can describe actions more accurately. Moreover, CZU-MHAD obtains the 3-axis acceleration and 3-axis angular velocity of 10 main motion joints by binding inertial sensors to them, and these data were captured at the same time. Experimental results are provided to show that this dataset can be used to study structural relationships between different parts of the human body when performing actions and fusion approaches that involve multi-modal sensor data.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03242", "id": "2202.03242", "pdf": "https://arxiv.org/pdf/2202.03242", "other": "https://arxiv.org/format/2202.03242"}, "title": "Unsupervised physics-informed disentanglement of multimodal data for high-throughput scientific discovery", "author_info": ["Nathaniel Trask", "Carianne Martinez", "Kookjin Lee", "Brad Boyce"], "summary": "We introduce physics-informed multimodal autoencoders (PIMA) - a variational inference framework for discovering shared information in multimodal scientific datasets representative of high-throughput testing. Individual modalities are embedded into a shared latent space and fused through a product of experts formulation, enabling a Gaussian mixture prior to identify shared features. Sampling from clusters allows cross-modal generative modeling, with a mixture of expert decoder imposing inductive biases encoding prior scientific knowledge and imparting structured disentanglement of the latent space. This approach enables discovery of fingerprints which may be detected in high-dimensional heterogeneous datasets, avoiding traditional bottlenecks related to high-fidelity measurement and characterization. Motivated by accelerated co-design and optimization of materials manufacturing processes, a dataset of lattice metamaterials from metal additive manufacturing demonstrates accurate cross modal inference between images of mesoscale topology and mechanical stress-strain response.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03189", "id": "2202.03189", "pdf": "https://arxiv.org/pdf/2202.03189", "other": "https://arxiv.org/format/2202.03189"}, "title": "Optical skin: Sensor-integration-free multimodal flexible sensing", "author_info": ["Sho Shimadera", "Kei Kitagawa", "Koyo Sagehashi", "Tomoaki Niiyama", "Satoshi Sunada"], "summary": "The biological skin enables animals to sense various stimuli. Extensive efforts have been made recently to develop smart skin-like sensors to extend the capabilities of biological skins; however, simultaneous sensing of several types of stimuli in a large area remains challenging because this requires large-scale sensor integration with numerous wire connections. We propose a simple, highly sensitive, and multimodal sensing approach, which does not require integrating multiple sensors. The proposed approach is based on an optical interference technique, which can encode the information of various stimuli as a spatial pattern. In contrast to the existing approach, the proposed approach, combined with a deep neural network, enables us to freely select the sensing mode according to our purpose. As a key example, we demonstrate simultaneous sensing mode of three different physical quantities, contact force, contact location, and temperature, using a single soft material without requiring complex integration. Another unique property of the proposed approach is spatially continuous sensing with ultrahigh resolution of few tens of micrometers, which enables identifying the shape of the object in contact. Furthermore, we present a haptic soft device for a human-machine interface. The proposed approach encourages the development of high-performance optical skins.", "comment": " Comments: 13 pages, 11 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.02027", "id": "2202.02027", "pdf": "https://arxiv.org/pdf/2202.02027", "other": "https://arxiv.org/format/2202.02027"}, "title": "Engineering multifunctionality at oxide interfaces by multimode coupling", "author_info": ["Monirul Shaikh", "Saurabh Ghosh"], "summary": "We employed first-principles density functional theory calculations guided by group-theoretical analysis and demonstrated the control of insulator-metal-insulator transition, polarization and two sublattice magnetization in (LaFeO3)1/(CaFeO3)1 superlattice via. multi structural mode coupling i.e., 'multimode coupling'. We have discovered a polar A-type charge disproportionation mode, QACD (analogous to the A-type antiferromagnetic ordering), and found that it couples with the trilinear coupling, QTri mode (common in Pnma perovskite oxides and involves three structural modes), and lowers the symmetry further. By tuning the strength of the coupling between the participating modes, the polar metallic phase, polar zero bandgap semiconducting, and polar insulating phases can be obtained. Here, QTri switches the polarization direction, whereas, QACD can trigger insulator-metal-insulator transition along with the polarization switching. The mechanism is true for any transition metal superlattices constituted with Pnma building blocks and with partially filled eg or t2g electron(s) at the transition metal sites.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.01439", "id": "2202.01439", "pdf": "https://arxiv.org/pdf/2202.01439"}, "title": "Sensing the Breath: A Multimodal Singing Tutoring Interface with Breath Guidance", "author_info": ["Ziyue Piao", "Gus Xia"], "summary": "Breath is a significant component in singing performance, which is still underresearched in most singing-related music interfaces. In this paper, we present a multimodal system that detects the learner's singing pitch and breathing states and provides real-time visual tutoring feedback. Specifically, the breath detector is a wearable belt with pressure sensors and flexible fabric. It monitors real-time body movement of the abdomen, back waist, and twin ribs. A breath visualization algorithm is developed to display real-time breath states, together with the singing pitch contours on an interactive score interface. User studies show that our system can help users not only gain deeper breath during singing but also improve pitch accuracy in vocal training, especially for those with some musical background.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.01072", "id": "2202.01072", "pdf": "https://arxiv.org/pdf/2202.01072", "other": "https://arxiv.org/format/2202.01072"}, "title": "Interpretability for Multimodal Emotion Recognition using Concept Activation Vectors", "author_info": ["Ashish Ramayee Asokan", "Nidarshan Kumar", "Anirudh Venkata Ragam", "Shylaja S Sharath"], "summary": "Multimodal Emotion Recognition refers to the classification of input video sequences into emotion labels based on multiple input modalities (usually video, audio and text). In recent years, Deep Neural networks have shown remarkable performance in recognizing human emotions, and are on par with human-level performance on this task. Despite the recent advancements in this field, emotion recognition systems are yet to be accepted for real world setups due to the obscure nature of their reasoning and decision-making process. Most of the research in this field deals with novel architectures to improve the performance for this task, with a few attempts at providing explanations for these models' decisions. In this paper, we address the issue of interpretability for neural networks in the context of emotion recognition using Concept Activation Vectors (CAVs). To analyse the model's latent space, we define human-understandable concepts specific to Emotion AI and map them to the widely-used IEMOCAP multimodal database. We then evaluate the influence of our proposed concepts at multiple layers of the Bi-directional Contextual LSTM (BC-LSTM) network to show that the reasoning process of neural networks for emotion recognition can be represented using human-understandable concepts. Finally, we perform hypothesis testing on our proposed concepts to show that they are significant for interpretability of this task.", "comment": " Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.00468", "id": "2202.00468", "pdf": "https://arxiv.org/pdf/2202.00468", "other": "https://arxiv.org/format/2202.00468"}, "title": "Unified Multimodal Punctuation Restoration Framework for Mixed-Modality Corpus", "author_info": ["Yaoming Zhu", "Liwei Wu", "Shanbo Cheng", "Mingxuan Wang"], "summary": "The punctuation restoration task aims to correctly punctuate the output transcriptions of automatic speech recognition systems. Previous punctuation models, either using text only or demanding the corresponding audio, tend to be constrained by real scenes, where unpunctuated sentences are a mixture of those with and without audio. This paper proposes a unified multimodal punctuation restoration framework, named UniPunc, to punctuate the mixed sentences with a single model. UniPunc jointly represents audio and non-audio samples in a shared latent space, based on which the model learns a hybrid representation and punctuates both kinds of samples. We validate the effectiveness of the UniPunc on real-world datasets, which outperforms various strong baselines (e.g. BERT, MuSe) by at least 0.8 overall F1 scores, making a new state-of-the-art. Extensive experiments show that UniPunc's design is a pervasive solution: by grafting onto previous models, UniPunc enables them to punctuate on the mixed corpus. Our code is available at github.com/Yaoming95/UniPunc", "comment": " Comments: 5 pages, accepted by ICASSP'2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.12925", "id": "2201.12925", "pdf": "https://arxiv.org/pdf/2201.12925", "other": "https://arxiv.org/format/2201.12925"}, "title": "Multimodal Maximum Entropy Dynamic Games", "author_info": ["Oswin So", "Kyle Stachowicz", "Evangelos A. Theodorou"], "summary": "Environments with multi-agent interactions often result a rich set of modalities of behavior between agents due to the inherent suboptimality of decision making processes when agents settle for satisfactory decisions. However, existing algorithms for solving these dynamic games are strictly unimodal and fail to capture the intricate multimodal behaviors of the agents. In this paper, we propose MMELQGames (Multimodal Maximum-Entropy Linear Quadratic Games), a novel constrained multimodal maximum entropy formulation of the Differential Dynamic Programming algorithm for solving generalized Nash equilibria. By formulating the problem as a certain dynamic game with incomplete and asymmetric information where agents are uncertain about the cost and dynamics of the game itself, the proposed method is able to reason about multiple local generalized Nash equilibria, enforce constraints with the Augmented Lagrangian framework and also perform Bayesian inference on the latent mode from past observations. We assess the efficacy of the proposed algorithm on two illustrative examples: multi-agent collision avoidance and autonomous racing. In particular, we show that only MMELQGames is able to effectively block a rear vehicle when given a speed disadvantage and the rear vehicle can overtake from multiple positions.", "comment": " Comments: Under review for RSS 2022. Supplementary Video: https://youtu.be/7molN_Q38dk "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.12337", "id": "2201.12337", "pdf": "https://arxiv.org/pdf/2201.12337", "other": "https://arxiv.org/format/2201.12337"}, "title": "Encoding qubits in multimode grid states", "author_info": ["Baptiste Royer", "Shraddha Singh", "Steven M. Girvin"], "summary": "Encoding logical quantum information in harmonic oscillator modes is a promising and hardware-efficient approach to the realization of a quantum computer. In this work, we propose to encode logical qubits in grid states of an ensemble of harmonic oscillator modes. We first discuss general results about these multimode bosonic codes; how to design them, how to practically implement them in different experimental platforms and how lattice symmetries can be leveraged to perform logical non-Clifford operations. We then introduce in detail two two-mode grid codes based on the hypercubic and D4 lattices, respectively, showing how to perform a universal set of logical operations. We demonstrate numerically that multimode grid codes have, compared to their single-mode counterpart, increased robustness against propagation of errors from ancillas used for error correction. Finally, we highlight some interesting links between multidimensional lattices and single-mode grid codes concatenated with qubit codes.", "comment": " Comments: 38 pages, 13 figures v2: Added missing reference, Updated inaccurate statement in Sect. 6, Updated the note added "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11677", "id": "2201.11677", "pdf": "https://arxiv.org/pdf/2201.11677", "other": "https://arxiv.org/format/2201.11677"}, "title": "Parallel black-box optimization of expensive high-dimensional multimodal functions via magnitude", "author_info": ["Steve Huntsman"], "summary": "Building on the recently developed theory of magnitude, we introduce the optimization algorithm EXPLO2 and carefully benchmark it. EXPLO2 advances the state of the art for optimizing high-dimensional (D\u2a8640) multimodal functions that are expensive to compute and for which derivatives are not available, such as arise in hyperparameter optimization or via simulations.", "comment": " MSC Class:           90-08                              ACM Class:           G.1.6                "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11559", "id": "2201.11559", "pdf": "https://arxiv.org/pdf/2201.11559", "other": "https://arxiv.org/format/2201.11559"}, "title": "A thermodynamic description of the near- and far-field intensity patterns emerging from multimode nonlinear waveguide arrays", "author_info": ["Mahmoud A. Selim", "Fan O. Wu", "Huizhong Ren", "Mercedeh Khajavikhan", "Demetrios Christodoulides"], "summary": "Nonlinear highly multimode photonic systems are ubiquitous in optics. Yet, the sheer complexity arising from the action of nonlinearity in multimode environments has posed theoretical challenges in describing these systems. In this work, we deploy concepts from optical thermodynamics to investigate the near- and far-field emission intensity patterns emerging from nonlinear waveguide arrays. An exact equation dictating the response of a nonlinear array is derived in terms of the systems invariants that act as extensive thermodynamic variables. In this respect, the near- and far-field characteristics emerging from a weakly nonlinear waveguide lattice are analytically addressed. We show that statistically, these patterns and the resulting far-field brightness are governed by the optical temperature and its corresponding chemical potential. The extensivity associated with the entropy of such configurations is discussed. The thermodynamic results presented here were found to be in good agreement with numerical simulations obtained from nonlinear coupled-mode theory.", "comment": " Journal ref:         Physical Review A, 105(1), 013514 (2022)       "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11517", "id": "2201.11517", "pdf": "https://arxiv.org/pdf/2201.11517", "other": "https://arxiv.org/format/2201.11517"}, "title": "Multimodal neural networks better explain multivoxel patterns in the hippocampus", "author_info": ["Bhavin Choksi", "Milad Mozafari", "Rufin VanRullen", "Leila Reddy"], "summary": "The human hippocampus possesses \"concept cells\", neurons that fire when presented with stimuli belonging to a specific concept, regardless of the modality. Recently, similar concept cells were discovered in a multimodal network called CLIP (Radford et at., 2021). Here, we ask whether CLIP can explain the fMRI activity of the human hippocampus better than a purely visual (or linguistic) model. We extend our analysis to a range of publicly available uni- and multi-modal models. We demonstrate that \"multimodality\" stands out as a key component when assessing the ability of a network to explain the multivoxel activity in the hippocampus.", "comment": " Comments: Oral at SVRHM Workshop (NeurIPS 2021) "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11382", "id": "2201.11382", "pdf": "https://arxiv.org/pdf/2201.11382", "other": "https://arxiv.org/format/2201.11382"}, "title": "Enabling Radio Sensing for Multimodal Intelligent Transportation Systems: From Virtual Testing to Immersive Testbeds", "author_info": ["Paul Schwarzbach", "Jonas Ninnemann", "Oliver Michler"], "summary": "In this paper, the necessity for application-oriented development and evaluation of Joint Communication and Sensing (JC&S) applications, especially in transportation, is addressed. More specifically, an integrative evaluation chain for immersively testing JC&S location capabilities, reaching from early-stage testing, over model- and scenario-enabled ray tracing simulation, to real-world evaluation (laboratory and field testing) is presented. This includes a discussion of both challenges and requirements for location-aware applications in Intelligent Transportation Systems. Within this scope, a reproducible methodology for testing sensing and localization capabilities is derived and application scenarios are presented. This includes a proposal of a scenario-based sensing evaluation using radio propagation simulation. The paper empirically discusses a proof-of-concept of the developed method given a smart parking scenario, in which a passive occupancy detection of vehicles is performed. The conducted findings underline the need for scenario-based JC&S evaluation in both virtual and real-world environments and proposes consecutive research work.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.10274", "id": "2201.10274", "pdf": "https://arxiv.org/pdf/2201.10274", "other": "https://arxiv.org/format/2201.10274"}, "title": "Multi-channel Attentive Graph Convolutional Network With Sentiment Fusion For Multimodal Sentiment Analysis", "author_info": ["Luwei Xiao", "Xingjiao Wu", "Wen Wu", "Jing Yang", "Liang He"], "summary": "Nowadays, with the explosive growth of multimodal reviews on social media platforms, multimodal sentiment analysis has recently gained popularity because of its high relevance to these social media posts. Although most previous studies design various fusion frameworks for learning an interactive representation of multiple modalities, they fail to incorporate sentimental knowledge into inter-modality learning. This paper proposes a Multi-channel Attentive Graph Convolutional Network (MAGCN), consisting of two main components: cross-modality interactive learning and sentimental feature fusion. For cross-modality interactive learning, we exploit the self-attention mechanism combined with densely connected graph convolutional networks to learn inter-modality dynamics. For sentimental feature fusion, we utilize multi-head self-attention to merge sentimental knowledge into inter-modality feature representations. Extensive experiments are conducted on three widely-used datasets. The experimental results demonstrate that the proposed model achieves competitive performance on accuracy and F1 scores compared to several state-of-the-art approaches.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.10108", "id": "2201.10108", "pdf": "https://arxiv.org/pdf/2201.10108", "other": "https://arxiv.org/format/2201.10108"}, "title": "LP-UIT: A Multimodal Framework for Link Prediction in Social Networks", "author_info": ["Huizi Wu", "Shiyi Wang", "Hui Fang"], "summary": "With the rapid information explosion on online social network sites (SNSs), it becomes difficult for users to seek new friends or broaden their social networks in an efficient way. Link prediction, which can effectively conquer this problem, has thus attracted wide attention. Previous methods on link prediction fail to comprehensively capture the factors leading to new link formation: 1) few models have considered the varied impacts of users' short-term and long-term interests on link prediction. Besides, they fail to jointly model the influence from social influence and \"weak links\"; 2) considering that different factors should be derived from information sources of different modalities, there is a lack of effective multi-modal framework for link prediction. In this view, we propose a novel multi-modal framework for link prediction (referred as LP-UIT) which fuses a comprehensive set of features (i.e., user information and topological features) extracted from multi-modal information (i.e., textual information, graph information, and numerical information). Specifically, we adopt graph convolutional network to process the network information to capture topological features, employ natural language processing techniques (i.e., TF-IDF and word2Vec) to model users' short-term and long-term interests, and identify social influence and \"weak links\" from numerical features. We further use an attention mechanism to model the relationship between textual and topological features. Finally, a multi-layer perceptron (MLP) is designed to combine the representations from three modalities for link prediction. Extensive experiments on two real-world datasets demonstrate the superiority of LP-UIT over the state-of-the-art methods.", "comment": " Comments: 8 pages, 3 figures, Accepted at IEEE TrustCom2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.09862", "id": "2201.09862", "pdf": "https://arxiv.org/pdf/2201.09862", "other": "https://arxiv.org/format/2201.09862"}, "title": "Learning to Act with Affordance-Aware Multimodal Neural SLAM", "author_info": ["Zhiwei Jia", "Kaixiang Lin", "Yizhou Zhao", "Qiaozi Gao", "Govind Thattai", "Gaurav Sukhatme"], "summary": "Recent years have witnessed an emerging paradigm shift toward embodied artificial intelligence, in which an agent must learn to solve challenging tasks by interacting with its environment. There are several challenges in solving embodied multimodal tasks, including long-horizon planning, vision-and-language grounding, and efficient exploration. We focus on a critical bottleneck, namely the performance of planning and navigation. To tackle this challenge, we propose a Neural SLAM approach that, for the first time, utilizes several modalities for exploration, predicts an affordance-aware semantic map, and plans over it at the same time. This significantly improves exploration efficiency, leads to robust long-horizon planning, and enables effective vision-and-language grounding. With the proposed Affordance-aware Multimodal Neural SLAM (AMSLAM) approach, we obtain more than 40% improvement over prior published work on the ALFRED benchmark and set a new state-of-the-art generalization performance at a success rate of 23.48% on the test unseen scenes.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.09828", "id": "2201.09828", "pdf": "https://arxiv.org/pdf/2201.09828", "other": "https://arxiv.org/format/2201.09828"}, "title": "MMLatch: Bottom-up Top-down Fusion for Multimodal Sentiment Analysis", "author_info": ["Georgios Paraskevopoulos", "Efthymios Georgiou", "Alexandros Potamianos"], "summary": "Current deep learning approaches for multimodal fusion rely on bottom-up fusion of high and mid-level latent modality representations (late/mid fusion) or low level sensory inputs (early fusion). Models of human perception highlight the importance of top-down fusion, where high-level representations affect the way sensory inputs are perceived, i.e. cognition affects perception. These top-down interactions are not captured in current deep learning models. In this work we propose a neural architecture that captures top-down cross-modal interactions, using a feedback mechanism in the forward pass during network training. The proposed mechanism extracts high-level representations for each modality and uses these representations to mask the sensory inputs, allowing the model to perform top-down feature masking. We apply the proposed model for multimodal sentiment recognition on CMU-MOSEI. Our method shows consistent improvements over the well established MulT and over our strong late fusion baseline, achieving state-of-the-art results.", "comment": " Comments: Accepted, ICASSP 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.09421", "id": "2201.09421", "pdf": "https://arxiv.org/pdf/2201.09421", "other": "https://arxiv.org/format/2201.09421"}, "title": "Mutual Attention-based Hybrid Dimensional Network for Multimodal Imaging Computer-aided Diagnosis", "author_info": ["Yin Dai", "Yifan Gao", "Fayu Liu", "Jun Fu"], "summary": "Recent works on Multimodal 3D Computer-aided diagnosis have demonstrated that obtaining a competitive automatic diagnosis model when a 3D convolution neural network (CNN) brings more parameters and medical images are scarce remains nontrivial and challenging. Considering both consistencies of regions of interest in multimodal images and diagnostic accuracy, we propose a novel mutual attention-based hybrid dimensional network for MultiModal 3D medical image classification (MMNet). The hybrid dimensional network integrates 2D CNN with 3D convolution modules to generate deeper and more informative feature maps, and reduce the training complexity of 3D fusion. Besides, the pre-trained model of ImageNet can be used in 2D CNN, which improves the performance of the model. The stereoscopic attention is focused on building rich contextual interdependencies of the region in 3D medical images. To improve the regional correlation of pathological tissues in multimodal medical images, we further design a mutual attention framework in the network to build the region-wise consistency in similar stereoscopic regions of different image modalities, providing an implicit manner to instruct the network to focus on pathological tissues. MMNet outperforms many previous solutions and achieves results competitive to the state-of-the-art on three multimodal imaging datasets, i.e., Parotid Gland Tumor (PGT) dataset, the MRNet dataset, and the PROSTATEx dataset, and its advantages are validated by extensive experiments.", "comment": " Comments: 11 pages, 8 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.09324", "id": "2201.09324", "pdf": "https://arxiv.org/pdf/2201.09324", "other": "https://arxiv.org/format/2201.09324"}, "title": "Supervised Visual Attention for Simultaneous Multimodal Machine Translation", "author_info": ["Veneta Haralampieva", "Ozan Caglayan", "Lucia Specia"], "summary": "Recently, there has been a surge in research in multimodal machine translation (MMT), where additional modalities such as images are used to improve translation quality of textual systems. A particular use for such multimodal systems is the task of simultaneous machine translation, where visual context has been shown to complement the partial information provided by the source sentence, especially in the early phases of translation (Caglayanet al., 2020a; Imankulova et al., 2020). In this paper, we propose the first Transformer-based simultaneous MMT architecture, which has not been previously explored in the field. Additionally, we extend this model with an auxiliary supervision signal that guides its visual attention mechanism using labelled phrase-region alignments. We perform comprehensive experiments on three language directions and conduct thorough quantitative and qualitative analyses using both automatic metrics and manual inspection. Our results show that (i) supervised visual attention consistently improves the translation quality of the MMT models, and (ii) fine-tuning the MMT with supervision loss enabled leads to better performance than training the MMT from scratch. Compared to the state-of-the-art, our proposed model achieves improvements of up to 2.3 BLEU and 3.5 METEOR points.", "comment": " Comments: Journal article under review "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.08747", "id": "2201.08747", "pdf": "https://arxiv.org/pdf/2201.08747"}, "title": "Inferring Brain Dynamics via Multimodal Joint Graph Representation EEG-fMRI", "author_info": ["Jalal Mirakhorli"], "summary": "Recent studies have shown that multi-modeling methods can provide new insights into the analysis of brain components that are not possible when each modality is acquired separately. The joint representations of different modalities is a robust model to analyze simultaneously acquired electroencephalography and functional magnetic resonance imaging (EEG-fMRI). Advances in precision instruments have given us the ability to observe the spatiotemporal neural dynamics of the human brain through non-invasive neuroimaging techniques such as EEG & fMRI. Nonlinear fusion methods of streams can extract effective brain components in different dimensions of temporal and spatial. Graph-based analyzes, which have many similarities to brain structure, can overcome the complexities of brain mapping analysis. Throughout, we outline the correlations of several different media in time shifts from one source with graph-based and deep learning methods. Determining overlaps can provide a new perspective for diagnosing functional changes in neuroplasticity studies.", "comment": " Comments: 13 pages, 2 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.08264", "id": "2201.08264", "pdf": "https://arxiv.org/pdf/2201.08264", "other": "https://arxiv.org/format/2201.08264"}, "title": "End-to-end Generative Pretraining for Multimodal Video Captioning", "author_info": ["Paul Hongsuck Seo", "Arsha Nagrani", "Anurag Arnab", "Cordelia Schmid"], "summary": "Recent video and language pretraining frameworks lack the ability to generate sentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new pretraining framework for learning from unlabelled videos which can be effectively used for generative tasks such as multimodal video captioning. Unlike recent video-language pretraining frameworks, our framework trains both a multimodal video encoder and a sentence decoder jointly. To overcome the lack of captions in unlabelled videos, we leverage the future utterance as an additional text source and propose a bidirectional generation objective -- we generate future utterances given the present mulitmodal context, and also the present utterance given future observations. With this objective, we train an encoder-decoder model end-to-end to generate a caption from raw pixels and transcribed speech directly. Our model achieves state-of-the-art performance for multimodal video captioning on four standard benchmarks, as well as for other video understanding tasks such as VideoQA, video retrieval and action classification.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.08248", "id": "2201.08248", "pdf": "https://arxiv.org/pdf/2201.08248"}, "title": "Packaging-enhanced optical fiber-chip interconnect with enlarged grating coupler and multimode fiber", "author_info": ["Chao Wang", "Chingwen Chang", "Jason Midkiff", "Aref Asghari", "James Fan", "Jianying Zhou", "Xiaochuan Xu", "Huiping Tian", "Ray T. Chen"], "summary": "Optical I/O plays a crucial role in the lifespan of lab-on-a-chip systems, from preliminary testing to operation in the target environment. However, due to the precise alignments required, efficient and reliable fiber-to-chip connections remain challenging, yielding inconsistent test results and unstable packaged performance. To overcome this issue, for use in single mode on-chip systems, we propose the incorporation of area-enlarged grating couplers working in conjunction with multimode fibers. This combination enables simpler, faster, and more reliable connections than the traditional small area grating coupler with single-mode fiber. In this work, we experimentally demonstrate a 3dB in-plane (X, Y) spatial tolerance of (10.2 \u03bcm, 17.3 \u03bcm) for the large area configuration, being at least (2.49, 3.33) times that of the small area one, and agreeing well with theoretical calculations. The simple concept is readily applicable to a range of photonic systems where cheaper more robust optical I/O is desired.", "comment": " Comments: 11 pages, 5 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.07683", "id": "2201.07683", "pdf": "https://arxiv.org/pdf/2201.07683", "other": "https://arxiv.org/format/2201.07683"}, "title": "Coupled Support Tensor Machine Classification for Multimodal Neuroimaging Data", "author_info": ["Li Peide", "Seyyid Emre Sofuoglu", "Tapabrata Maiti", "Selin Aviyente"], "summary": "Multimodal data arise in various applications where information about the same phenomenon is acquired from multiple sensors and across different imaging modalities. Learning from multimodal data is of great interest in machine learning and statistics research as this offers the possibility of capturing complementary information among modalities. Multimodal modeling helps to explain the interdependence between heterogeneous data sources, discovers new insights that may not be available from a single modality, and improves decision-making. Recently, coupled matrix-tensor factorization has been introduced for multimodal data fusion to jointly estimate latent factors and identify complex interdependence among the latent factors. However, most of the prior work on coupled matrix-tensor factors focuses on unsupervised learning and there is little work on supervised learning using the jointly estimated latent factors. This paper considers the multimodal tensor data classification problem. A Coupled Support Tensor Machine (C-STM) built upon the latent factors jointly estimated from the Advanced Coupled Matrix Tensor Factorization (ACMTF) is proposed. C-STM combines individual and shared latent factors with multiple kernels and estimates a maximal-margin classifier for coupled matrix tensor data. The classification risk of C-STM is shown to converge to the optimal Bayes risk, making it a statistically consistent rule. C-STM is validated through simulation studies as well as a simultaneous EEG-fMRI analysis. The empirical evidence shows that C-STM can utilize information from multiple sources and provide a better classification performance than traditional single-mode classifiers.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.07520", "id": "2201.07520", "pdf": "https://arxiv.org/pdf/2201.07520", "other": "https://arxiv.org/format/2201.07520"}, "title": "CM3: A Causal Masked Multimodal Model of the Internet", "author_info": ["Armen Aghajanyan", "Bernie Huang", "Candace Ross", "Vladimir Karpukhin", "Hu Xu", "Naman Goyal", "Dmytro Okhonko", "Mandar Joshi", "Gargi Ghosh", "Mike Lewis", "Luke Zettlemoyer"], "summary": "We introduce CM3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents that can contain both text and image tokens. Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated at the end of the string, instead of their original positions. The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. We train causally masked language-image models on large-scale web and Wikipedia articles, where each document contains all of the text, hypertext markup, hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they appear in the original HTML source (before masking). The resulting CM3 models can generate rich structured, multi-modal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. They can be prompted to recover, in a zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM. We set the new state-of-the-art in zero-shot summarization, entity linking, and entity disambiguation while maintaining competitive performance in the fine-tuning setting. We can generate images unconditionally, conditioned on text (like DALL-E) and do captioning all in a zero-shot setting with a single model.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.06786", "id": "2201.06786", "pdf": "https://arxiv.org/pdf/2201.06786", "other": "https://arxiv.org/format/2201.06786"}, "title": "Unsupervised Multimodal Word Discovery based on Double Articulation Analysis with Co-occurrence cues", "author_info": ["Akira Taniguchi", "Hiroaki Murakami", "Ryo Ozaki", "Tadahiro Taniguchi"], "summary": "Human infants acquire their verbal lexicon from minimal prior knowledge of language based on the statistical properties of phonological distributions and the co-occurrence of other sensory stimuli. In this study, we propose a novel fully unsupervised learning method discovering speech units by utilizing phonological information as a distributional cue and object information as a co-occurrence cue. The proposed method can not only (1) acquire words and phonemes from speech signals using unsupervised learning, but can also (2) utilize object information based on multiple modalities (i.e., vision, tactile, and auditory) simultaneously. The proposed method is based on the Nonparametric Bayesian Double Articulation Analyzer (NPB-DAA) discovering phonemes and words from phonological features, and Multimodal Latent Dirichlet Allocation (MLDA) categorizing multimodal information obtained from objects. In the experiment, the proposed method showed higher word discovery performance than the baseline methods. In particular, words that expressed the characteristics of the object (i.e., words corresponding to nouns and adjectives) were segmented accurately. Furthermore, we examined how learning performance is affected by differences in the importance of linguistic information. When the weight of the word modality was increased, the performance was further improved compared to the fixed condition.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.06779", "id": "2201.06779", "pdf": "https://arxiv.org/pdf/2201.06779", "other": "https://arxiv.org/format/2201.06779"}, "title": "Label Dependent Attention Model for Disease Risk Prediction Using Multimodal Electronic Health Records", "author_info": ["Shuai Niu", "Qing Yin", "Yunya Song", "Yike Guo", "Xian Yang"], "summary": "Disease risk prediction has attracted increasing attention in the field of modern healthcare, especially with the latest advances in artificial intelligence (AI). Electronic health records (EHRs), which contain heterogeneous patient information, are widely used in disease risk prediction tasks. One challenge of applying AI models for risk prediction lies in generating interpretable evidence to support the prediction results while retaining the prediction ability. In order to address this problem, we propose the method of jointly embedding words and labels whereby attention modules learn the weights of words from medical notes according to their relevance to the names of risk prediction labels. This approach boosts interpretability by employing an attention mechanism and including the names of prediction tasks in the model. However, its application is only limited to the handling of textual inputs such as medical notes. In this paper, we propose a label dependent attention model LDAM to 1) improve the interpretability by exploiting Clinical-BERT (a biomedical language model pre-trained on a large clinical corpus) to encode biomedically meaningful features and labels jointly; 2) extend the idea of joint embedding to the processing of time-series data, and develop a multi-modal learning framework for integrating heterogeneous information from medical notes and time-series health status indicators. To demonstrate our method, we apply LDAM to the MIMIC-III dataset to predict different disease risks. We evaluate our method both quantitatively and qualitatively. Specifically, the predictive power of LDAM will be shown, and case studies will be carried out to illustrate its interpretability.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.06309", "id": "2201.06309", "pdf": "https://arxiv.org/pdf/2201.06309", "other": "https://arxiv.org/format/2201.06309"}, "title": "Group Gated Fusion on Attention-based Bidirectional Alignment for Multimodal Emotion Recognition", "author_info": ["Pengfei Liu", "Kun Li", "Helen Meng"], "summary": "Emotion recognition is a challenging and actively-studied research area that plays a critical role in emotion-aware human-computer interaction systems. In a multimodal setting, temporal alignment between different modalities has not been well investigated yet. This paper presents a new model named as Gated Bidirectional Alignment Network (GBAN), which consists of an attention-based bidirectional alignment network over LSTM hidden states to explicitly capture the alignment relationship between speech and text, and a novel group gated fusion (GGF) layer to integrate the representations of different modalities. We empirically show that the attention-aligned representations outperform the last-hidden-states of LSTM significantly, and the proposed GBAN model outperforms existing state-of-the-art multimodal approaches on the IEMOCAP dataset.", "comment": " Journal ref:         INTERSPEECH 2020       "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.06263", "id": "2201.06263", "pdf": "https://arxiv.org/pdf/2201.06263"}, "title": "Learning-based multiplexed transmission of scattered twisted light through a kilometer-scale standard multimode fiber", "author_info": ["Yifan Liu", "Zhisen Zhang", "Panpan Yu", "Yijing Wu", "Ziqiang Wang", "Yinmei Li", "Wen Liu", "Lei Gong"], "summary": "Multiplexing multiple orbital angular momentum (OAM) modes of light has the potential to increase data capacity in optical communication. However, the distribution of such modes over long distances remains challenging. Free-space transmission is strongly influenced by atmospheric turbulence and light scattering, while the wave distortion induced by the mode dispersion in fibers disables OAM demultiplexing in fiber-optic communications. Here, a deep-learning-based approach is developed to recover the data from scattered OAM channels without measuring any phase information. Over a 1-km-long standard multimode fiber, the method is able to identify different OAM modes with an accuracy of more than 99.9% in parallel demultiplexing of 24 scattered OAM channels. To demonstrate the transmission quality, color images are encoded in multiplexed twisted light and our method achieves decoding the transmitted data with an error rate of 0.13%. Our work shows the artificial intelligence algorithm could benefit the use of OAM multiplexing in commercial fiber networks and high-performance optical communication in turbulent environments.", "comment": " Comments: 12 pages, 5 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.05996", "id": "2201.05996", "pdf": "https://arxiv.org/pdf/2201.05996", "other": "https://arxiv.org/format/2201.05996"}, "title": "Hardware Implementation of Multimodal Biometric using Fingerprint and Iris", "author_info": ["Tariq M Khan"], "summary": "In this paper, a hardware architecture of a multimodal biometric system is presented that massively exploits the inherent parallelism. The proposed system is based on multiple biometric fusion that uses two biometric traits, fingerprint and iris. Each biometric trait is first optimised at the software level, by addressing some of the issues that directly affect the FAR and FRR. Then the hardware architectures for both biometric traits are presented, followed by a final multimodal hardware architecture. To the best of the author's knowledge, no other FPGA-based design exits that used these two traits.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.05545", "id": "2201.05545", "pdf": "https://arxiv.org/pdf/2201.05545"}, "title": "Multimodal registration of FISH and nanoSIMS images using convolutional neural network models", "author_info": ["Xiaojia He", "Christof Meile", "Suchendra M. Bhandarkar"], "summary": "Nanoscale secondary ion mass spectrometry (nanoSIMS) and fluorescence in situ hybridization (FISH) microscopy provide high-resolution, multimodal image representations of the identity and cell activity respectively of targeted microbial communities in microbiological research. Despite its importance to microbiologists, multimodal registration of FISH and nanoSIMS images is challenging given the morphological distortion and background noise in both images. In this study, we use convolutional neural networks (CNNs) for multiscale feature extraction, shape context for computation of the minimum transformation cost feature matching and the thin-plate spline (TPS) model for multimodal registration of the FISH and nanoSIMS images. All the six tested CNN models, VGG16, VGG19, GoogLeNet and ShuffleNet, ResNet18 and ResNet101 performed well, demonstrating the utility of CNNs in the registration of multimodal images with significant background noise and morphology distortion. We also show aggregate shape preserved by binarization to be a robust feature for registering multimodal microbiology-related images.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.05471", "id": "2201.05471", "pdf": "https://arxiv.org/pdf/2201.05471", "other": "https://arxiv.org/format/2201.05471"}, "title": "Multimodal Anti-Reflective Coatings for Perfecting Anomalous Reflection from Arbitrary Periodic Structures", "author_info": ["Sherman W. Marcus", "Vinay K. Killamsetty", "Ariel Epstein"], "summary": "Metasurfaces possess vast wave-manipulation capabilities, including reflection and refraction of a plane wave into non-standard directions. This requires meticulously-designed sub-wavelength meta-atoms in each period of the metasurface which guarantee unitary coupling to the desired Floquet-Bloch mode or, equivalently, suppression of the coupling to other modes. Herein, we propose an entirely different scheme to achieve such suppression, alleviating the need to devise and realize such dense scrupulously-engineered polarizable particles. Extending the concept of anti-reflective coatings to enable simultaneous manipulation of multiple modes, we show theoretically and experimentally that a simple superstrate consisting of only several uniform dielectric layers can be modularly applied to \\textit{aribtrary} periodic structures to yield perfect anomalous reflection. This multimodal anti-reflective coating (MARC), designed based on an analytical model, presents a conceptually and practically simpler paradigm for wave-control across a wide range of physical branches, from electromagnetics and acoustics to seismics and beyond.", "comment": " Comments: 6 pages, 6 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.04990", "id": "2201.04990", "pdf": "https://arxiv.org/pdf/2201.04990", "other": "https://arxiv.org/format/2201.04990"}, "title": "Toddler-Guidance Learning: Impacts of Critical Period on Multimodal AI Agents", "author_info": ["Junseok Park", "Kwanyoung Park", "Hyunseok Oh", "Ganghun Lee", "Minsu Lee", "Youngki Lee", "Byoung-Tak Zhang"], "summary": "Critical periods are phases during which a toddler's brain develops in spurts. To promote children's cognitive development, proper guidance is critical in this stage. However, it is not clear whether such a critical period also exists for the training of AI agents. Similar to human toddlers, well-timed guidance and multimodal interactions might significantly enhance the training efficiency of AI agents as well. To validate this hypothesis, we adapt this notion of critical periods to learning in AI agents and investigate the critical period in the virtual environment for AI agents. We formalize the critical period and Toddler-guidance learning in the reinforcement learning (RL) framework. Then, we built up a toddler-like environment with VECA toolkit to mimic human toddlers' learning characteristics. We study three discrete levels of mutual interaction: weak-mentor guidance (sparse reward), moderate mentor guidance (helper-reward), and mentor demonstration (behavioral cloning). We also introduce the EAVE dataset consisting of 30,000 real-world images to fully reflect the toddler's viewpoint. We evaluate the impact of critical periods on AI agents from two perspectives: how and when they are guided best in both uni- and multimodal learning. Our experimental results show that both uni- and multimodal agents with moderate mentor guidance and critical period on 1 million and 2 million training steps show a noticeable improvement. We validate these results with transfer learning on the EAVE dataset and find the performance advancement on the same critical period and the guidance.", "comment": " ACM Class:           I.2.0; I.6.5                "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.04712", "id": "2201.04712", "pdf": "https://arxiv.org/pdf/2201.04712", "other": "https://arxiv.org/format/2201.04712"}, "title": "Deep Learning on Multimodal Sensor Data at the Wireless Edge for Vehicular Network", "author_info": ["Batool Salehi", "Guillem Reus-Muns", "Debashri Roy", "Zifeng Wang", "Tong Jian", "Jennifer Dy", "Stratis Ioannidis", "Kaushik Chowdhury"], "summary": "Beam selection for millimeter-wave links in a vehicular scenario is a challenging problem, as an exhaustive search among all candidate beam pairs cannot be assuredly completed within short contact times. We solve this problem via a novel expediting beam selection by leveraging multimodal data collected from sensors like LiDAR, camera images, and GPS. We propose individual modality and distributed fusion-based deep learning (F-DL) architectures that can execute locally as well as at a mobile edge computing center (MEC), with a study on associated tradeoffs. We also formulate and solve an optimization problem that considers practical beam-searching, MEC processing and sensor-to-MEC data delivery latency overheads for determining the output dimensions of the above F-DL architectures. Results from extensive evaluations conducted on publicly available synthetic and home-grown real-world datasets reveal 95% and 96% improvement in beam selection speed over classical RF-only beam sweeping, respectively. F-DL also outperforms the state-of-the-art techniques by 20-22% in predicting top-10 best beam pairs.", "comment": " Comments: 16 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.03969", "id": "2201.03969", "pdf": "https://arxiv.org/pdf/2201.03969", "other": "https://arxiv.org/format/2201.03969"}, "title": "Multimodal Representations Learning Based on Mutual Information Maximization and Minimization and Identity Embedding for Multimodal Sentiment Analysis", "author_info": ["Jiahao Zheng", "Sen Zhang", "Xiaoping Wang", "Zhigang Zeng"], "summary": "Multimodal sentiment analysis (MSA) is a fundamental complex research problem due to the heterogeneity gap between different modalities and the ambiguity of human emotional expression. Although there have been many successful attempts to construct multimodal representations for MSA, there are still two challenges to be addressed: 1) A more robust multimodal representation needs to be constructed to bridge the heterogeneity gap and cope with the complex multimodal interactions, and 2) the contextual dynamics must be modeled effectively throughout the information flow. In this work, we propose a multimodal representation model based on Mutual information Maximization and Minimization and Identity Embedding (MMMIE). We combine mutual information maximization between modal pairs, and mutual information minimization between input data and corresponding features to mine the modal-invariant and task-related information. Furthermore, Identity Embedding is proposed to prompt the downstream network to perceive the contextual information. Experimental results on two public datasets demonstrate the effectiveness of the proposed model.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.03597", "id": "2201.03597", "pdf": "https://arxiv.org/pdf/2201.03597", "other": "https://arxiv.org/format/2201.03597"}, "title": "Cross-Modality Sub-Image Retrieval using Contrastive Multimodal Image Representations", "author_info": ["Eva Breznik", "Elisabeth Wetzer", "Joakim Lindblad", "Nata\u0161a Sladoje"], "summary": "In tissue characterization and cancer diagnostics, multimodal imaging has emerged as a powerful technique. Thanks to computational advances, large datasets can be exploited to improve diagnosis and discover patterns in pathologies. However, this requires efficient and scalable image retrieval methods. Cross-modality image retrieval is particularly demanding, as images of the same content captured in different modalities may display little common information. We propose a content-based image retrieval system (CBIR) for reverse (sub-)image search to retrieve microscopy images in one modality given a corresponding image captured by a different modality, where images are not aligned and share only few structures. We propose to combine deep learning to generate representations which embed both modalities in a common space, with classic, fast, and robust feature extractors (SIFT, SURF) to create a bag-of-words model for efficient and reliable retrieval. Our application-independent approach shows promising results on a publicly available dataset of brightfield and second harmonic generation microscopy images. We obtain 75.4% and 83.6% top-10 retrieval success for retrieval in one or the other direction. Our proposed method significantly outperforms both direct retrieval of the original multimodal (sub-)images, as well as their corresponding generative adversarial network (GAN)-based image-to-image translations. We establish that the proposed method performs better in comparison with a recent sub-image retrieval toolkit, GAN-based image-to-image translations, and learnt feature extractors for the downstream task of cross-modal image retrieval. We highlight the shortcomings of the latter methods and observe the importance of equivariance and invariance properties of the learnt representations and feature extractors in the CBIR pipeline. Code will be available at github.com/MIDA-group.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.03553", "id": "2201.03553", "pdf": "https://arxiv.org/pdf/2201.03553"}, "title": "Study of decoherence of a superposition of macroscopic quantum states by means the consideration of a multimode state of a Schrodinger cat", "author_info": ["D. V. Fastovets", "Yu. I. Bogdanov", "N. A. Bogdanova", "V. F. Lukichev"], "summary": "Quantum Schrodinger cat states are of great interest in quantum communications and quantum optics. These states are used in various scientific fields such as quantum computing, quantum error correction and high-precision measurements. The analysis of the Schrodinger cat states coherence is an important task for their complete practical application. Our developed approach makes it possible to estimate the coherence of the quantum Schrodinger cat state of arbitrary dimension, as well as to find the interference visibility of the state - an important optical characteristic. The obtained simple quantitative relationship between coherence and the Schmidt number, as well as the developed approach of reducing the multidimensional quantum cat state to a two-mode analog allow us to analyze macroscopic states formed by a large number of modes. Several explicit formulas for the reduced states that arise after measuring of some modes of the considered multimode system are obtained. The research results have significant application and can be used in the development of high-dimensional quantum information processing systems.", "comment": " Comments: 10 pages, 4 figures, The 14th International Conference 'Micro- and Nanoelectronics - 2021' (ICMNE-2021) with the Extended Session 'Quantum Informatics', Zvenigorod, October 4-8, 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.02184", "id": "2201.02184", "pdf": "https://arxiv.org/pdf/2201.02184", "other": "https://arxiv.org/format/2201.02184"}, "title": "Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction", "author_info": ["Bowen Shi", "Wei-Ning Hsu", "Kushal Lakhotia", "Abdelrahman Mohamed"], "summary": "Video recordings of speech contain correlated audio and visual information, providing a strong signal for speech representation learning from the speaker's lip movements and the produced sound. We introduce Audio-Visual Hidden Unit BERT (AV-HuBERT), a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. AV-HuBERT learns powerful audio-visual speech representation benefiting both lip-reading and automatic speech recognition. On the largest public lip-reading benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of labeled data, outperforming the former state-of-the-art approach (33.6%) trained with a thousand times more transcribed video data (31K hours). The lip-reading WER is further reduced to 26.9% when using all 433 hours of labeled data from LRS3 and combined with self-training. Using our audio-visual representation on the same benchmark for audio-only speech recognition leads to a 40% relative WER reduction over the state-of-the-art performance (1.3% vs 2.3%). Our code and models are available at https://github.com/facebookresearch/av_hubert", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.01249", "id": "2201.01249", "pdf": "https://arxiv.org/pdf/2201.01249", "other": "https://arxiv.org/format/2201.01249"}, "title": "ExAID: A Multimodal Explanation Framework for Computer-Aided Diagnosis of Skin Lesions", "author_info": ["Adriano Lucieri", "Muhammad Naseer Bajwa", "Stephan Alexander Braun", "Muhammad Imran Malik", "Andreas Dengel", "Sheraz Ahmed"], "summary": "One principal impediment in the successful deployment of AI-based Computer-Aided Diagnosis (CAD) systems in clinical workflows is their lack of transparent decision making. Although commonly used eXplainable AI methods provide some insight into opaque algorithms, such explanations are usually convoluted and not readily comprehensible except by highly trained experts. The explanation of decisions regarding the malignancy of skin lesions from dermoscopic images demands particular clarity, as the underlying medical problem definition is itself ambiguous. This work presents ExAID (Explainable AI for Dermatology), a novel framework for biomedical image analysis, providing multi-modal concept-based explanations consisting of easy-to-understand textual explanations supplemented by visual maps justifying the predictions. ExAID relies on Concept Activation Vectors to map human concepts to those learnt by arbitrary Deep Learning models in latent space, and Concept Localization Maps to highlight concepts in the input space. This identification of relevant concepts is then used to construct fine-grained textual explanations supplemented by concept-wise location information to provide comprehensive and coherent multi-modal explanations. All information is comprehensively presented in a diagnostic interface for use in clinical routines. An educational mode provides dataset-level explanation statistics and tools for data and model exploration to aid medical research and education. Through rigorous quantitative and qualitative evaluation of ExAID, we show the utility of multi-modal explanations for CAD-assisted scenarios even in case of wrong predictions. We believe that ExAID will provide dermatologists an effective screening tool that they both understand and trust. Moreover, it will be the basis for similar applications in other biomedical imaging fields.", "comment": " Comments: Accepted for publication in Computer Methods and Programs in Biomedicine "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.00693", "id": "2201.00693", "pdf": "https://arxiv.org/pdf/2201.00693", "other": "https://arxiv.org/format/2201.00693"}, "title": "Multimodal Entity Tagging with Multimodal Knowledge Base", "author_info": ["Hao Peng", "Hang Li", "Lei Hou", "Juanzi Li", "Chao Qiao"], "summary": "To enhance research on multimodal knowledge base and multimodal information processing, we propose a new task called multimodal entity tagging (MET) with a multimodal knowledge base (MKB). We also develop a dataset for the problem using an existing MKB. In an MKB, there are entities and their associated texts and images. In MET, given a text-image pair, one uses the information in the MKB to automatically identify the related entity in the text-image pair. We solve the task by using the information retrieval paradigm and implement several baselines using state-of-the-art methods in NLP and CV. We conduct extensive experiments and make analyses on the experimental results. The results show that the task is challenging, but current technologies can achieve relatively high performance. We will release the dataset, code, and models for future research.", "comment": " Comments: 11 pages, 4 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.00203", "id": "2201.00203", "pdf": "https://arxiv.org/pdf/2201.00203", "other": "https://arxiv.org/format/2201.00203"}, "title": "NOMA Computation Over Multi-Access Channels for Multimodal Sensing", "author_info": ["Michel Kulhandjian", "Gunes Karabulut Kurt", "Hovannes Kulhandjian", "Halim Yanikomeroglu", "Claude D'Amours"], "summary": "An improved mean squared error (MSE) minimization solution based on eigenvector decomposition approach is conceived for wideband non-orthogonal multiple-access based computation over multi-access channel (NOMA-CoMAC) framework. This work aims at further developing NOMA-CoMAC for next-generation multimodal sensor networks, where a multimodal sensor monitors several environmental parameters such as temperature, pollution, humidity, or pressure. We demonstrate that our proposed scheme achieves an MSE value approximately 0.7 lower at E_b/N_o = 1 dB in comparison to that for the average sum-channel based method. Moreover, the MSE performance gain of our proposed solution increases even more for larger values of subcarriers and sensor nodes due to the benefit of the diversity gain. This, in return, suggests that our proposed scheme is eminently suitable for multimodal sensor networks.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.14298", "id": "2112.14298", "pdf": "https://arxiv.org/pdf/2112.14298", "other": "https://arxiv.org/format/2112.14298"}, "title": "Multimodal perception for dexterous manipulation", "author_info": ["Guanqun Cao", "Shan Luo"], "summary": "Humans usually perceive the world in a multimodal way that vision, touch, sound are utilised to understand surroundings from various dimensions. These senses are combined together to achieve a synergistic effect where the learning is more effectively than using each sense separately. For robotics, vision and touch are two key senses for the dexterous manipulation. Vision usually gives us apparent features like shape, color, and the touch provides local information such as friction, texture, etc. Due to the complementary properties between visual and tactile senses, it is desirable for us to combine vision and touch for a synergistic perception and manipulation. Many researches have been investigated about multimodal perception such as cross-modal learning, 3D reconstruction, multimodal translation with vision and touch. Specifically, we propose a cross-modal sensory data generation framework for the translation between vision and touch, which is able to generate realistic pseudo data. By using this cross-modal translation method, it is desirable for us to make up inaccessible data, helping us to learn the object's properties from different views. Recently, the attention mechanism becomes a popular method either in visual perception or in tactile perception. We propose a spatio-temporal attention model for tactile texture recognition, which takes both spatial features and time dimension into consideration. Our proposed method not only pays attention to the salient features in each spatial feature, but also models the temporal correlation in the through the time. The obvious improvement proves the efficiency of our selective attention mechanism. The spatio-temporal attention method has potential in many applications such as grasping, recognition, and multimodal perception.", "comment": " Comments: 19 pages, 10 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.13913", "id": "2112.13913", "pdf": "https://arxiv.org/pdf/2112.13913", "other": "https://arxiv.org/format/2112.13913"}, "title": "Some mathematical aspects of Anderson localization: boundary effect, multimodality, and bifurcation", "author_info": ["Chen Jia", "Ziqi Liu", "Zhimin Zhang"], "summary": "Anderson localization is a famous wave phenomenon that describes the absence of diffusion of waves in a disordered medium. Here we investigate some novel mathematical aspects of Anderson localization that are seldom discussed before. First, we observe that under the Neumann boundary condition, the low energy quantum states are localized on the boundary of the domain with high probability. We provide a detailed explanation of this phenomenon using the concept of extended subregions and obtain an analytical expression of this probability in the one-dimensional case. Second, we find that the quantum states may be localized in multiple different subregions with high probability in the one-dimensional case and we derive an explicit expression of this probability for various boundary conditions. Finally, we examine a bifurcation phenomenon of the localization subregion as the strength of disorder varies. The critical threshold of bifurcation is analytically computed based on a toy model and the dependence of the critical threshold on model parameters is analyzed.", "comment": " MSC Class:           35J10; 35J25; 82B44; 47A75; 35Q40                          "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.13696", "id": "2112.13696", "pdf": "https://arxiv.org/pdf/2112.13696", "other": "https://arxiv.org/format/2112.13696"}, "title": "Thermalization of orbital angular momentum beams in multimode optical fibers", "author_info": ["E. V. Podivilov", "F. Mangini", "O. S. Sidelnikov", "M. Ferraro", "M. Gervaziev", "D. S. Kharenko", "M. Zitelli", "M. P. Fedoruk", "S. A. Babin", "S. Wabnitz"], "summary": "We present a general theory of thermalization of light in multimode optical fibers, including optical beams with nonzero orbital angular momentum or vortex beams. A generalized Rayleigh-Jeans distribution of asymptotic mode composition is obtained, based on the conservation of the angular momentum. We confirm our predictions by numerical simulations and experiments based on holographic mode decomposition of multimode beams. This establishes new constraints for the achievement of spatial beam self-cleaning, giving previously unforeseen insights into the underlying physical mechanisms.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.13592", "id": "2112.13592", "pdf": "https://arxiv.org/pdf/2112.13592", "other": "https://arxiv.org/format/2112.13592"}, "title": "Multimodal Image Synthesis and Editing: A Survey", "author_info": ["Fangneng Zhan", "Yingchen Yu", "Rongliang Wu", "Jiahui Zhang", "Shijian Lu"], "summary": "As information exists in various modalities in real world, effective interaction and fusion among multimodal information plays a key role for the creation and perception of multimodal data in computer vision and deep learning research. With superb power in modelling the interaction among multimodal information, multimodal image synthesis and editing have become a hot research topic in recent years. Different from traditional visual guidance which provides explicit clues, multimodal guidance offers intuitive and flexible means in image synthesis and editing. On the other hand, this field is also facing several challenges in alignment of features with inherent modality gaps, synthesis of high-resolution images, faithful evaluation metrics, etc. In this survey, we comprehensively contextualize the advance of the recent multimodal image synthesis \\& editing and formulate taxonomies according to data modality and model architectures. We start with an introduction to different types of guidance modalities in image synthesis and editing. We then describe multimodal image synthesis and editing approaches extensively with detailed frameworks including Generative Adversarial Networks (GANs), GAN Inversion, Transformers, and other methods such as NeRF and Diffusion models. This is followed by a comprehensive description of benchmark datasets and corresponding evaluation metrics as widely adopted in multimodal image synthesis and editing, as well as detailed comparisons of different synthesis methods with analysis of respective advantages and limitations. Finally, we provide insights into the current research challenges and possible future research directions. A project associated with this survey is available at https://github.com/fnzhan/MISE", "comment": " Comments: 20 pages, 19 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.12908", "id": "2112.12908", "pdf": "https://arxiv.org/pdf/2112.12908", "other": "https://arxiv.org/format/2112.12908"}, "title": "Annealed Leap-Point Sampler for Multimodal Target Distributions", "author_info": ["Nicholas G. Tawn", "Matthew T. Moores", "Gareth O. Roberts"], "summary": "In Bayesian statistics, exploring multimodal posterior distribution poses major challenges for existing techniques such as Markov Chain Monte Carlo (MCMC). These problems are exacerbated in high-dimensional settings where MCMC methods typically rely upon localised proposal mechanisms. This paper introduces the Annealed Leap-Point Sampler (ALPS), which augments the target distribution state space with modified annealed (cooled) target distributions, in contrast to traditional approaches which have employed tempering. The temperature of the coldest state is chosen such that its corresponding annealed target density can be sufficiently well-approximated by a Laplace approximation. As a result, a Gaussian mixture independence Metropolis-Hastings sampler can perform mode-jumping proposals even in high-dimensional problems. The ability of this procedure to \"mode hop\" at this super-cold state is then filtered through to the target state using a sequence of tempered targets in a similar way to that in parallel tempering methods. ALPS also incorporates the best aspects of current gold-standard approaches to multimodal sampling in high-dimensional contexts. A theoretical analysis of the ALPS approach in high dimensions is given, providing practitioners with a gauge on the optimal setup as well as the scalability of the algorithm. For a d-dimensional problem the it is shown that the coldest inverse temperature level required for the ALPS only needs to be linear in the dimension, O(d), and this means that for a collection of multimodal problems the algorithmic cost is polynomial, O(d3). ALPS is illustrated on a complex multimodal posterior distribution that arises from a seemingly-unrelated regression (SUR) model of longitudinal data from U.S. manufacturing firms.", "comment": " MSC Class:           62-08 (Primary) 60J22; 62-04 (Secondary)                          "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.12792", "id": "2112.12792", "pdf": "https://arxiv.org/pdf/2112.12792", "other": "https://arxiv.org/format/2112.12792"}, "title": "Understanding and Measuring Robustness of Multimodal Learning", "author_info": ["Nishant Vishwamitra", "Hongxin Hu", "Ziming Zhao", "Long Cheng", "Feng Luo"], "summary": "The modern digital world is increasingly becoming multimodal. Although multimodal learning has recently revolutionized the state-of-the-art performance in multimodal tasks, relatively little is known about the robustness of multimodal learning in an adversarial setting. In this paper, we introduce a comprehensive measurement of the adversarial robustness of multimodal learning by focusing on the fusion of input modalities in multimodal models, via a framework called MUROAN (MUltimodal RObustness ANalyzer). We first present a unified view of multimodal models in MUROAN and identify the fusion mechanism of multimodal models as a key vulnerability. We then introduce a new type of multimodal adversarial attacks called decoupling attack in MUROAN that aims to compromise multimodal models by decoupling their fused modalities. We leverage the decoupling attack of MUROAN to measure several state-of-the-art multimodal models and find that the multimodal fusion mechanism in all these models is vulnerable to decoupling attacks. We especially demonstrate that, in the worst case, the decoupling attack of MUROAN achieves an attack success rate of 100% by decoupling just 1.16% of the input space. Finally, we show that traditional adversarial training is insufficient to improve the robustness of multimodal models with respect to decoupling attacks. We hope our findings encourage researchers to pursue improving the robustness of multimodal learning.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.12180", "id": "2112.12180", "pdf": "https://arxiv.org/pdf/2112.12180", "other": "https://arxiv.org/format/2112.12180"}, "title": "Multimodal Personality Recognition using Cross-Attention Transformer and Behaviour Encoding", "author_info": ["Tanay Agrawal", "Dhruv Agarwal", "Michal Balazia", "Neelabh Sinha", "Francois Bremond"], "summary": "Personality computing and affective computing have gained recent interest in many research areas. The datasets for the task generally have multiple modalities like video, audio, language and bio-signals. In this paper, we propose a flexible model for the task which exploits all available data. The task involves complex relations and to avoid using a large model for video processing specifically, we propose the use of behaviour encoding which boosts performance with minimal change to the model. Cross-attention using transformers has become popular in recent times and is utilised for fusion of different modalities. Since long term relations may exist, breaking the input into chunks is not desirable, thus the proposed model processes the entire input together. Our experiments show the importance of each of the above contributions", "comment": " Comments: Preprint. Final paper accepted at the 17th International Conference on Computer Vision Theory and Applications, VISAPP 2021, Virtual, February 6-8, 2022. 8 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.12072", "id": "2112.12072", "pdf": "https://arxiv.org/pdf/2112.12072", "other": "https://arxiv.org/format/2112.12072"}, "title": "Hierarchical Cross-Modality Semantic Correlation Learning Model for Multimodal Summarization", "author_info": ["Litian Zhang", "Xiaoming Zhang", "Junshu Pan", "Feiran Huang"], "summary": "Multimodal summarization with multimodal output (MSMO) generates a summary with both textual and visual content. Multimodal news report contains heterogeneous contents, which makes MSMO nontrivial. Moreover, it is observed that different modalities of data in the news report correlate hierarchically. Traditional MSMO methods indistinguishably handle different modalities of data by learning a representation for the whole data, which is not directly adaptable to the heterogeneous contents and hierarchical correlation. In this paper, we propose a hierarchical cross-modality semantic correlation learning model (HCSCL) to learn the intra- and inter-modal correlation existing in the multimodal data. HCSCL adopts a graph network to encode the intra-modal correlation. Then, a hierarchical fusion framework is proposed to learn the hierarchical correlation between text and images. Furthermore, we construct a new dataset with relevant image annotation and image object label information to provide the supervision information for the learning procedure. Extensive experiments on the dataset show that HCSCL significantly outperforms the baseline methods in automatic summarization metrics and fine-grained diversity tests.", "comment": " Comments: Accepted by AAAI2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.12044", "id": "2112.12044", "pdf": "https://arxiv.org/pdf/2112.12044", "other": "https://arxiv.org/format/2112.12044"}, "title": "A simple way to incorporate loss when modelling multimode entangled state generation", "author_info": ["Colin Vendromin", "Marc M. Dignam"], "summary": "We show that the light generated via spontaneous four-wave mixing or parametric down conversion in multiple, coupled, lossy cavities is a multimode squeezed thermal state. Requiring this state to be the solution of the Lindblad master equation results in a set of coupled first-order differential equations for the time-dependent squeezing parameters and thermal photon numbers of the state. The benefit of this semi-analytic approach is that the number of coupled equations scales linearly with the number of modes but is independent of the number of photons generated. With this analytic form of the state, correlation variances are easily expressed as analytic functions of the time-dependent mode parameters. Thus, our solution makes it computationally tractable and relatively straight forward to calculate the generation and evolution of multimode entangled states in multiple coupled, lossy cavities, even when there are a large number of modes and/or photons.", "comment": " Comments: 15 pages, 1 figure "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.11850", "id": "2112.11850", "pdf": "https://arxiv.org/pdf/2112.11850"}, "title": "Multimodal Analysis of memes for sentiment extraction", "author_info": ["Nayan Varma Alluri", "Neeli Dheeraj Krishna"], "summary": "Memes are one of the most ubiquitous forms of social media communication. The study and processing of memes, which are intrinsically multimedia, is a popular topic right now. The study presented in this research is based on the Memotion dataset, which involves categorising memes based on irony, comedy, motivation, and overall-sentiment. Three separate innovative transformer-based techniques have been developed, and their outcomes have been thoroughly reviewed.The best algorithm achieved a macro F1 score of 0.633 for humour classification, 0.55 for motivation classification, 0.61 for sarcasm classification, and 0.575 for overall sentiment of the meme out of all our techniques.", "comment": " Comments: 5 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.11244", "id": "2112.11244", "pdf": "https://arxiv.org/pdf/2112.11244", "other": "https://arxiv.org/format/2112.11244"}, "title": "Hateful Memes Challenge: An Enhanced Multimodal Framework", "author_info": ["Aijing Gao", "Bingjun Wang", "Jiaqi Yin", "Yating Tian"], "summary": "Hateful Meme Challenge proposed by Facebook AI has attracted contestants around the world. The challenge focuses on detecting hateful speech in multimodal memes. Various state-of-the-art deep learning models have been applied to this problem and the performance on challenge's leaderboard has also been constantly improved. In this paper, we enhance the hateful detection framework, including utilizing Detectron for feature extraction, exploring different setups of VisualBERT and UNITER models with different loss functions, researching the association between the hateful memes and the sensitive text features, and finally building ensemble method to boost model performance. The AUROC of our fine-tuned VisualBERT, UNITER, and ensemble method achieves 0.765, 0.790, and 0.803 on the challenge's test set, respectively, which beats the baseline models. Our code is available at https://github.com/yatingtian/hateful-meme", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.10841", "id": "2112.10841", "pdf": "https://arxiv.org/pdf/2112.10841", "other": "https://arxiv.org/format/2112.10841"}, "title": "Thermalization of light's orbital angular momentum in nonlinear multimode waveguide systems", "author_info": ["Fan O. Wu", "Qi Zhong", "Huizhong Ren", "Pawel S. Jung", "Konstantinos G. Makris", "Demetrios N. Christodoulides"], "summary": "We show that the orbital angular momentum (OAM) of a light field can be thermalized in a nonlinear cylindrical multimode optical waveguide. We find, that upon thermal equilibrium, the maximization of the optical entropy leads to a generalized Rayleigh-Jeans distribution that governs the power modal occupancies with respect to the discrete OAM charge numbers. This distribution is characterized by a temperature that is by nature different from that associated with the longitudinal electromagnetic momentum flow of the optical field. Counterintuitively and in contrast to previous results, we demonstrate that even under positive temperatures, the ground state of the fiber is not always the most populated in terms of power. Instead, because of OAM, the thermalization processes may favor higher order modes. A new equation of state is derived along with an extended Euler equation -- resulting from the extensivity of the entropy itself. By monitoring the nonlinear interaction between two multimoded optical wavefronts with opposite spins, we show that the exchange of angular momentum is dictated by the difference in OAM temperatures, in full accord with the second law of thermodynamics. The theoretical analysis presented here is corroborated by numerical simulations that take into account the complex nonlinear dynamics of hundreds of modes. Our results may pave the way towards high power optical sources with controllable orbital angular momenta, and at a more fundamental level, could shed light on the physics of other complex multimoded nonlinear bosonic systems that display additional conservation laws.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.10384", "id": "2112.10384", "pdf": "https://arxiv.org/pdf/2112.10384", "other": "https://arxiv.org/format/2112.10384"}, "title": "Multimodal Adversarially Learned Inference with Factorized Discriminators", "author_info": ["Wenxue Chen", "Jianke Zhu"], "summary": "Learning from multimodal data is an important research topic in machine learning, which has the potential to obtain better representations. In this work, we propose a novel approach to generative modeling of multimodal data based on generative adversarial networks. To learn a coherent multimodal generative model, we show that it is necessary to align different encoder distributions with the joint decoder distribution simultaneously. To this end, we construct a specific form of the discriminator to enable our model to utilize data efficiently, which can be trained constrastively. By taking advantage of contrastive learning through factorizing the discriminator, we train our model on unimodal data. We have conducted experiments on the benchmark datasets, whose promising results show that our proposed approach outperforms the-state-of-the-art methods on a variety of metrics. The source code will be made publicly available.", "comment": " Comments: 9 pages, 6 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.09905", "id": "2112.09905", "pdf": "https://arxiv.org/pdf/2112.09905"}, "title": "New evidence for a nonclassical behavior of laser multimode light", "author_info": ["M. Lebedev", "A. Demenev", "A. Parakhonsky", "O. Misochko"], "summary": "In this work, we present new experimental evidence of a nonclassical behavior of a multimode Fabry-Perot (FP) semiconductor laser by the measurements of intensity correlation functions. Because of the multimode quantum state occurrence, instead of expected correlations between the intensities of the laser modes (a semiclassical theory), their anticorrelations were revealed.", "comment": " Comments: 5 pages, 5 figures, the manuscript was send to the publisher Optics MDPI "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.09527", "id": "2112.09527", "pdf": "https://arxiv.org/pdf/2112.09527"}, "title": "Modeling of Multimodal Scattering by Conducting Bodies in Quantum Optics: the Method of Characteristic Modes", "author_info": ["Gregory Ya. Slepyan", "Dmitri Mogilevtsev", "Ilay Levie", "Amir Boag"], "summary": "We propose a numerical technique for modeling the quantum multimode light scattering by a perfectly conducting body. Using the novel quantization technique, we give the quantum adaptation of the characteristic mode approach widely used in the classical electrodynamics. The method is universal with respect to the body's configuration, as well as its dimensions relative to the wavelength. Using this method and calculating the first- and the second-order field correlation functions, we demonstrate how scattering affects quantum-statistical features of the field. As an example, we consider scattering of the two single-photon incident Gaussian beams on the cylinder with circular cross-section. We show that the scattering is accompanied by the two-photon interference and demonstrates the Hong-Ou-Mandel effect. It is shown, that the scattered two-photon field and its correlations are able to manifest a varying directive propagation, which is controllable by various means (angles of incidence, configuration of the body, relations between its sizes with the frequency). We expect that this method will be useful for designing quantum-optical devices.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.09499", "id": "2112.09499", "pdf": "https://arxiv.org/pdf/2112.09499", "other": "https://arxiv.org/format/2112.09499"}, "title": "Non-Markovian Quantum Dynamics in Strongly Coupled Multimode Cavities Conditioned on Continuous Measurement", "author_info": ["Valentin Link", "Kai M\u00fcller", "Rosaria G. Lena", "Kimmo Luoma", "Fran\u00e7ois Damanet", "Walter T. Strunz", "Andrew J. Daley"], "summary": "An important challenge in non-Markovian open quantum systems is to understand what information we gain from continuous measurement of an output field. For example, atoms in multimode cavity QED systems provide an exciting platform to study many-body phenomena in regimes where the atoms are strongly coupled amongst themselves and with the cavity, but the strong coupling makes it complicated to infer the conditioned state of the atoms from the output light. In this work we address this problem, describing the reduced atomic state via a conditioned hierarchy of equations of motion, which provides an exact conditioned reduced description under monitoring (and continuous feedback). We utilise this formalism to study how different monitoring for modes of a multimode cavity affects our information gain for an atomic state, and to improve spin squeezing via measurement and feedback in a strong coupling regime. This work opens opportunities to understand continuous monitoring of non-Markovian open quantum systems, both on a practical and fundamental level.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.09467", "id": "2112.09467", "pdf": "https://arxiv.org/pdf/2112.09467", "other": "https://arxiv.org/format/2112.09467"}, "title": "A Multimodal Approach for Automatic Mania Assessment in Bipolar Disorder", "author_info": ["P\u0131nar Baki"], "summary": "Bipolar disorder is a mental health disorder that causes mood swings that range from depression to mania. Diagnosis of bipolar disorder is usually done based on patient interviews, and reports obtained from the caregivers of the patients. Subsequently, the diagnosis depends on the experience of the expert, and it is possible to have confusions of the disorder with other mental disorders. Automated processes in the diagnosis of bipolar disorder can help providing quantitative indicators, and allow easier observations of the patients for longer periods. Furthermore, the need for remote treatment and diagnosis became especially important during the COVID-19 pandemic. In this thesis, we create a multimodal decision system based on recordings of the patient in acoustic, linguistic, and visual modalities. The system is trained on the Bipolar Disorder corpus. Comprehensive analysis of unimodal and multimodal systems, as well as various fusion techniques are performed. Besides processing entire patient sessions using unimodal features, a task-level investigation of the clips is studied. Using acoustic, linguistic, and visual features in a multimodal fusion system, we achieved a 64.8% unweighted average recall score, which improves the state-of-the-art performance achieved on this dataset.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.09253", "id": "2112.09253", "pdf": "https://arxiv.org/pdf/2112.09253", "other": "https://arxiv.org/format/2112.09253"}, "title": "Logically at the Factify 2022: Multimodal Fact Verification", "author_info": ["Jie Gao", "Hella-Franziska Hoffmann", "Stylianos Oikonomou", "David Kiskovski", "Anil Bandhakavi"], "summary": "This paper describes our participant system for the multi-modal fact verification (Factify) challenge at AAAI 2022. Despite the recent advance in text based verification techniques and large pre-trained multimodal models cross vision and language, very limited work has been done in applying multimodal techniques to automate fact checking process, particularly considering the increasing prevalence of claims and fake news about images and videos on social media. In our work, the challenge is treated as multimodal entailment task and framed as multi-class classification. Two baseline approaches are proposed and explored including an ensemble model (combining two uni-modal models) and a multi-modal attention network (modeling the interaction between image and text pair from claim and evidence document). We conduct several experiments investigating and benchmarking different SoTA pre-trained transformers and vision models in this work. Our best model is ranked first in leaderboard which obtains a weighted average F-measure of 0.77 on both validation and test set. Exploratory analysis of dataset is also carried out on the Factify data set and uncovers salient patterns and issues (e.g., word overlapping, visual entailment correlation, source bias) that motivates our hypothesis. Finally, we highlight challenges of the task and multimodal dataset for future research.", "comment": " Comments: Accepted in AAAI'22: First Workshop on Multimodal Fact-Checking and Hate Speech Detection, Februrary 22 - March 1, 2022,Vancouver, BC, Canada "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.09081", "id": "2112.09081", "pdf": "https://arxiv.org/pdf/2112.09081", "other": "https://arxiv.org/format/2112.09081"}, "title": "CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data", "author_info": ["Qi Yan", "Jianhao Zheng", "Simon Reding", "Shanci Li", "Iordan Doytchinov"], "summary": "We present a visual localization system that learns to estimate camera poses in the real world with the help of synthetic data. Despite significant progress in recent years, most learning-based approaches to visual localization target at a single domain and require a dense database of geo-tagged images to function well. To mitigate the data scarcity issue and improve the scalability of the neural localization models, we introduce TOPO-DataGen, a versatile synthetic data generation tool that traverses smoothly between the real and virtual world, hinged on the geographic camera viewpoint. New large-scale sim-to-real benchmark datasets are proposed to showcase and evaluate the utility of the said synthetic data. Our experiments reveal that synthetic data generically enhances the neural network performance on real data. Furthermore, we introduce CrossLoc, a cross-modal visual representation learning approach to pose estimation that makes full use of the scene coordinate ground truth via self-supervision. Without any extra data, CrossLoc significantly outperforms the state-of-the-art methods and achieves substantially higher real-data sample efficiency. Our code is available at https://github.com/TOPO-EPFL/CrossLoc.", "comment": " Comments: Preprint. Our code is available at https://github.com/TOPO-EPFL/CrossLoc "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.08594", "id": "2112.08594", "pdf": "https://arxiv.org/pdf/2112.08594", "other": "https://arxiv.org/format/2112.08594"}, "title": "Twitter-COMMs: Detecting Climate, COVID, and Military Multimodal Misinformation", "author_info": ["Giscard Biamby", "Grace Luo", "Trevor Darrell", "Anna Rohrbach"], "summary": "Detecting out-of-context media, such as \"miscaptioned\" images on Twitter, often requires detecting inconsistencies between the two modalities. This paper describes our approach to the Image-Text Inconsistency Detection challenge of the DARPA Semantic Forensics (SemaFor) Program. First, we collect Twitter-COMMs, a large-scale multimodal dataset with 884k tweets relevant to the topics of Climate Change, COVID-19, and Military Vehicles. We train our approach, based on the state-of-the-art CLIP model, leveraging automatically generated random and hard negatives. Our method is then tested on a hidden human-generated evaluation set. We achieve the best result on the program leaderboard, with 11% detection improvement in a high precision regime over a zero-shot CLIP baseline.", "comment": " Comments: 11 pages, 6 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.08470", "id": "2112.08470", "pdf": "https://arxiv.org/pdf/2112.08470", "other": "https://arxiv.org/format/2112.08470"}, "title": "Insta-VAX: A Multimodal Benchmark for Anti-Vaccine and Misinformation Posts Detection on Social Media", "author_info": ["Mingyang Zhou", "Mahasweta Chakraborti", "Sijia Qian", "Zhou Yu", "Jingwen Zhang"], "summary": "Sharing of anti-vaccine posts on social media, including misinformation posts, has been shown to create confusion and reduce the publics confidence in vaccines, leading to vaccine hesitancy and resistance. Recent years have witnessed the fast rise of such anti-vaccine posts in a variety of linguistic and visual forms in online networks, posing a great challenge for effective content moderation and tracking. Extending previous work on leveraging textual information to understand vaccine information, this paper presents Insta-VAX, a new multi-modal dataset consisting of a sample of 64,957 Instagram posts related to human vaccines. We applied a crowdsourced annotation procedure verified by two trained expert judges to this dataset. We then bench-marked several state-of-the-art NLP and computer vision classifiers to detect whether the posts show anti-vaccine attitude and whether they contain misinformation. Extensive experiments and analyses demonstrate the multimodal models can classify the posts more accurately than the uni-modal models, but still need improvement especially on visual context understanding and external knowledge cooperation. The dataset and classifiers contribute to monitoring and tracking of vaccine discussions for social scientific and public health efforts in combating the problem of vaccine misinformation.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.08443", "id": "2112.08443", "pdf": "https://arxiv.org/pdf/2112.08443", "other": "https://arxiv.org/format/2112.08443"}, "title": "Event-Aware Multimodal Mobility Nowcasting", "author_info": ["Zhaonan Wang", "Renhe Jiang", "Hao Xue", "Flora D. Salim", "Xuan Song", "Ryosuke Shibasaki"], "summary": "As a decisive part in the success of Mobility-as-a-Service (MaaS), spatio-temporal predictive modeling for crowd movements is a challenging task particularly considering scenarios where societal events drive mobility behavior deviated from the normality. While tremendous progress has been made to model high-level spatio-temporal regularities with deep learning, most, if not all of the existing methods are neither aware of the dynamic interactions among multiple transport modes nor adaptive to unprecedented volatility brought by potential societal events. In this paper, we are therefore motivated to improve the canonical spatio-temporal network (ST-Net) from two perspectives: (1) design a heterogeneous mobility information network (HMIN) to explicitly represent intermodality in multimodal mobility; (2) propose a memory-augmented dynamic filter generator (MDFG) to generate sequence-specific parameters in an on-the-fly fashion for various scenarios. The enhanced event-aware spatio-temporal network, namely EAST-Net, is evaluated on several real-world datasets with a wide variety and coverage of societal events. Both quantitative and qualitative experimental results verify the superiority of our approach compared with the state-of-the-art baselines. Code and data are published on https://github.com/underdoc-wang/EAST-Net.", "comment": " Comments: Accepted by AAAI 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.08078", "id": "2112.08078", "pdf": "https://arxiv.org/pdf/2112.08078", "other": "https://arxiv.org/format/2112.08078"}, "title": "Joint Demand Prediction for Multimodal Systems: A Multi-task Multi-relational Spatiotemporal Graph Neural Network Approach", "author_info": ["Yuebing Liang", "Guan Huang", "Zhan Zhao"], "summary": "Dynamic demand prediction is crucial for the efficient operation and management of urban transportation systems. Extensive research has been conducted on single-mode demand prediction, ignoring the fact that the demands for different transportation modes can be correlated with each other. Despite some recent efforts, existing approaches to multimodal demand prediction are generally not flexible enough to account for multiplex networks with diverse spatial units and heterogeneous spatiotemporal correlations across different modes. To tackle these issues, this study proposes a multi-relational spatiotemporal graph neural network (ST-MRGNN) for multimodal demand prediction. Specifically, the spatial dependencies across modes are encoded with multiple intra- and inter-modal relation graphs. A multi-relational graph neural network (MRGNN) is introduced to capture cross-mode heterogeneous spatial dependencies, consisting of generalized graph convolution networks to learn the message passing mechanisms within relation graphs and an attention-based aggregation module to summarize different relations. We further integrate MRGNNs with temporal gated convolution layers to jointly model heterogeneous spatiotemporal correlations. Extensive experiments are conducted using real-world subway and ride-hailing datasets from New York City, and the results verify the improved performance of our proposed approach over existing methods across modes. The improvement is particularly large for demand-sparse locations. Further analysis of the attention mechanisms of ST-MRGNN also demonstrates its good interpretability for understanding cross-mode interactions.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07973", "id": "2112.07973", "pdf": "https://arxiv.org/pdf/2112.07973", "other": "https://arxiv.org/format/2112.07973"}, "title": "Weak branch and multimodal convection in rapidly rotating spheres at low Prandtl number", "author_info": ["Ferran Garcia", "Frank Stefani", "Emmanuel Dormy"], "summary": "The focus of this study is to investigate primary and secondary bifurcations to weakly nonlinear flows (weak branch) in convective rotating spheres in a regime where only strongly nonlinear oscillatory sub- and super-critical flows (strong branch) were previously found in [E. J. Kaplan, N. Schaeffer, J. Vidal, and P. Cardin, Phys. Rev. Lett. 119, 094501 (2017)]. The relevant regime corresponds to low Prandtl and Ekman numbers, indicating a predominance of Coriolis forces and thermal diffusion in the system. We provide the bifurcation diagrams for rotating waves (RWs) computed by means of continuation methods and the corresponding stability analysis of these periodic flows to detect secondary bifurcations giving rise to quasiperiodic modulated rotating waves (MRWs). Additional direct numerical simulations (DNS) are performed for the analysis of these quasiperiodic flows for which Poincar\u00e9 sections and kinetic energy spectra are presented. The diffusion time scales are investigated as well. Our study reveals very large initial transients (more than 30 diffusion time units) for the nonlinear saturation of solutions on the weak branch, either RWs or MRWs, when DNS are employed. In addition, we demonstrate that MRWs have multimodal nature involving resonant triads. The modes can be located in the bulk of the fluid or attached to the outer sphere and exhibit multicellular structures. The different resonant modes forming the nonlinear quasiperiodic flows can be predicted with the stability analysis of RWs, close to the Hopf bifurcation point, by analyzing the leading unstable Floquet eigenmode.", "comment": " MSC Class:           76U60; 76E20; 76E30; 37N10; 37M20; 37G40                          "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07668", "id": "2112.07668", "pdf": "https://arxiv.org/pdf/2112.07668", "other": "https://arxiv.org/format/2112.07668"}, "title": "Dual-Key Multimodal Backdoors for Visual Question Answering", "author_info": ["Matthew Walmer", "Karan Sikka", "Indranil Sur", "Abhinav Shrivastava", "Susmit Jha"], "summary": "The success of deep learning has enabled advances in multimodal tasks that require non-trivial fusion of multiple input domains. Although multimodal models have shown potential in many problems, their increased complexity makes them more vulnerable to attacks. A Backdoor (or Trojan) attack is a class of security vulnerability wherein an attacker embeds a malicious secret behavior into a network (e.g. targeted misclassification) that is activated when an attacker-specified trigger is added to an input. In this work, we show that multimodal networks are vulnerable to a novel type of attack that we refer to as Dual-Key Multimodal Backdoors. This attack exploits the complex fusion mechanisms used by state-of-the-art networks to embed backdoors that are both effective and stealthy. Instead of using a single trigger, the proposed attack embeds a trigger in each of the input modalities and activates the malicious behavior only when both the triggers are present. We present an extensive study of multimodal backdoors on the Visual Question Answering (VQA) task with multiple architectures and visual feature backbones. A major challenge in embedding backdoors in VQA models is that most models use visual features extracted from a fixed pretrained object detector. This is challenging for the attacker as the detector can distort or ignore the visual trigger entirely, which leads to models where backdoors are over-reliant on the language trigger. We tackle this problem by proposing a visual trigger optimization strategy designed for pretrained object detectors. Through this method, we create Dual-Key Backdoors with over a 98% attack success rate while only poisoning 1% of the training data. Finally, we release TrojVQA, a large collection of clean and trojan VQA models to enable research in defending against multimodal backdoors.", "comment": " Comments: 22 pages, 11 figures, 12 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07294", "id": "2112.07294", "pdf": "https://arxiv.org/pdf/2112.07294", "other": "https://arxiv.org/format/2112.07294"}, "title": "Are multimodal events a sign of the strangelet passage through the matter?", "author_info": ["Ewa G\u0142adysz-Dziadu\u015b"], "summary": "A possible connection between the multimodal events (MME) observed in the very high energy extensive air showers by the HORIZON-T experiment and the so-called strongly penetrating component observed in the homogenous lead emulsion chambers of the Pamir and Chacaltaya Experiments was studied. We found that both experimental observations could be connected one to the other, and could be the manifestation of the same physical process, i.e. penetration of a strangelet through the matter. In the first case a strangelet produces the many-maxima long range cascades observed in the homogenous lead emulsion chambers. In the second case the successive interactions of a strangelet in the air are seen in the HORIZON-T detectors as the consecutive signals. Time intervals between signals are between several dozen to several hundred nanoseconds. I", "comment": " Comments: 25 pages, 17 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07263", "id": "2112.07263", "pdf": "https://arxiv.org/pdf/2112.07263", "other": "https://arxiv.org/format/2112.07263"}, "title": "Quantifying Multimodality in World Models", "author_info": ["Andreas Sedlmeier", "Michael K\u00f6lle", "Robert M\u00fcller", "Leo Baudrexel", "Claudia Linnhoff-Popien"], "summary": "Model-based Deep Reinforcement Learning (RL) assumes the availability of a model of an environment's underlying transition dynamics. This model can be used to predict future effects of an agent's possible actions. When no such model is available, it is possible to learn an approximation of the real environment, e.g. by using generative neural networks, sometimes also called World Models. As most real-world environments are stochastic in nature and the transition dynamics are oftentimes multimodal, it is important to use a modelling technique that is able to reflect this multimodal uncertainty. In order to safely deploy such learning systems in the real world, especially in an industrial context, it is paramount to consider these uncertainties. In this work, we analyze existing and propose new metrics for the detection and quantification of multimodal uncertainty in RL based World Models. The correct modelling & detection of uncertain future states lays the foundation for handling critical situations in a safe way, which is a prerequisite for deploying RL systems in real-world settings.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.06558", "id": "2112.06558", "pdf": "https://arxiv.org/pdf/2112.06558", "other": "https://arxiv.org/format/2112.06558"}, "title": "MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-based Image Captioning", "author_info": ["Wenqiao Zhang", "Haochen Shi", "Jiannan Guo", "Shengyu Zhang", "Qingpeng Cai", "Juncheng Li", "Sihui Luo", "Yueting Zhuang"], "summary": "Text-based image captioning (TextCap) requires simultaneous comprehension of visual content and reading the text of images to generate a natural language description. Although a task can teach machines to understand the complex human environment further given that text is omnipresent in our daily surroundings, it poses additional challenges in normal captioning. A text-based image intuitively contains abundant and complex multimodal relational content, that is, image details can be described diversely from multiview rather than a single caption. Certainly, we can introduce additional paired training data to show the diversity of images' descriptions, this process is labor-intensive and time-consuming for TextCap pair annotations with extra texts. Based on the insight mentioned above, we investigate how to generate diverse captions that focus on different image parts using an unpaired training paradigm. We propose the Multimodal relAtional Graph adversarIal inferenCe (MAGIC) framework for diverse and unpaired TextCap. This framework can adaptively construct multiple multimodal relational graphs of images and model complex relationships among graphs to represent descriptive diversity. Moreover, a cascaded generative adversarial network is developed from modeled graphs to infer the unpaired caption generation in image-sentence feature alignment and linguistic coherence levels. We validate the effectiveness of MAGIC in generating diverse captions from different relational information items of an image. Experimental results show that MAGIC can generate very promising outcomes without using any image-caption training pairs.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.06193", "id": "2112.06193", "pdf": "https://arxiv.org/pdf/2112.06193", "other": "https://arxiv.org/format/2112.06193"}, "title": "Multimodal-based Scene-Aware Framework for Aquatic Animal Segmentation", "author_info": ["Minh-Quan Le", "Trung-Nghia Le", "Tam V. Nguyen", "Isao Echizen", "Minh-Triet Tran"], "summary": "Recent years have witnessed great advances in object segmentation research. In addition to generic objects, aquatic animals have attracted research attention. Deep learning-based methods are widely used for aquatic animal segmentation and have achieved promising performance. However, there is a lack of challenging datasets for benchmarking. Therefore, we have created a new dataset dubbed \"Aquatic Animal Species.\" Furthermore, we devised a novel multimodal-based scene-aware segmentation framework that leverages the advantages of multiple view segmentation models to segment images of aquatic animals effectively. To improve training performance, we developed a guided mixup augmentation method. Extensive experiments comparing the performance of the proposed framework with state-of-the-art instance segmentation methods demonstrated that our method is effective and that it significantly outperforms existing methods.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.05827", "id": "2112.05827", "pdf": "https://arxiv.org/pdf/2112.05827", "other": "https://arxiv.org/format/2112.05827"}, "title": "Quality-Aware Multimodal Biometric Recognition", "author_info": ["Sobhan Soleymani", "Ali Dabouei", "Fariborz Taherkhani", "Seyed Mehdi Iranmanesh", "Jeremy Dawson", "Nasser M. Nasrabadi"], "summary": "We present a quality-aware multimodal recognition framework that combines representations from multiple biometric traits with varying quality and number of samples to achieve increased recognition accuracy by extracting complimentary identification information based on the quality of the samples. We develop a quality-aware framework for fusing representations of input modalities by weighting their importance using quality scores estimated in a weakly-supervised fashion. This framework utilizes two fusion blocks, each represented by a set of quality-aware and aggregation networks. In addition to architecture modifications, we propose two task-specific loss functions: multimodal separability loss and multimodal compactness loss. The first loss assures that the representations of modalities for a class have comparable magnitudes to provide a better quality estimation, while the multimodal representations of different classes are distributed to achieve maximum discrimination in the embedding space. The second loss, which is considered to regularize the network weights, improves the generalization performance by regularizing the framework. We evaluate the performance by considering three multimodal datasets consisting of face, iris, and fingerprint modalities. The efficacy of the framework is demonstrated through comparison with the state-of-the-art algorithms. In particular, our framework outperforms the rank- and score-level fusion of modalities of BIOMDATA by more than 30% for true acceptance rate at false acceptance rate of 10\u22124.", "comment": " Comments: IEEE Transactions on Biometrics, Behavior, and Identity Science "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.05587", "id": "2112.05587", "pdf": "https://arxiv.org/pdf/2112.05587", "other": "https://arxiv.org/format/2112.05587"}, "title": "Unified Multimodal Pre-training and Prompt-based Tuning for Vision-Language Understanding and Generation", "author_info": ["Tianyi Liu", "Zuxuan Wu", "Wenhan Xiong", "Jingjing Chen", "Yu-Gang Jiang"], "summary": "Most existing vision-language pre-training methods focus on understanding tasks and use BERT-like objectives (masked language modeling and image-text matching) during pretraining. Although they perform well in many understanding downstream tasks, e.g., visual question answering, image-text retrieval and visual entailment, they do not possess the ability to generate. To tackle this problem, we propose Unified multimodal pre-training for both Vision-Language understanding and generation (UniVL). The proposed UniVL is capable of handling both understanding tasks and generative tasks. We augment existing pretraining paradigms that only use random masks with causal masks, i.e., triangular masks that mask out future tokens, such that the pre-trained models can have autoregressive generation abilities by design. We formulate several previous understanding tasks as a text generation task and propose to use prompt-based method for fine-tuning on different downstream tasks. Our experiments show that there is a trade-off between understanding tasks and generation tasks while using the same model, and a feasible way to improve both tasks is to use more data. Our UniVL framework attains comparable performance to recent vision-language pre-training methods on both understanding tasks and generation tasks. Moreover, we demostrate that prompt-based finetuning is more data-efficient - it outperforms discriminative methods in few-shot scenarios.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.05328", "id": "2112.05328", "pdf": "https://arxiv.org/pdf/2112.05328", "other": "https://arxiv.org/format/2112.05328"}, "title": "Multimodal Interactions Using Pretrained Unimodal Models for SIMMC 2.0", "author_info": ["Joosung Lee", "Kijong Han"], "summary": "This paper presents our work on the Situated Interactive MultiModal Conversations 2.0 challenge held at Dialog State Tracking Challenge 10. SIMMC 2.0 includes 4 subtasks, and we introduce our multimodal approaches for the subtask \\#1, \\#2 and the generation of subtask \\#4. SIMMC 2.0 dataset is a multimodal dataset containing image and text information, which is more challenging than the problem of only text-based conversations because it must be solved by understanding the relationship between image and text. Therefore, since there is a limit to solving only text models such as BERT or GPT2, we propose a multimodal model combining image and text. We first pretrain the multimodal model to understand the relationship between image and text, then finetune our model for each task. We achieve the 3rd best performance in subtask \\#1, \\#2 and a runner-up in the generation of subtask \\#4. The source code is available at https://github.com/rungjoo/simmc2.0.", "comment": " Comments: Accepted to DSTC10 challenge wokrshop at AAAI 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.05253", "id": "2112.05253", "pdf": "https://arxiv.org/pdf/2112.05253", "other": "https://arxiv.org/format/2112.05253"}, "title": "MAGMA -- Multimodal Augmentation of Generative Models through Adapter-based Finetuning", "author_info": ["Constantin Eichenberg", "Sidney Black", "Samuel Weinbach", "Letitia Parcalabescu", "Anette Frank"], "summary": "Large-scale pretraining is fast becoming the norm in Vision-Language (VL) modeling. However, prevailing VL approaches are limited by the requirement for labeled data and the use of complex multi-step pretraining objectives. We present MAGMA - a simple method for augmenting generative language models with additional modalities using adapter-based finetuning. Building on Frozen, we train a series of VL models that autoregressively generate text from arbitrary combinations of visual and textual input. The pretraining is entirely end-to-end using a single language modeling objective, simplifying optimization compared to previous approaches. Importantly, the language model weights remain unchanged during training, allowing for transfer of encyclopedic knowledge and in-context learning abilities from language pretraining. MAGMA outperforms Frozen on open-ended generative tasks, achieving state of the art results on the OKVQA benchmark and competitive results on a range of other popular VL benchmarks, while pretraining on 0.2% of the number of samples used to train SimVLM.", "comment": " ACM Class:           I.2.7; I.4.8; I.5.1                "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.05130", "id": "2112.05130", "pdf": "https://arxiv.org/pdf/2112.05130", "other": "https://arxiv.org/format/2112.05130"}, "title": "Multimodal Conditional Image Synthesis with Product-of-Experts GANs", "author_info": ["Xun Huang", "Arun Mallya", "Ting-Chun Wang", "Ming-Yu Liu"], "summary": "Existing conditional image synthesis frameworks generate images based on user inputs in a single modality, such as text, segmentation, sketch, or style reference. They are often unable to leverage multimodal user inputs when available, which reduces their practicality. To address this limitation, we propose the Product-of-Experts Generative Adversarial Networks (PoE-GAN) framework, which can synthesize images conditioned on multiple input modalities or any subset of them, even the empty set. PoE-GAN consists of a product-of-experts generator and a multimodal multiscale projection discriminator. Through our carefully designed training scheme, PoE-GAN learns to synthesize images with high quality and diversity. Besides advancing the state of the art in multimodal conditional image synthesis, PoE-GAN also outperforms the best existing unimodal conditional image synthesis approaches when tested in the unimodal setting. The project website is available at https://deepimagination.github.io/PoE-GAN .", "comment": ""}]}