{"data": [{"arxiv": {"page": "https://arxiv.org/abs/2202.07712", "id": "2202.07712", "pdf": "https://arxiv.org/pdf/2202.07712", "other": "https://arxiv.org/format/2202.07712"}, "title": "Privacy Preserving Visual Question Answering", "author_info": ["Cristian-Paul Bara", "Qing Ping", "Abhinav Mathur", "Govind Thattai", "Rohith MV", "Gaurav S. Sukhatme"], "summary": "We introduce a novel privacy-preserving methodology for performing Visual Question Answering on the edge. Our method constructs a symbolic representation of the visual scene, using a low-complexity computer vision model that jointly predicts classes, attributes and predicates. This symbolic representation is non-differentiable, which means it cannot be used to recover the original image, thereby keeping the original image private. Our proposed hybrid solution uses a vision model which is more than 25 times smaller than the current state-of-the-art (SOTA) vision models, and 100 times smaller than end-to-end SOTA VQA models. We report detailed error analysis and discuss the trade-offs of using a distilled vision model and a symbolic representation of the visual scene.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07630", "id": "2202.07630", "pdf": "https://arxiv.org/pdf/2202.07630", "other": "https://arxiv.org/format/2202.07630"}, "title": "Delving Deeper into Cross-lingual Visual Question Answering", "author_info": ["Chen Liu", "Jonas Pfeiffer", "Anna Korhonen", "Ivan Vulic", "Iryna Gurevych"], "summary": "Visual question answering (VQA) is one of the crucial vision-and-language tasks. Yet, the bulk of research until recently has focused only on the English language due to the lack of appropriate evaluation resources. Previous work on cross-lingual VQA has reported poor zero-shot transfer performance of current multilingual multimodal Transformers and large gaps to monolingual performance, attributed mostly to misalignment of text embeddings between the source and target languages, without providing any additional deeper analyses. In this work, we delve deeper and address different aspects of cross-lingual VQA holistically, aiming to understand the impact of input data, fine-tuning and evaluation regimes, and interactions between the two modalities in cross-lingual setups. 1) We tackle low transfer performance via novel methods that substantially reduce the gap to monolingual English performance, yielding +10 accuracy points over existing transfer methods. 2) We study and dissect cross-lingual VQA across different question types of varying complexity, across different multilingual multi-modal Transformers, and in zero-shot and few-shot scenarios. 3) We further conduct extensive analyses on modality biases in training data and models, aimed to further understand why zero-shot performance gaps remain for some question types and languages. We hope that the novel methods and detailed analyses will guide further progress in multilingual VQA.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.04306", "id": "2202.04306", "pdf": "https://arxiv.org/pdf/2202.04306", "other": "https://arxiv.org/format/2202.04306"}, "title": "Can Open Domain Question Answering Systems Answer Visual Knowledge Questions?", "author_info": ["Jiawen Zhang", "Abhijit Mishra", "Avinesh P. V. S", "Siddharth Patwardhan", "Sachin Agarwal"], "summary": "The task of Outside Knowledge Visual Question Answering (OKVQA) requires an automatic system to answer natural language questions about pictures and images using external knowledge. We observe that many visual questions, which contain deictic referential phrases referring to entities in the image, can be rewritten as \"non-grounded\" questions and can be answered by existing text-based question answering systems. This allows for the reuse of existing text-based Open Domain Question Answering (QA) Systems for visual question answering. In this work, we propose a potentially data-efficient approach that reuses existing systems for (a) image analysis, (b) question rewriting, and (c) text-based question answering to answer such visual questions. Given an image and a question pertaining to that image (a visual question), we first extract the entities present in the image using pre-trained object and scene classifiers. Using these detected entities, the visual questions can be rewritten so as to be answerable by open domain QA systems. We explore two rewriting strategies: (1) an unsupervised method using BERT for masking and rewriting, and (2) a weakly supervised approach that combines adaptive rewriting and reinforcement learning techniques to use the implicit feedback from the QA system. We test our strategies on the publicly available OKVQA dataset and obtain a competitive performance with state-of-the-art models while using only 10% of the training data.", "comment": " Comments: 9 pages (including references), 5 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.01993", "id": "2202.01993", "pdf": "https://arxiv.org/pdf/2202.01993", "other": "https://arxiv.org/format/2202.01993"}, "title": "Grounding Answers for Visual Questions Asked by Visually Impaired People", "author_info": ["Chongyan Chen", "Samreen Anjum", "Danna Gurari"], "summary": "Visual question answering is the task of answering questions about images. We introduce the VizWiz-VQA-Grounding dataset, the first dataset that visually grounds answers to visual questions asked by people with visual impairments. We analyze our dataset and compare it with five VQA-Grounding datasets to demonstrate what makes it similar and different. We then evaluate the SOTA VQA and VQA-Grounding models and demonstrate that current SOTA algorithms often fail to identify the correct visual evidence where the answer is located. These models regularly struggle when the visual evidence occupies a small fraction of the image, for images that are higher quality, as well as for visual questions that require skills in text recognition. The dataset, evaluation server, and leaderboard all can be found at the following link: https://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.", "comment": " Comments: Computer Vision and Pattern Recognition "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11316", "id": "2201.11316", "pdf": "https://arxiv.org/pdf/2201.11316", "other": "https://arxiv.org/format/2201.11316"}, "title": "Transformer Module Networks for Systematic Generalization in Visual Question Answering", "author_info": ["Moyuru Yamada", "Vanessa D'Amario", "Kentaro Takemoto", "Xavier Boix", "Tomotake Sasaki"], "summary": "Transformer-based models achieve great performance on Visual Question Answering (VQA). However, when we evaluate them on systematic generalization, i.e., handling novel combinations of known concepts, their performance degrades. Neural Module Networks (NMNs) are a promising approach for systematic generalization that consists on composing modules, i.e., neural networks that tackle a sub-task. Inspired by Transformers and NMNs, we propose Transformer Module Network (TMN), a novel Transformer-based model for VQA that dynamically composes modules into a question-specific Transformer network. TMNs achieve state-of-the-art systematic generalization performance in three VQA datasets, namely, CLEVR-CoGenT, CLOSURE and GQA-SGL, in some cases improving more than 30% over standard Transformers.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.10656", "id": "2201.10656", "pdf": "https://arxiv.org/pdf/2201.10656", "other": "https://arxiv.org/format/2201.10656"}, "title": "MGA-VQA: Multi-Granularity Alignment for Visual Question Answering", "author_info": ["Peixi Xiong", "Yilin Shen", "Hongxia Jin"], "summary": "Learning to answer visual questions is a challenging task since the multi-modal inputs are within two feature spaces. Moreover, reasoning in visual question answering requires the model to understand both image and question, and align them in the same space, rather than simply memorize statistics about the question-answer pairs. Thus, it is essential to find component connections between different modalities and within each modality to achieve better attention. Previous works learned attention weights directly on the features. However, the improvement is limited since these two modality features are in two domains: image features are highly diverse, lacking structure and grammatical rules as language, and natural language features have a higher probability of missing detailed information. To better learn the attention between visual and text, we focus on how to construct input stratification and embed structural information to improve the alignment between different level components. We propose Multi-Granularity Alignment architecture for Visual Question Answering task (MGA-VQA), which learns intra- and inter-modality correlations by multi-granularity alignment, and outputs the final result by the decision fusion module. In contrast to previous works, our model splits alignment into different levels to achieve learning better correlations without needing additional data and annotations. The experiments on the VQA-v2 and GQA datasets demonstrate that our model significantly outperforms non-pretrained state-of-the-art methods on both datasets without extra pretraining data and annotations. Moreover, it even achieves better results over the pre-trained methods on GQA.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.10654", "id": "2201.10654", "pdf": "https://arxiv.org/pdf/2201.10654", "other": "https://arxiv.org/format/2201.10654"}, "title": "SA-VQA: Structured Alignment of Visual and Semantic Representations for Visual Question Answering", "author_info": ["Peixi Xiong", "Quanzeng You", "Pei Yu", "Zicheng Liu", "Ying Wu"], "summary": "Visual Question Answering (VQA) attracts much attention from both industry and academia. As a multi-modality task, it is challenging since it requires not only visual and textual understanding, but also the ability to align cross-modality representations. Previous approaches extensively employ entity-level alignments, such as the correlations between the visual regions and their semantic labels, or the interactions across question words and object features. These attempts aim to improve the cross-modality representations, while ignoring their internal relations. Instead, we propose to apply structured alignments, which work with graph representation of visual and textual content, aiming to capture the deep connections between the visual and textual modalities. Nevertheless, it is nontrivial to represent and integrate graphs for structured alignments. In this work, we attempt to solve this issue by first converting different modality entities into sequential nodes and the adjacency graph, then incorporating them for structured alignments. As demonstrated in our experimental results, such a structured alignment improves reasoning performance. In addition, our model also exhibits better interpretability for each generated answer. The proposed model, without any pretraining, outperforms the state-of-the-art methods on GQA dataset, and beats the non-pretrained state-of-the-art methods on VQA-v2 dataset.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.05299", "id": "2201.05299", "pdf": "https://arxiv.org/pdf/2201.05299", "other": "https://arxiv.org/format/2201.05299"}, "title": "A Thousand Words Are Worth More Than a Picture: Natural Language-Centric Outside-Knowledge Visual Question Answering", "author_info": ["Feng Gao", "Qing Ping", "Govind Thattai", "Aishwarya Reganti", "Ying Nian Wu", "Prem Natarajan"], "summary": "Outside-knowledge visual question answering (OK-VQA) requires the agent to comprehend the image, make use of relevant knowledge from the entire web, and digest all the information to answer the question. Most previous works address the problem by first fusing the image and question in the multi-modal space, which is inflexible for further fusion with a vast amount of external knowledge. In this paper, we call for a paradigm shift for the OK-VQA task, which transforms the image into plain text, so that we can enable knowledge passage retrieval, and generative question-answering in the natural language space. This paradigm takes advantage of the sheer volume of gigantic knowledge bases and the richness of pre-trained language models. A Transform-Retrieve-Generate framework (TRiG) framework is proposed, which can be plug-and-played with alternative image-to-text models and textual knowledge bases. Experimental results show that our TRiG framework outperforms all state-of-the-art supervised methods by at least 11.1% absolute margin.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.03965", "id": "2201.03965", "pdf": "https://arxiv.org/pdf/2201.03965", "other": "https://arxiv.org/format/2201.03965"}, "title": "On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering", "author_info": ["Ankur Sikarwar", "Gabriel Kreiman"], "summary": "In recent years, multi-modal transformers have shown significant progress in Vision-Language tasks, such as Visual Question Answering (VQA), outperforming previous architectures by a considerable margin. This improvement in VQA is often attributed to the rich interactions between vision and language streams. In this work, we investigate the efficacy of co-attention transformer layers in helping the network focus on relevant regions while answering the question. We generate visual attention maps using the question-conditioned image attention scores in these co-attention layers. We evaluate the effect of the following critical components on visual attention of a state-of-the-art VQA model: (i) number of object region proposals, (ii) question part of speech (POS) tags, (iii) question semantics, (iv) number of co-attention layers, and (v) answer accuracy. We compare the neural network attention maps against human attention maps both qualitatively and quantitatively. Our findings indicate that co-attention transformer modules are crucial in attending to relevant regions of the image given a question. Importantly, we observe that the semantic meaning of the question is not what drives visual attention, but specific keywords in the question do. Our work sheds light on the function and interpretation of co-attention transformer layers, highlights gaps in current networks, and can guide the development of future VQA models and networks that simultaneously process visual and language streams.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.13906", "id": "2112.13906", "pdf": "https://arxiv.org/pdf/2112.13906", "other": "https://arxiv.org/format/2112.13906"}, "title": "Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?", "author_info": ["Sedigheh Eslami", "Gerard de Melo", "Christoph Meinel"], "summary": "Contrastive Language--Image Pre-training (CLIP) has shown remarkable success in learning with cross-modal supervision from extensive amounts of image--text pairs collected online. Thus far, the effectiveness of CLIP has been investigated primarily in general-domain multimodal problems. This work evaluates the effectiveness of CLIP for the task of Medical Visual Question Answering (MedVQA). To this end, we present PubMedCLIP, a fine-tuned version of CLIP for the medical domain based on PubMed articles. Our experiments are conducted on two MedVQA benchmark datasets and investigate two MedVQA methods, MEVF (Mixture of Enhanced Visual Features) and QCR (Question answering via Conditional Reasoning). For each of these, we assess the merits of visual representation learning using PubMedCLIP, the original CLIP, and state-of-the-art MAML (Model-Agnostic Meta-Learning) networks pre-trained only on visual data. We open source the code for our MedVQA pipeline and pre-training PubMedCLIP. CLIP and PubMedCLIP achieve improvements in comparison to MAML's visual encoder. PubMedCLIP achieves the best results with gains in the overall accuracy of up to 3%. Individual examples illustrate the strengths of PubMedCLIP in comparison to the previously widely used MAML networks. Visual representation learning with language supervision in PubMedCLIP leads to noticeable improvements for MedVQA. Our experiments reveal distributional differences in the two MedVQA benchmark datasets that have not been imparted in previous work and cause different back-end visual encoders in PubMedCLIP to exhibit different behavior on these datasets. Moreover, we witness fundamental performance differences of VQA in general versus medical domains.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.13706", "id": "2112.13706", "pdf": "https://arxiv.org/pdf/2112.13706", "other": "https://arxiv.org/format/2112.13706"}, "title": "Multi-Image Visual Question Answering", "author_info": ["Harsh Raj", "Janhavi Dadhania", "Akhilesh Bhardwaj", "Prabuchandran KJ"], "summary": "While a lot of work has been done on developing models to tackle the problem of Visual Question Answering, the ability of these models to relate the question to the image features still remain less explored. We present an empirical study of different feature extraction methods with different loss functions. We propose New dataset for the task of Visual Question Answering with multiple image inputs having only one ground truth, and benchmark our results on them. Our final model utilising Resnet + RCNN image features and Bert embeddings, inspired from stacked attention network gives 39% word accuracy and 99% image accuracy on CLEVER+TinyImagenet dataset.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.11691", "id": "2112.11691", "pdf": "https://arxiv.org/pdf/2112.11691", "other": "https://arxiv.org/format/2112.11691"}, "title": "CLEVR3D: Compositional Language and Elementary Visual Reasoning for Question Answering in 3D Real-World Scenes", "author_info": ["Xu Yan", "Zhihao Yuan", "Yuhao Du", "Yinghong Liao", "Yao Guo", "Zhen Li", "Shuguang Cui"], "summary": "3D scene understanding is a relatively emerging research field. In this paper, we introduce the Visual Question Answering task in 3D real-world scenes (VQA-3D), which aims to answer all possible questions given a 3D scene. To tackle this problem, the first VQA-3D dataset, namely CLEVR3D, is proposed, which contains 60K questions in 1,129 real-world scenes. Specifically, we develop a question engine leveraging 3D scene graph structures to generate diverse reasoning questions, covering the questions of objects' attributes (i.e., size, color, and material) and their spatial relationships. Built upon this dataset, we further design the first VQA-3D baseline model, TransVQA3D. The TransVQA3D model adopts well-designed Transformer architectures to achieve superior VQA-3D performance, compared with the pure language baseline and previous 3D reasoning methods directly applied to 3D scenarios. Experimental results verify that taking VQA-3D as an auxiliary task can boost the performance of 3D scene understanding, including scene graph analysis for the node-wise classification and whole-graph recognition.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07668", "id": "2112.07668", "pdf": "https://arxiv.org/pdf/2112.07668", "other": "https://arxiv.org/format/2112.07668"}, "title": "Dual-Key Multimodal Backdoors for Visual Question Answering", "author_info": ["Matthew Walmer", "Karan Sikka", "Indranil Sur", "Abhinav Shrivastava", "Susmit Jha"], "summary": "The success of deep learning has enabled advances in multimodal tasks that require non-trivial fusion of multiple input domains. Although multimodal models have shown potential in many problems, their increased complexity makes them more vulnerable to attacks. A Backdoor (or Trojan) attack is a class of security vulnerability wherein an attacker embeds a malicious secret behavior into a network (e.g. targeted misclassification) that is activated when an attacker-specified trigger is added to an input. In this work, we show that multimodal networks are vulnerable to a novel type of attack that we refer to as Dual-Key Multimodal Backdoors. This attack exploits the complex fusion mechanisms used by state-of-the-art networks to embed backdoors that are both effective and stealthy. Instead of using a single trigger, the proposed attack embeds a trigger in each of the input modalities and activates the malicious behavior only when both the triggers are present. We present an extensive study of multimodal backdoors on the Visual Question Answering (VQA) task with multiple architectures and visual feature backbones. A major challenge in embedding backdoors in VQA models is that most models use visual features extracted from a fixed pretrained object detector. This is challenging for the attacker as the detector can distort or ignore the visual trigger entirely, which leads to models where backdoors are over-reliant on the language trigger. We tackle this problem by proposing a visual trigger optimization strategy designed for pretrained object detectors. Through this method, we create Dual-Key Backdoors with over a 98% attack success rate while only poisoning 1% of the training data. Finally, we release TrojVQA, a large collection of clean and trojan VQA models to enable research in defending against multimodal backdoors.", "comment": " Comments: 22 pages, 11 figures, 12 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07270", "id": "2112.07270", "pdf": "https://arxiv.org/pdf/2112.07270", "other": "https://arxiv.org/format/2112.07270"}, "title": "Bilateral Cross-Modality Graph Matching Attention for Feature Fusion in Visual Question Answering", "author_info": ["JianJian Cao", "Xiameng Qin", "Sanyuan Zhao", "Jianbing Shen"], "summary": "Answering semantically-complicated questions according to an image is challenging in Visual Question Answering (VQA) task. Although the image can be well represented by deep learning, the question is always simply embedded and cannot well indicate its meaning. Besides, the visual and textual features have a gap for different modalities, it is difficult to align and utilize the cross-modality information. In this paper, we focus on these two problems and propose a Graph Matching Attention (GMA) network. Firstly, it not only builds graph for the image, but also constructs graph for the question in terms of both syntactic and embedding information. Next, we explore the intra-modality relationships by a dual-stage graph encoder and then present a bilateral cross-modality graph matching attention to infer the relationships between the image and the question. The updated cross-modality features are then sent into the answer prediction module for final answer prediction. Experiments demonstrate that our network achieves state-of-the-art performance on the GQA dataset and the VQA 2.0 dataset. The ablation studies verify the effectiveness of each modules in our GMA network.", "comment": " Comments: pre-print, TNNLS, 12 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.06888", "id": "2112.06888", "pdf": "https://arxiv.org/pdf/2112.06888", "other": "https://arxiv.org/format/2112.06888"}, "title": "Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection", "author_info": ["Diego Garcia-Olano", "Yasumasa Onoe", "Joydeep Ghosh"], "summary": "Knowledge-Based Visual Question Answering (KBVQA) is a bi-modal task requiring external world knowledge in order to correctly answer a text question and associated image. Recent single modality text work has shown knowledge injection into pre-trained language models, specifically entity enhanced knowledge graph embeddings, can improve performance on downstream entity-centric tasks. In this work, we empirically study how and whether such methods, applied in a bi-modal setting, can improve an existing VQA system's performance on the KBVQA task. We experiment with two large publicly available VQA datasets, (1) KVQA which contains mostly rare Wikipedia entities and (2) OKVQA which is less entity-centric and more aligned with common sense reasoning. Both lack explicit entity spans and we study the effect of different weakly supervised and manual methods for obtaining them. Additionally we analyze how recently proposed bi-modal and single modal attention explanations are affected by the incorporation of such entity enhanced representations. Our results show substantial improved performance on the KBVQA task without the need for additional costly pre-training and we provide insights for when entity knowledge injection helps improve a model's understanding. We provide code and enhanced datasets for reproducibility.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.06343", "id": "2112.06343", "pdf": "https://arxiv.org/pdf/2112.06343", "other": "https://arxiv.org/format/2112.06343"}, "title": "Change Detection Meets Visual Question Answering", "author_info": ["Zhenghang Yuan", "Lichao Mou", "Zhitong Xiong", "Xiaoxiang Zhu"], "summary": "The Earth's surface is continually changing, and identifying changes plays an important role in urban planning and sustainability. Although change detection techniques have been successfully developed for many years, these techniques are still limited to experts and facilitators in related fields. In order to provide every user with flexible access to change information and help them better understand land-cover changes, we introduce a novel task: change detection-based visual question answering (CDVQA) on multi-temporal aerial images. In particular, multi-temporal images can be queried to obtain high level change-based information according to content changes between two input images. We first build a CDVQA dataset including multi-temporal image-question-answer triplets using an automatic question-answer generation method. Then, a baseline CDVQA framework is devised in this work, and it contains four parts: multi-temporal feature encoding, multi-temporal fusion, multi-modal fusion, and answer prediction. In addition, we also introduce a change enhancing module to multi-temporal feature encoding, aiming at incorporating more change-related information. Finally, effects of different backbones and multi-temporal fusion strategies are studied on the performance of CDVQA task. The experimental results provide useful insights for developing better CDVQA models, which are important for future research on this task. We will make our dataset and code publicly available.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.14547", "id": "2111.14547", "pdf": "https://arxiv.org/pdf/2111.14547", "other": "https://arxiv.org/format/2111.14547"}, "title": "LiVLR: A Lightweight Visual-Linguistic Reasoning Framework for Video Question Answering", "author_info": ["Jingjing Jiang", "Ziyi Liu", "Nanning Zheng"], "summary": "Video Question Answering (VideoQA), aiming to correctly answer the given question based on understanding multi-modal video content, is challenging due to the rich video content. From the perspective of video understanding, a good VideoQA framework needs to understand the video content at different semantic levels and flexibly integrate the diverse video content to distill question-related content. To this end, we propose a Lightweight Visual-Linguistic Reasoning framework named LiVLR. Specifically, LiVLR first utilizes the graph-based Visual and Linguistic Encoders to obtain multi-grained visual and linguistic representations. Subsequently, the obtained representations are integrated with the devised Diversity-aware Visual-Linguistic Reasoning module (DaVL). The DaVL considers the difference between the different types of representations and can flexibly adjust the importance of different types of representations when generating the question-related joint representation, which is an effective and general representation integration method. The proposed LiVLR is lightweight and shows its performance advantage on two VideoQA benchmarks, MRSVTT-QA and KnowIT VQA. Extensive ablation studies demonstrate the effectiveness of LiVLR key components.", "comment": " Comments: 11 pages, 5 figures, Code: https://github.com/jingjing12110/LiVLR-VideoQA "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.10056", "id": "2111.10056", "pdf": "https://arxiv.org/pdf/2111.10056", "other": "https://arxiv.org/format/2111.10056"}, "title": "Medical Visual Question Answering: A Survey", "author_info": ["Zhihong Lin", "Donghao Zhang", "Qingyi Tac", "Danli Shi", "Gholamreza Haffari", "Qi Wu", "Mingguang He", "Zongyuan Ge"], "summary": "Medical Visual Question Answering~(VQA) is a combination of medical artificial intelligence and popular VQA challenges. Given a medical image and a clinically relevant question in natural language, the medical VQA system is expected to predict a plausible and convincing answer. Although the general-domain VQA has been extensively studied, the medical VQA still needs specific investigation and exploration due to its task features. In the first part of this survey, we collect and discuss the publicly available medical VQA datasets up to date about the data source, data quantity, and task feature. In the second part, we review the approaches used in medical VQA tasks. We summarize and discuss their techniques, innovation, and potential improvement. In the last part, we analyze some medical-specific challenges for the field and discuss future research directions. Our goal is to provide comprehensive information for researchers interested in medical artificial intelligence.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.08896", "id": "2111.08896", "pdf": "https://arxiv.org/pdf/2111.08896", "other": "https://arxiv.org/format/2111.08896"}, "title": "Achieving Human Parity on Visual Question Answering", "author_info": ["Ming Yan", "Haiyang Xu", "Chenliang Li", "Junfeng Tian", "Bin Bi", "Wei Wang", "Weihua Chen", "Xianzhe Xu", "Fan Wang", "Zheng Cao", "Zhicheng Zhang", "Qiyu Zhang", "Ji Zhang", "Songfang Huang", "Fei Huang", "Luo Si", "Rong Jin"], "summary": "The Visual Question Answering (VQA) task utilizes both visual image and language analysis to answer a textual question with respect to an image. It has been a popular research topic with an increasing number of real-world applications in the last decade. This paper describes our recent research of AliceMind-MMU (ALIbaba's Collection of Encoder-decoders from Machine IntelligeNce lab of Damo academy - MultiMedia Understanding) that obtains similar or even slightly better results than human being does on VQA. This is achieved by systematically improving the VQA pipeline including: (1) pre-training with comprehensive visual and textual feature representation; (2) effective cross-modal interaction with learning to attend; and (3) A novel knowledge mining framework with specialized expert modules for the complex VQA task. Treating different types of visual questions with corresponding expertise needed plays an important role in boosting the performance of our VQA architecture up to the human level. An extensive set of experiments and analysis are conducted to demonstrate the effectiveness of the new research work.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.08531", "id": "2111.08531", "pdf": "https://arxiv.org/pdf/2111.08531", "other": "https://arxiv.org/format/2111.08531"}, "title": "Language bias in Visual Question Answering: A Survey and Taxonomy", "author_info": ["Desen Yuan"], "summary": "Visual question answering (VQA) is a challenging task, which has attracted more and more attention in the field of computer vision and natural language processing. However, the current visual question answering has the problem of language bias, which reduces the robustness of the model and has an adverse impact on the practical application of visual question answering. In this paper, we conduct a comprehensive review and analysis of this field for the first time, and classify the existing methods according to three categories, including enhancing visual information, weakening language priors, data enhancement and training strategies. At the same time, the relevant representative methods are introduced, summarized and analyzed in turn. The causes of language bias are revealed and classified. Secondly, this paper introduces the datasets mainly used for testing, and reports the experimental results of various existing methods. Finally, we discuss the possible future research directions in this field.", "comment": " Comments: 10 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.04785", "id": "2111.04785", "pdf": "https://arxiv.org/pdf/2111.04785", "other": "https://arxiv.org/format/2111.04785"}, "title": "Visual Question Answering based on Formal Logic", "author_info": ["Muralikrishnna G. Sethuraman", "Ali Payani", "Faramarz Fekri", "J. Clayton Kerce"], "summary": "Visual question answering (VQA) has been gaining a lot of traction in the machine learning community in the recent years due to the challenges posed in understanding information coming from multiple modalities (i.e., images, language). In VQA, a series of questions are posed based on a set of images and the task at hand is to arrive at the answer. To achieve this, we take a symbolic reasoning based approach using the framework of formal logic. The image and the questions are converted into symbolic representations on which explicit reasoning is performed. We propose a formal logic framework where (i) images are converted to logical background facts with the help of scene graphs, (ii) the questions are translated to first-order predicate logic clauses using a transformer based deep learning model, and (iii) perform satisfiability checks, by using the background knowledge and the grounding of predicate clauses, to obtain the answer. Our proposed method is highly interpretable and each step in the pipeline can be easily analyzed by a human. We validate our approach on the CLEVR and the GQA dataset. We achieve near perfect accuracy of 99.6% on the CLEVR dataset comparable to the state of art models, showcasing that formal logic is a viable tool to tackle visual question answering. Our model is also data efficient, achieving 99.1% accuracy on CLEVR dataset when trained on just 10% of the training data.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.10906", "id": "2110.10906", "pdf": "https://arxiv.org/pdf/2110.10906", "other": "https://arxiv.org/format/2110.10906"}, "title": "Single-Modal Entropy based Active Learning for Visual Question Answering", "author_info": ["Dong-Jin Kim", "Jae Won Cho", "Jinsoo Choi", "Yunjae Jung", "In So Kweon"], "summary": "Constructing a large-scale labeled dataset in the real world, especially for high-level tasks (eg, Visual Question Answering), can be expensive and time-consuming. In addition, with the ever-growing amounts of data and architecture complexity, Active Learning has become an important aspect of computer vision research. In this work, we address Active Learning in the multi-modal setting of Visual Question Answering (VQA). In light of the multi-modal inputs, image and question, we propose a novel method for effective sample acquisition through the use of ad hoc single-modal branches for each input to leverage its information. Our mutual information based sample acquisition strategy Single-Modal Entropic Measure (SMEM) in addition to our self-distillation technique enables the sample acquisitor to exploit all present modalities and find the most informative samples. Our novel idea is simple to implement, cost-efficient, and readily adaptable to other multi-modal tasks. We confirm our findings on various VQA datasets through state-of-the-art performance by comparing to existing Active Learning baselines.", "comment": " Comments: Accepted to BMVC 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.05159", "id": "2110.05159", "pdf": "https://arxiv.org/pdf/2110.05159", "other": "https://arxiv.org/format/2110.05159"}, "title": "Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking", "author_info": ["Dirk V\u00e4th", "Pascal Tilli", "Ngoc Thang Vu"], "summary": "On the way towards general Visual Question Answering (VQA) systems that are able to answer arbitrary questions, the need arises for evaluation beyond single-metric leaderboards for specific datasets. To this end, we propose a browser-based benchmarking tool for researchers and challenge organizers, with an API for easy integration of new models and datasets to keep up with the fast-changing landscape of VQA. Our tool helps test generalization capabilities of models across multiple datasets, evaluating not just accuracy, but also performance in more realistic real-world scenarios such as robustness to input noise. Additionally, we include metrics that measure biases and uncertainty, to further explain model behavior. Interactive filtering facilitates discovery of problematic behavior, down to the data sample level. As proof of concept, we perform a case study on four models. We find that state-of-the-art VQA models are optimized for specific tasks or datasets, but fail to generalize even to other in-domain test sets, for example they cannot recognize text in images. Our metrics allow us to quantify which image and question embeddings provide most robustness to a model. All code is publicly available.", "comment": " Comments: 6.5 pages, 9 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.05122", "id": "2110.05122", "pdf": "https://arxiv.org/pdf/2110.05122", "other": "https://arxiv.org/format/2110.05122"}, "title": "Pano-AVQA: Grounded Audio-Visual Question Answering on 360\u2218 Videos", "author_info": ["Heeseung Yun", "Youngjae Yu", "Wonsuk Yang", "Kangil Lee", "Gunhee Kim"], "summary": "360\u2218 videos convey holistic views for the surroundings of a scene. It provides audio-visual cues beyond pre-determined normal field of views and displays distinctive spatial relations on a sphere. However, previous benchmark tasks for panoramic videos are still limited to evaluate the semantic understanding of audio-visual relationships or spherical spatial property in surroundings. We propose a novel benchmark named Pano-AVQA as a large-scale grounded audio-visual question answering dataset on panoramic videos. Using 5.4K 360\u2218 video clips harvested online, we collect two types of novel question-answer pairs with bounding-box grounding: spherical spatial relation QAs and audio-visual relation QAs. We train several transformer-based models from Pano-AVQA, where the results suggest that our proposed spherical spatial embeddings and multimodal training objectives fairly contribute to a better semantic understanding of the panoramic surroundings on the dataset.", "comment": " Comments: Published to ICCV2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.02526", "id": "2110.02526", "pdf": "https://arxiv.org/pdf/2110.02526", "other": "https://arxiv.org/format/2110.02526"}, "title": "Coarse-to-Fine Reasoning for Visual Question Answering", "author_info": ["Binh X. Nguyen", "Tuong Do", "Huy Tran", "Erman Tjiputra", "Quang D. Tran", "Anh Nguyen"], "summary": "Bridging the semantic gap between image and question is an important step to improve the accuracy of the Visual Question Answering (VQA) task. However, most of the existing VQA methods focus on attention mechanisms or visual relations for reasoning the answer, while the features at different semantic levels are not fully utilized. In this paper, we present a new reasoning framework to fill the gap between visual features and semantic clues in the VQA task. Our method first extracts the features and predicates from the image and question. We then propose a new reasoning framework to effectively jointly learn these features and predicates in a coarse-to-fine manner. The intensively experimental results on three large-scale VQA datasets show that our proposed approach achieves superior accuracy comparing with other state-of-the-art methods. Furthermore, our reasoning framework also provides an explainable way to understand the decision of the deep neural network when predicting the answer.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.01013", "id": "2110.01013", "pdf": "https://arxiv.org/pdf/2110.01013", "other": "https://arxiv.org/format/2110.01013"}, "title": "Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering", "author_info": ["Long Chen", "Yuhang Zheng", "Yulei Niu", "Hanwang Zhang", "Jun Xiao"], "summary": "Today's VQA models still tend to capture superficial linguistic correlations in the training set and fail to generalize to the test set with different QA distributions. To reduce these language biases, recent VQA works introduce an auxiliary question-only model to regularize the training of targeted VQA model, and achieve dominating performance on diagnostic benchmarks for out-of-distribution testing. However, due to complex model design, these ensemble-based methods are unable to equip themselves with two indispensable characteristics of an ideal VQA model: 1) Visual-explainable: The model should rely on the right visual regions when making decisions. 2) Question-sensitive: The model should be sensitive to the linguistic variations in questions. To this end, we propose a novel model-agnostic Counterfactual Samples Synthesizing and Training (CSST) strategy. After training with CSST, VQA models are forced to focus on all critical objects and words, which significantly improves both visual-explainable and question-sensitive abilities. Specifically, CSST is composed of two parts: Counterfactual Samples Synthesizing (CSS) and Counterfactual Samples Training (CST). CSS generates counterfactual samples by carefully masking critical objects in images or words in questions and assigning pseudo ground-truth answers. CST not only trains the VQA models with both complementary samples to predict respective ground-truth answers, but also urges the VQA models to further distinguish the original samples and superficially similar counterfactual ones. To facilitate the CST training, we propose two variants of supervised contrastive loss for VQA, and design an effective positive and negative sample selection mechanism based on CSS. Extensive experiments have shown the effectiveness of CSST. Particularly, by building on top of model LMH+SAR, we achieve record-breaking performance on all OOD benchmarks.", "comment": " Comments: Extension of CVPR'20 work (Counterfactual Samples Synthesizing for Robust Visual Question Answering). arXiv admin note: substantial text overlap with arXiv:2003.06576 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.13139", "id": "2109.13139", "pdf": "https://arxiv.org/pdf/2109.13139", "other": "https://arxiv.org/format/2109.13139"}, "title": "Multimodal Integration of Human-Like Attention in Visual Question Answering", "author_info": ["Ekta Sood", "Fabian K\u00f6gel", "Philipp M\u00fcller", "Dominike Thomas", "Mihai Bace", "Andreas Bulling"], "summary": "Human-like attention as a supervisory signal to guide neural attention has shown significant promise but is currently limited to uni-modal integration - even for inherently multimodal tasks such as visual question answering (VQA). We present the Multimodal Human-like Attention Network (MULAN) - the first method for multimodal integration of human-like attention on image and text during training of VQA models. MULAN integrates attention predictions from two state-of-the-art text and image saliency models into neural self-attention layers of a recent transformer-based VQA model. Through evaluations on the challenging VQAv2 dataset, we show that MULAN achieves a new state-of-the-art performance of 73.98% accuracy on test-std and 73.72% on test-dev and, at the same time, has approximately 80% fewer trainable parameters than prior work. Overall, our work underlines the potential of integrating multimodal human-like and neural attention for VQA", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2109.13116", "id": "2109.13116", "pdf": "https://arxiv.org/pdf/2109.13116", "other": "https://arxiv.org/format/2109.13116"}, "title": "VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering", "author_info": ["Ekta Sood", "Fabian K\u00f6gel", "Florian Strohm", "Prajit Dhar", "Andreas Bulling"], "summary": "We present VQA-MHUG - a novel 49-participant dataset of multimodal human gaze on both images and questions during visual question answering (VQA) collected using a high-speed eye tracker. We use our dataset to analyze the similarity between human and neural attentive strategies learned by five state-of-the-art VQA models: Modular Co-Attention Network (MCAN) with either grid or region features, Pythia, Bilinear Attention Network (BAN), and the Multimodal Factorized Bilinear Pooling Network (MFB). While prior work has focused on studying the image modality, our analyses show - for the first time - that for all models, higher correlation with human attention on text is a significant predictor of VQA performance. This finding points at a potential for improving VQA performance and, at the same time, calls for further research on neural text attention mechanisms and their integration into architectures for vision and language tasks, including but potentially also beyond VQA.", "comment": " Comments: CoNLL 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.11848", "id": "2109.11848", "pdf": "https://arxiv.org/pdf/2109.11848", "other": "https://arxiv.org/format/2109.11848"}, "title": "How to find a good image-text embedding for remote sensing visual question answering?", "author_info": ["Christel Chappuis", "Sylvain Lobry", "Benjamin Kellenberger", "Bertrand Le Saux", "Devis Tuia"], "summary": "Visual question answering (VQA) has recently been introduced to remote sensing to make information extraction from overhead imagery more accessible to everyone. VQA considers a question (in natural language, therefore easy to formulate) about an image and aims at providing an answer through a model based on computer vision and natural language processing methods. As such, a VQA model needs to jointly consider visual and textual features, which is frequently done through a fusion step. In this work, we study three different fusion methodologies in the context of VQA for remote sensing and analyse the gains in accuracy with respect to the model complexity. Our findings indicate that more complex fusion mechanisms yield an improved performance, yet that seeking a trade-of between model complexity and performance is worthwhile in practice.", "comment": " Comments: 10 pages, 4 figures, presented in the MACLEAN workshop during ECML PKDD 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.08029", "id": "2109.08029", "pdf": "https://arxiv.org/pdf/2109.08029", "other": "https://arxiv.org/format/2109.08029"}, "title": "Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering", "author_info": ["Ander Salaberria", "Gorka Azkune", "Oier Lopez de Lacalle", "Aitor Soroa", "Eneko Agirre"], "summary": "Integrating outside knowledge for reasoning in visio-linguistic tasks such as visual question answering (VQA) is an open problem. Given that pretrained language models have been shown to include world knowledge, we propose to use a unimodal (text-only) train and inference procedure based on automatic off-the-shelf captioning of images and pretrained language models. Our results on a visual question answering task which requires external knowledge (OK-VQA) show that our text-only model outperforms pretrained multimodal (image-text) models of comparable number of parameters. In contrast, our model is less effective in a standard VQA task (VQA 2.0) confirming that our text-only method is specially effective for tasks requiring external knowledge. In addition, we show that our unimodal model is complementary to multimodal models in both OK-VQA and VQA 2.0, and yield the best result to date in OK-VQA among systems not using external knowledge graphs, and comparable to systems that do use them. Our qualitative analysis on OK-VQA reveals that automatic captions often fail to capture relevant information in the images, which seems to be balanced by the better inference ability of the text-only language models. Our work opens up possibilities to further improve inference in visio-linguistic tasks.", "comment": " Comments: Under review. 11 pages with 3 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.06122", "id": "2109.06122", "pdf": "https://arxiv.org/pdf/2109.06122", "other": "https://arxiv.org/format/2109.06122"}, "title": "Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering", "author_info": ["Jihyung Kil", "Cheng Zhang", "Dong Xuan", "Wei-Lun Chao"], "summary": "Visual question answering (VQA) is challenging not only because the model has to handle multi-modal information, but also because it is just so hard to collect sufficient training examples -- there are too many questions one can ask about an image. As a result, a VQA model trained solely on human-annotated examples could easily over-fit specific question styles or image contents that are being asked, leaving the model largely ignorant about the sheer diversity of questions. Existing methods address this issue primarily by introducing an auxiliary task such as visual grounding, cycle consistency, or debiasing. In this paper, we take a drastically different approach. We found that many of the \"unknowns\" to the learned VQA model are indeed \"known\" in the dataset implicitly. For instance, questions asking about the same object in different images are likely paraphrases; the number of detected or annotated objects in an image already provides the answer to the \"how many\" question, even if the question has not been annotated for that image. Building upon these insights, we present a simple data augmentation pipeline SimpleAug to turn this \"known\" knowledge into training examples for VQA. We show that these augmented examples can notably improve the learned VQA models' performance, not only on the VQA-CP dataset with language prior shifts but also on the VQA v2 dataset without such shifts. Our method further opens up the door to leverage weakly-labeled or unlabeled images in a principled way to enhance VQA models. Our code and data are publicly available at https://github.com/heendung/simpleAUG.", "comment": " Comments: Accepted to EMNLP 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.06082", "id": "2109.06082", "pdf": "https://arxiv.org/pdf/2109.06082", "other": "https://arxiv.org/format/2109.06082"}, "title": "xGQA: Cross-Lingual Visual Question Answering", "author_info": ["Jonas Pfeiffer", "Gregor Geigle", "Aishwarya Kamath", "Jan-Martin O. Steitz", "Stefan Roth", "Ivan Vuli\u0107", "Iryna Gurevych"], "summary": "Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts. In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task. We extend the established English GQA dataset to 7 typologically diverse languages, enabling us to detect and explore crucial challenges in cross-lingual visual question answering. We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and -- vice versa -- multilingual models to become multimodal. Our proposed methods outperform current state-of-the-art multilingual multimodal models (e.g., M3P) in zero-shot cross-lingual settings, but the accuracy remains low across the board; a performance drop of around 38 accuracy points in target languages showcases the difficulty of zero-shot cross-lingual transfer for this task. Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, calling for more sophisticated methods for vision and multilingual language modeling. The xGQA dataset is available online at: https://github.com/Adapter-Hub/xGQA.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2109.04653", "id": "2109.04653", "pdf": "https://arxiv.org/pdf/2109.04653", "other": "https://arxiv.org/format/2109.04653"}, "title": "Towards Developing a Multilingual and Code-Mixed Visual Question Answering System by Knowledge Distillation", "author_info": ["Humair Raj Khan", "Deepak Gupta", "Asif Ekbal"], "summary": "Pre-trained language-vision models have shown remarkable performance on the visual question answering (VQA) task. However, most pre-trained models are trained by only considering monolingual learning, especially the resource-rich language like English. Training such models for multilingual setups demand high computing resources and multilingual language-vision dataset which hinders their application in practice. To alleviate these challenges, we propose a knowledge distillation approach to extend an English language-vision model (teacher) into an equally effective multilingual and code-mixed model (student). Unlike the existing knowledge distillation methods, which only use the output from the last layer of the teacher network for distillation, our student model learns and imitates the teacher from multiple intermediate layers (language and vision encoders) with appropriately designed distillation objectives for incremental knowledge extraction. We also create the large-scale multilingual and code-mixed VQA dataset in eleven different language setups considering the multiple Indian and European languages. Experimental results and in-depth analysis show the effectiveness of the proposed VQA model over the pre-trained language-vision models on eleven diverse language setups.", "comment": " Comments: Accepted in EMNLP-Findings (2021) "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.04014", "id": "2109.04014", "pdf": "https://arxiv.org/pdf/2109.04014", "other": "https://arxiv.org/format/2109.04014"}, "title": "Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering", "author_info": ["Man Luo", "Yankai Zeng", "Pratyay Banerjee", "Chitta Baral"], "summary": "Knowledge-based visual question answering (VQA) requires answering questions with external knowledge in addition to the content of images. One dataset that is mostly used in evaluating knowledge-based VQA is OK-VQA, but it lacks a gold standard knowledge corpus for retrieval. Existing work leverage different knowledge bases (e.g., ConceptNet and Wikipedia) to obtain external knowledge. Because of varying knowledge bases, it is hard to fairly compare models' performance. To address this issue, we collect a natural language knowledge base that can be used for any VQA system. Moreover, we propose a Visual Retriever-Reader pipeline to approach knowledge-based VQA. The visual retriever aims to retrieve relevant knowledge, and the visual reader seeks to predict answers based on given knowledge. We introduce various ways to retrieve knowledge using text and images and two reader styles: classification and extraction. Both the retriever and reader are trained with weak supervision. Our experimental results show that a good retriever can significantly improve the reader's performance on the OK-VQA challenge. The code and corpus are provided in https://github.com/luomancs/retriever\\_reader\\_for\\_okvqa.git", "comment": " Comments: accepted at EMNLP 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.02370", "id": "2109.02370", "pdf": "https://arxiv.org/pdf/2109.02370", "other": "https://arxiv.org/format/2109.02370"}, "title": "Improved RAMEN: Towards Domain Generalization for Visual Question Answering", "author_info": ["Bhanuka Manesha Samarasekara Vitharana Gamage", "Lim Chern Hong"], "summary": "Currently nearing human-level performance, Visual Question Answering (VQA) is an emerging area in artificial intelligence.", "comment": " Comments: 11 pages, 3 figures, 2 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.01934", "id": "2109.01934", "pdf": "https://arxiv.org/pdf/2109.01934", "other": "https://arxiv.org/format/2109.01934"}, "title": "Weakly Supervised Relative Spatial Reasoning for Visual Question Answering", "author_info": ["Pratyay Banerjee", "Tejas Gokhale", "Yezhou Yang", "Chitta Baral"], "summary": "Vision-and-language (V\\&L) reasoning necessitates perception of visual concepts such as objects and actions, understanding semantics and language grounding, and reasoning about the interplay between the two modalities. One crucial aspect of visual reasoning is spatial understanding, which involves understanding relative locations of objects, i.e.\\ implicitly learning the geometry of the scene. In this work, we evaluate the faithfulness of V\\&L models to such geometric understanding, by formulating the prediction of pair-wise relative locations of objects as a classification as well as a regression task. Our findings suggest that state-of-the-art transformer-based V\\&L models lack sufficient abilities to excel at this task. Motivated by this, we design two objectives as proxies for 3D spatial reasoning (SR) -- object centroid estimation, and relative position estimation, and train V\\&L with weak supervision from off-the-shelf depth estimators. This leads to considerable improvements in accuracy for the \"GQA\" visual question answering challenge (in fully supervised, few-shot, and O.O.D settings) as well as improvements in relative spatial reasoning. Code and data will be released \\href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.", "comment": " Comments: Accepted to ICCV 2021. PaperId : ICCV2021-10857 Copyright transferred to IEEE ICCV. DOI will be updated later "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.12585", "id": "2108.12585", "pdf": "https://arxiv.org/pdf/2108.12585", "other": "https://arxiv.org/format/2108.12585"}, "title": "On the Significance of Question Encoder Sequence Model in the Out-of-Distribution Performance in Visual Question Answering", "author_info": ["Gouthaman KV", "Anurag Mittal"], "summary": "Generalizing beyond the experiences has a significant role in developing practical AI systems. It has been shown that current Visual Question Answering (VQA) models are over-dependent on the language-priors (spurious correlations between question-types and their most frequent answers) from the train set and pose poor performance on Out-of-Distribution (OOD) test sets. This conduct limits their generalizability and restricts them from being utilized in real-world situations. This paper shows that the sequence model architecture used in the question-encoder has a significant role in the generalizability of VQA models. To demonstrate this, we performed a detailed analysis of various existing RNN-based and Transformer-based question-encoders, and along, we proposed a novel Graph attention network (GAT)-based question-encoder. Our study found that a better choice of sequence model in the question-encoder improves the generalizability of VQA models even without using any additional relatively complex bias-mitigation approaches.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2108.10568", "id": "2108.10568", "pdf": "https://arxiv.org/pdf/2108.10568", "other": "https://arxiv.org/format/2108.10568"}, "title": "Auto-Parsing Network for Image Captioning and Visual Question Answering", "author_info": ["Xu Yang", "Chongyang Gao", "Hanwang Zhang", "Jianfei Cai"], "summary": "We propose an Auto-Parsing Network (APN) to discover and exploit the input data's hidden tree structures for improving the effectiveness of the Transformer-based vision-language systems. Specifically, we impose a Probabilistic Graphical Model (PGM) parameterized by the attention operations on each self-attention layer to incorporate sparse assumption. We use this PGM to softly segment an input sequence into a few clusters where each cluster can be treated as the parent of the inside entities. By stacking these PGM constrained self-attention layers, the clusters in a lower layer compose into a new sequence, and the PGM in a higher layer will further segment this sequence. Iteratively, a sparse tree can be implicitly parsed, and this tree's hierarchical knowledge is incorporated into the transformed embeddings, which can be used for solving the target vision-language tasks. Specifically, we showcase that our APN can strengthen Transformer based networks in two major vision-language tasks: Captioning and Visual Question Answering. Also, a PGM probability-based parsing algorithm is developed by which we can discover what the hidden structure of input is during the inference.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2107.12651", "id": "2107.12651", "pdf": "https://arxiv.org/pdf/2107.12651", "other": "https://arxiv.org/format/2107.12651"}, "title": "Greedy Gradient Ensemble for Robust Visual Question Answering", "author_info": ["Xinzhe Han", "Shuhui Wang", "Chi Su", "Qingming Huang", "Qi Tian"], "summary": "Language bias is a critical issue in Visual Question Answering (VQA), where models often exploit dataset biases for the final decision without considering the image information. As a result, they suffer from performance drop on out-of-distribution data and inadequate visual explanation. Based on experimental analysis for existing robust VQA methods, we stress the language bias in VQA that comes from two aspects, i.e., distribution bias and shortcut bias. We further propose a new de-bias framework, Greedy Gradient Ensemble (GGE), which combines multiple biased models for unbiased base model learning. With the greedy strategy, GGE forces the biased models to over-fit the biased data distribution in priority, thus makes the base model pay more attention to examples that are hard to solve by biased models. The experiments demonstrate that our method makes better use of visual information and achieves state-of-the-art performance on diagnosing dataset VQA-CP without using extra annotations.", "comment": " Comments: Accepted by ICCV 2021. Code: https://github.com/GeraldHan/GGE "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.11576", "id": "2107.11576", "pdf": "https://arxiv.org/pdf/2107.11576", "other": "https://arxiv.org/format/2107.11576"}, "title": "X-GGM: Graph Generative Modeling for Out-of-Distribution Generalization in Visual Question Answering", "author_info": ["Jingjing Jiang", "Ziyi Liu", "Yifan Liu", "Zhixiong Nan", "Nanning Zheng"], "summary": "Encouraging progress has been made towards Visual Question Answering (VQA) in recent years, but it is still challenging to enable VQA models to adaptively generalize to out-of-distribution (OOD) samples. Intuitively, recompositions of existing visual concepts (\\ie, attributes and objects) can generate unseen compositions in the training set, which will promote VQA models to generalize to OOD samples. In this paper, we formulate OOD generalization in VQA as a compositional generalization problem and propose a graph generative modeling-based training scheme (X-GGM) to implicitly model the problem. X-GGM leverages graph generative modeling to iteratively generate a relation matrix and node representations for the predefined graph that utilizes attribute-object pairs as nodes. Furthermore, to alleviate the unstable training issue in graph generative modeling, we propose a gradient distribution consistency loss to constrain the data distribution with adversarial perturbations and the generated distribution. The baseline VQA model (LXMERT) trained with the X-GGM scheme achieves state-of-the-art OOD performance on two standard VQA OOD benchmarks, \\ie, VQA-CP v2 and GQA-OOD. Extensive ablation studies demonstrate the effectiveness of X-GGM components. Code is available at \\url{https://github.com/jingjing12110/x-ggm}.", "comment": " Comments: Accepted by ACM MM2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.09106", "id": "2107.09106", "pdf": "https://arxiv.org/pdf/2107.09106", "other": "https://arxiv.org/format/2107.09106"}, "title": "Separating Skills and Concepts for Novel Visual Question Answering", "author_info": ["Spencer Whitehead", "Hui Wu", "Heng Ji", "Rogerio Feris", "Kate Saenko"], "summary": "Generalization to out-of-distribution data has been a problem for Visual Question Answering (VQA) models. To measure generalization to novel questions, we propose to separate them into \"skills\" and \"concepts\". \"Skills\" are visual tasks, such as counting or attribute recognition, and are applied to \"concepts\" mentioned in the question, such as objects and people. VQA methods should be able to compose skills and concepts in novel ways, regardless of whether the specific composition has been seen in training, yet we demonstrate that existing models have much to improve upon towards handling new compositions. We present a novel method for learning to compose skills and concepts that separates these two factors implicitly within a model by learning grounded concept representations and disentangling the encoding of skills from that of concepts. We enforce these properties with a novel contrastive learning procedure that does not rely on external annotations and can be learned from unlabeled image-question pairs. Experiments demonstrate the effectiveness of our approach for improving compositional and grounding performance.", "comment": " Comments: Paper at CVPR 2021. 14 pages, 7 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.06325", "id": "2107.06325", "pdf": "https://arxiv.org/pdf/2107.06325", "other": "https://arxiv.org/format/2107.06325"}, "title": "Graphhopper: Multi-Hop Scene Graph Reasoning for Visual Question Answering", "author_info": ["Rajat Koner", "Hang Li", "Marcel Hildebrandt", "Deepan Das", "Volker Tresp", "Stephan G\u00fcnnemann"], "summary": "Visual Question Answering (VQA) is concerned with answering free-form questions about an image. Since it requires a deep semantic and linguistic understanding of the question and the ability to associate it with various objects that are present in the image, it is an ambitious task and requires multi-modal reasoning from both computer vision and natural language processing. We propose Graphhopper, a novel method that approaches the task by integrating knowledge graph reasoning, computer vision, and natural language processing techniques. Concretely, our method is based on performing context-driven, sequential reasoning based on the scene entities and their semantic and spatial relationships. As a first step, we derive a scene graph that describes the objects in the image, as well as their attributes and their mutual relationships. Subsequently, a reinforcement learning agent is trained to autonomously navigate in a multi-hop manner over the extracted scene graph to generate reasoning paths, which are the basis for deriving answers. We conduct an experimental study on the challenging dataset GQA, based on both manually curated and automatically generated scene graphs. Our results show that we keep up with a human performance on manually curated scene graphs. Moreover, we find that Graphhopper outperforms another state-of-the-art scene graph reasoning model on both manually curated and automatically generated scene graphs by a significant margin.", "comment": " Comments: arXiv admin note: text overlap with arXiv:2007.01072 "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.05348", "id": "2107.05348", "pdf": "https://arxiv.org/pdf/2107.05348", "other": "https://arxiv.org/format/2107.05348"}, "title": "Zero-shot Visual Question Answering using Knowledge Graph", "author_info": ["Zhuo Chen", "Jiaoyan Chen", "Yuxia Geng", "Jeff Z. Pan", "Zonggang Yuan", "Huajun Chen"], "summary": "Incorporating external knowledge to Visual Question Answering (VQA) has become a vital practical need. Existing methods mostly adopt pipeline approaches with different components for knowledge matching and extraction, feature learning, etc.However, such pipeline approaches suffer when some component does not perform well, which leads to error propagation and poor overall performance. Furthermore, the majority of existing approaches ignore the answer bias issue -- many answers may have never appeared during training (i.e., unseen answers) in real-word application. To bridge these gaps, in this paper, we propose a Zero-shot VQA algorithm using knowledge graphs and a mask-based learning mechanism for better incorporating external knowledge, and present new answer-based Zero-shot VQA splits for the F-VQA dataset. Experiments show that our method can achieve state-of-the-art performance in Zero-shot VQA with unseen answers, meanwhile dramatically augment existing end-to-end models on the normal F-VQA task.", "comment": " Comments: accepted at the International Semantic Web Conference '21 (ISWC 2021) "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.04768", "id": "2107.04768", "pdf": "https://arxiv.org/pdf/2107.04768", "other": "https://arxiv.org/format/2107.04768"}, "title": "DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering", "author_info": ["Jianyu Wang", "Bing-Kun Bao", "Changsheng Xu"], "summary": "Video question answering is a challenging task, which requires agents to be able to understand rich video contents and perform spatial-temporal reasoning. However, existing graph-based methods fail to perform multi-step reasoning well, neglecting two properties of VideoQA: (1) Even for the same video, different questions may require different amount of video clips or objects to infer the answer with relational reasoning; (2) During reasoning, appearance and motion features have complicated interdependence which are correlated and complementary to each other. Based on these observations, we propose a Dual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an end-to-end fashion. The first contribution of our DualVGR is the design of an explainable Query Punishment Module, which can filter out irrelevant visual features through multiple cycles of reasoning. The second contribution is the proposed Video-based Multi-view Graph Attention Network, which captures the relations between appearance and motion features. Our DualVGR network achieves state-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and demonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is available at https://github.com/MMIR/DualVGR-VideoQA.", "comment": " Journal ref:         IEEE Transactions on Multimedia 2021       "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.03216", "id": "2107.03216", "pdf": "https://arxiv.org/pdf/2107.03216", "other": "https://arxiv.org/format/2107.03216"}, "title": "MuVAM: A Multi-View Attention-based Model for Medical Visual Question Answering", "author_info": ["Haiwei Pan", "Shuning He", "Kejia Zhang", "Bo Qu", "Chunling Chen", "Kun Shi"], "summary": "Medical Visual Question Answering (VQA) is a multi-modal challenging task widely considered by research communities of the computer vision and natural language processing. Since most current medical VQA models focus on visual content, ignoring the importance of text, this paper proposes a multi-view attention-based model(MuVAM) for medical visual question answering which integrates the high-level semantics of medical images on the basis of text description. Firstly, different methods are utilized to extract the features of the image and the question for the two modalities of vision and text. Secondly, this paper proposes a multi-view attention mechanism that include Image-to-Question (I2Q) attention and Word-to-Text (W2T) attention. Multi-view attention can correlate the question with image and word in order to better analyze the question and get an accurate answer. Thirdly, a composite loss is presented to predict the answer accurately after multi-modal feature fusion and improve the similarity between visual and textual cross-modal features. It consists of classification loss and image-question complementary (IQC) loss. Finally, for data errors and missing labels in the VQA-RAD dataset, we collaborate with medical experts to correct and complete this dataset and then construct an enhanced dataset, VQA-RADPh. The experiments on these two datasets show that the effectiveness of MuVAM surpasses the state-of-the-art method.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2107.02331", "id": "2107.02331", "pdf": "https://arxiv.org/pdf/2107.02331", "other": "https://arxiv.org/format/2107.02331"}, "title": "Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering", "author_info": ["Siddharth Karamcheti", "Ranjay Krishna", "Li Fei-Fei", "Christopher D. Manning"], "summary": "Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition. However, we uncover a striking contrast to this promise: across 5 models and 4 datasets on the task of visual question answering, a wide variety of active learning approaches fail to outperform random selection. To understand this discrepancy, we profile 8 active learning methods on a per-example basis, and identify the problem as collective outliers -- groups of examples that active learning methods prefer to acquire but models fail to learn (e.g., questions that ask about text in images or require external knowledge). Through systematic ablation experiments and qualitative visualizations, we verify that collective outliers are a general phenomenon responsible for degrading pool-based active learning. Notably, we show that active learning sample efficiency increases significantly as the number of collective outliers in the active learning pool decreases. We conclude with a discussion and prescriptive recommendations for mitigating the effects of these outliers in future work.", "comment": " Comments: Accepted at ACL-IJCNLP 2021. 17 pages, 16 Figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2106.14476", "id": "2106.14476", "pdf": "https://arxiv.org/pdf/2106.14476", "other": "https://arxiv.org/format/2106.14476"}, "title": "Adventurer's Treasure Hunt: A Transparent System for Visually Grounded Compositional Visual Question Answering based on Scene Graphs", "author_info": ["Daniel Reich", "Felix Putze", "Tanja Schultz"], "summary": "With the expressed goal of improving system transparency and visual grounding in the reasoning process in VQA, we present a modular system for the task of compositional VQA based on scene graphs. Our system is called \"Adventurer's Treasure Hunt\" (or ATH), named after an analogy we draw between our model's search procedure for an answer and an adventurer's search for treasure. We developed ATH with three characteristic features in mind: 1. By design, ATH allows us to explicitly quantify the impact of each of the sub-components on overall VQA performance, as well as their performance on their individual sub-task. 2. By modeling the search task after a treasure hunt, ATH inherently produces an explicit, visually grounded inference path for the processed question. 3. ATH is the first GQA-trained VQA system that dynamically extracts answers by querying the visual knowledge base directly, instead of selecting one from a specially learned classifier's output distribution over a pre-fixed answer vocabulary. We report detailed results on all components and their contributions to overall VQA performance on the GQA dataset and show that ATH achieves the highest visual grounding score among all examined systems.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2106.13445", "id": "2106.13445", "pdf": "https://arxiv.org/pdf/2106.13445", "other": "https://arxiv.org/format/2106.13445"}, "title": "A Picture May Be Worth a Hundred Words for Visual Question Answering", "author_info": ["Yusuke Hirota", "Noa Garcia", "Mayu Otani", "Chenhui Chu", "Yuta Nakashima", "Ittetsu Taniguchi", "Takao Onoye"], "summary": "How far can we go with textual representations for understanding pictures? In image understanding, it is essential to use concise but detailed image representations. Deep visual features extracted by vision models, such as Faster R-CNN, are prevailing used in multiple tasks, and especially in visual question answering (VQA). However, conventional deep visual features may struggle to convey all the details in an image as we humans do. Meanwhile, with recent language models' progress, descriptive text may be an alternative to this problem. This paper delves into the effectiveness of textual representations for image understanding in the specific context of VQA. We propose to take description-question pairs as input, instead of deep visual features, and fed them into a language-only Transformer model, simplifying the process and the computational cost. We also experiment with data augmentation techniques to increase the diversity in the training set and avoid learning statistical bias. Extensive evaluations have shown that textual representations require only about a hundred words to compete with deep visual features on both VQA 2.0 and VQA-CP v2.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2106.10548", "id": "2106.10548", "pdf": "https://arxiv.org/pdf/2106.10548", "other": "https://arxiv.org/format/2106.10548"}, "title": "VQA-Aid: Visual Question Answering for Post-Disaster Damage Assessment and Analysis", "author_info": ["Argho Sarkar", "Maryam Rahnemoonfar"], "summary": "Visual Question Answering system integrated with Unmanned Aerial Vehicle (UAV) has a lot of potentials to advance the post-disaster damage assessment purpose. Providing assistance to affected areas is highly dependent on real-time data assessment and analysis. Scope of the Visual Question Answering is to understand the scene and provide query related answer which certainly faster the recovery process after any disaster. In this work, we address the importance of \\textit{visual question answering (VQA)} task for post-disaster damage assessment by presenting our recently developed VQA dataset called \\textit{HurMic-VQA} collected during hurricane Michael, and comparing the performances of baseline VQA models.", "comment": " Comments: 4 pages, 2 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2106.04605", "id": "2106.04605", "pdf": "https://arxiv.org/pdf/2106.04605", "other": "https://arxiv.org/format/2106.04605"}, "title": "Check It Again: Progressive Visual Question Answering via Visual Entailment", "author_info": ["Qingyi Si", "Zheng Lin", "Mingyu Zheng", "Peng Fu", "Weiping Wang"], "summary": "While sophisticated Visual Question Answering models have achieved remarkable success, they tend to answer questions only according to superficial correlations between question and answer. Several recent approaches have been developed to address this language priors problem. However, most of them predict the correct answer according to one best output without checking the authenticity of answers. Besides, they only explore the interaction between image and question, ignoring the semantics of candidate answers. In this paper, we propose a select-and-rerank (SAR) progressive framework based on Visual Entailment. Specifically, we first select the candidate answers relevant to the question or the image, then we rerank the candidate answers by a visual entailment task, which verifies whether the image semantically entails the synthetic statement of the question and each candidate answer. Experimental results show the effectiveness of our proposed framework, which establishes a new state-of-the-art accuracy on VQA-CP v2 with a 7.55% improvement.", "comment": " Comments: ACL-2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2106.02280", "id": "2106.02280", "pdf": "https://arxiv.org/pdf/2106.02280", "other": "https://arxiv.org/format/2106.02280"}, "title": "Human-Adversarial Visual Question Answering", "author_info": ["Sasha Sheng", "Amanpreet Singh", "Vedanuj Goswami", "Jose Alberto Lopez Magana", "Wojciech Galuba", "Devi Parikh", "Douwe Kiela"], "summary": "Performance on the most commonly used Visual Question Answering dataset (VQA v2) is starting to approach human accuracy. However, in interacting with state-of-the-art VQA models, it is clear that the problem is far from being solved. In order to stress test VQA models, we benchmark them against human-adversarial examples. Human subjects interact with a state-of-the-art VQA model, and for each image in the dataset, attempt to find a question where the model's predicted answer is incorrect. We find that a wide range of state-of-the-art models perform poorly when evaluated on these examples. We conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions. We hope that this Adversarial VQA (AdVQA) benchmark can help drive progress in the field and advance the state of the art.", "comment": " Comments: 22 pages, 13 figures. First two authors contributed equally "}, {"arxiv": {"page": "https://arxiv.org/abs/2105.14300", "id": "2105.14300", "pdf": "https://arxiv.org/pdf/2105.14300", "other": "https://arxiv.org/format/2105.14300"}, "title": "LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering", "author_info": ["Zujie Liang", "Haifeng Hu", "Jiaying Zhu"], "summary": "Most existing Visual Question Answering (VQA) systems tend to overly rely on language bias and hence fail to reason from the visual clue. To address this issue, we propose a novel Language-Prior Feedback (LPF) objective function, to re-balance the proportion of each answer's loss value in the total VQA loss. The LPF firstly calculates a modulating factor to determine the language bias using a question-only branch. Then, the LPF assigns a self-adaptive weight to each training sample in the training process. With this reweighting mechanism, the LPF ensures that the total VQA loss can be reshaped to a more balanced form. By this means, the samples that require certain visual information to predict will be efficiently used during training. Our method is simple to implement, model-agnostic, and end-to-end trainable. We conduct extensive experiments and the results show that the LPF (1) brings a significant improvement over various VQA models, (2) achieves competitive performance on the bias-sensitive VQA-CP v2 benchmark.", "comment": " Comments: Accepted by ACM SIGIR 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2105.08913", "id": "2105.08913", "pdf": "https://arxiv.org/pdf/2105.08913", "other": "https://arxiv.org/format/2105.08913"}, "title": "Multiple Meta-model Quantifying for Medical Visual Question Answering", "author_info": ["Tuong Do", "Binh X. Nguyen", "Erman Tjiputra", "Minh Tran", "Quang D. Tran", "Anh Nguyen"], "summary": "Transfer learning is an important step to extract meaningful features and overcome the data limitation in the medical Visual Question Answering (VQA) task. However, most of the existing medical VQA methods rely on external data for transfer learning, while the meta-data within the dataset is not fully utilized. In this paper, we present a new multiple meta-model quantifying method that effectively learns meta-annotation and leverages meaningful features to the medical VQA task. Our proposed method is designed to increase meta-data by auto-annotation, deal with noisy labels, and output meta-models which provide robust features for medical VQA tasks. Extensively experimental results on two public medical VQA datasets show that our approach achieves superior accuracy in comparison with other state-of-the-art methods, while does not require external data to train meta-models.", "comment": " Comments: Provisional accepted in MICCAI 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2105.04836", "id": "2105.04836", "pdf": "https://arxiv.org/pdf/2105.04836", "other": "https://arxiv.org/format/2105.04836"}, "title": "Found a Reason for me? Weakly-supervised Grounded Visual Question Answering using Capsules", "author_info": ["Aisha Urooj Khan", "Hilde Kuehne", "Kevin Duarte", "Chuang Gan", "Niels Lobo", "Mubarak Shah"], "summary": "The problem of grounding VQA tasks has seen an increased attention in the research community recently, with most attempts usually focusing on solving this task by using pretrained object detectors. However, pre-trained object detectors require bounding box annotations for detecting relevant objects in the vocabulary, which may not always be feasible for real-life large-scale applications. In this paper, we focus on a more relaxed setting: the grounding of relevant visual entities in a weakly supervised manner by training on the VQA task alone. To address this problem, we propose a visual capsule module with a query-based selection mechanism of capsule features, that allows the model to focus on relevant regions based on the textual cues about visual information in the question. We show that integrating the proposed capsule module in existing VQA systems significantly improves their performance on the weakly supervised grounding task. Overall, we demonstrate the effectiveness of our approach on two state-of-the-art VQA systems, stacked NMN and MAC, on the CLEVR-Answers benchmark, our new evaluation set based on CLEVR scenes with ground truth bounding boxes for objects that are relevant for the correct answer, as well as on GQA, a real world VQA dataset with compositional questions. We show that the systems with the proposed capsule module consistently outperform the respective baseline systems in terms of answer grounding, while achieving comparable performance on VQA task.", "comment": " Comments: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2105.04780", "id": "2105.04780", "pdf": "https://arxiv.org/pdf/2105.04780", "other": "https://arxiv.org/format/2105.04780"}, "title": "Cross-Modal Generative Augmentation for Visual Question Answering", "author_info": ["Zixu Wang", "Yishu Miao", "Lucia Specia"], "summary": "Data augmentation has been shown to effectively improve the performance of multimodal machine learning models. This paper introduces a generative model for data augmentation by leveraging the correlations among multiple modalities. Different from conventional data augmentation approaches that apply low-level operations with deterministic heuristics, our method learns a generator that generates samples of the target modality conditioned on observed modalities in the variational auto-encoder framework. Additionally, the proposed model is able to quantify the confidence of augmented data by its generative probability, and can be jointly optimised with a downstream task. Experiments on Visual Question Answering as downstream task demonstrate the effectiveness of the proposed generative model, which is able to improve strong UpDn-based models to achieve state-of-the-art performance.", "comment": " Comments: BMVC 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2105.03938", "id": "2105.03938", "pdf": "https://arxiv.org/pdf/2105.03938", "other": "https://arxiv.org/format/2105.03938"}, "title": "Passage Retrieval for Outside-Knowledge Visual Question Answering", "author_info": ["Chen Qu", "Hamed Zamani", "Liu Yang", "W. Bruce Croft", "Erik Learned-Miller"], "summary": "In this work, we address multi-modal information needs that contain text questions and images by focusing on passage retrieval for outside-knowledge visual question answering. This task requires access to outside knowledge, which in our case we define to be a large unstructured passage collection. We first conduct sparse retrieval with BM25 and study expanding the question with object names and image captions. We verify that visual clues play an important role and captions tend to be more informative than object names in sparse retrieval. We then construct a dual-encoder dense retriever, with the query encoder being LXMERT, a multi-modal pre-trained transformer. We further show that dense retrieval significantly outperforms sparse retrieval that uses object expansion. Moreover, dense retrieval matches the performance of sparse retrieval that leverages human-generated captions.", "comment": " Comments: Accepted to SIGIR'21 as a short paper "}, {"arxiv": {"page": "https://arxiv.org/abs/2105.00136", "id": "2105.00136", "pdf": "https://arxiv.org/pdf/2105.00136", "other": "https://arxiv.org/format/2105.00136"}, "title": "Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering", "author_info": ["Haifan Gong", "Guanqi Chen", "Sishuo Liu", "Yizhou Yu", "Guanbin Li"], "summary": "Due to the severe lack of labeled data, existing methods of medical visual question answering usually rely on transfer learning to obtain effective image feature representation and use cross-modal fusion of visual and linguistic features to achieve question-related answer prediction. These two phases are performed independently and without considering the compatibility and applicability of the pre-trained features for cross-modal fusion. Thus, we reformulate image feature pre-training as a multi-task learning paradigm and witness its extraordinary superiority, forcing it to take into account the applicability of features for the specific image comprehension task. Furthermore, we introduce a cross-modal self-attention~(CMSA) module to selectively capture the long-range contextual relevance for more effective fusion of visual and linguistic features. Experimental results demonstrate that the proposed method outperforms existing state-of-the-art methods. Our code and models are available at https://github.com/haifangong/CMSA-MTPT-4-MedicalVQA.", "comment": " Comments: ICMR '21: ACM International Conference on Multimedia Retrieval, Taipei, Taiwan, August 21-24, 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2104.14741", "id": "2104.14741", "pdf": "https://arxiv.org/pdf/2104.14741", "other": "https://arxiv.org/format/2104.14741"}, "title": "Chop Chop BERT: Visual Question Answering by Chopping VisualBERT's Heads", "author_info": ["Chenyu Gao", "Qi Zhu", "Peng Wang", "Qi Wu"], "summary": "Vision-and-Language (VL) pre-training has shown great potential on many related downstream tasks, such as Visual Question Answering (VQA), one of the most popular problems in the VL field. All of these pre-trained models (such as VisualBERT, ViLBERT, LXMERT and UNITER) are built with Transformer, which extends the classical attention mechanism to multiple layers and heads. To investigate why and how these models work on VQA so well, in this paper we explore the roles of individual heads and layers in Transformer models when handling 12 different types of questions. Specifically, we manually remove (chop) heads (or layers) from a pre-trained VisualBERT model at a time, and test it on different levels of questions to record its performance. As shown in the interesting echelon shape of the result matrices, experiments reveal different heads and layers are responsible for different question types, with higher-level layers activated by higher-level visual reasoning questions. Based on this observation, we design a dynamic chopping module that can automatically remove heads and layers of the VisualBERT at an instance level when dealing with different questions. Our dynamic chopping module can effectively reduce the parameters of the original model by 50%, while only damaging the accuracy by less than 1% on the VQA task.", "comment": " MSC Class:           68T45                              ACM Class:           I.4.8                "}, {"arxiv": {"page": "https://arxiv.org/abs/2104.14336", "id": "2104.14336", "pdf": "https://arxiv.org/pdf/2104.14336", "other": "https://arxiv.org/format/2104.14336"}, "title": "Document Collection Visual Question Answering", "author_info": ["Rub\u00e8n Tito", "Dimosthenis Karatzas", "Ernest Valveny"], "summary": "Current tasks and methods in Document Understanding aims to process documents as single elements. However, documents are usually organized in collections (historical records, purchase invoices), that provide context useful for their interpretation. To address this problem, we introduce Document Collection Visual Question Answering (DocCVQA) a new dataset and related task, where questions are posed over a whole collection of document images and the goal is not only to provide the answer to the given question, but also to retrieve the set of documents that contain the information needed to infer the answer. Along with the dataset we propose a new evaluation metric and baselines which provide further insights to the new dataset and task.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2104.10283", "id": "2104.10283", "pdf": "https://arxiv.org/pdf/2104.10283", "other": "https://arxiv.org/format/2104.10283"}, "title": "GraghVQA: Language-Guided Graph Neural Networks for Graph-based Visual Question Answering", "author_info": ["Weixin Liang", "Yanhao Jiang", "Zixuan Liu"], "summary": "Images are more than a collection of objects or attributes -- they represent a web of relationships among interconnected objects. Scene Graph has emerged as a new modality for a structured graphical representation of images. Scene Graph encodes objects as nodes connected via pairwise relations as edges. To support question answering on scene graphs, we propose GraphVQA, a language-guided graph neural network framework that translates and executes a natural language question as multiple iterations of message passing among graph nodes. We explore the design space of GraphVQA framework, and discuss the trade-off of different design choices. Our experiments on GQA dataset show that GraphVQA outperforms the state-of-the-art model by a large margin (88.43% vs. 94.78%).", "comment": " Comments: NAACL 2021 MAI-Workshop. Code available at https://github.com/codexxxl/GraphVQA "}, {"arxiv": {"page": "https://arxiv.org/abs/2104.05981", "id": "2104.05981", "pdf": "https://arxiv.org/pdf/2104.05981", "other": "https://arxiv.org/format/2104.05981"}, "title": "CLEVR_HYP: A Challenge Dataset and Baselines for Visual Question Answering with Hypothetical Actions over Images", "author_info": ["Shailaja Keyur Sampat", "Akshay Kumar", "Yezhou Yang", "Chitta Baral"], "summary": "Most existing research on visual question answering (VQA) is limited to information explicitly present in an image or a video. In this paper, we take visual understanding to a higher level where systems are challenged to answer questions that involve mentally simulating the hypothetical consequences of performing specific actions in a given scenario. Towards that end, we formulate a vision-language question answering task based on the CLEVR (Johnson et. al., 2017) dataset. We then modify the best existing VQA methods and propose baseline solvers for this task. Finally, we motivate the development of better vision-language models by providing insights about the capability of diverse architectures to perform joint reasoning over image-text modality. Our dataset setup scripts and codes will be made publicly available at https://github.com/shailaja183/clevr_hyp.", "comment": " Comments: 16 pages, 11 figures, Accepted as a Long Paper at NAACL-HLT 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2104.05965", "id": "2104.05965", "pdf": "https://arxiv.org/pdf/2104.05965", "other": "https://arxiv.org/format/2104.05965"}, "title": "Dealing with Missing Modalities in the Visual Question Answer-Difference Prediction Task through Knowledge Distillation", "author_info": ["Jae Won Cho", "Dong-Jin Kim", "Jinsoo Choi", "Yunjae Jung", "In So Kweon"], "summary": "In this work, we address the issues of missing modalities that have arisen from the Visual Question Answer-Difference prediction task and find a novel method to solve the task at hand. We address the missing modality-the ground truth answers-that are not present at test time and use a privileged knowledge distillation scheme to deal with the issue of the missing modality. In order to efficiently do so, we first introduce a model, the \"Big\" Teacher, that takes the image/question/answer triplet as its input and outperforms the baseline, then use a combination of models to distill knowledge to a target network (student) that only takes the image/question pair as its inputs. We experiment our models on the VizWiz and VQA-V2 Answer Difference datasets and show through extensive experimentation and ablation the performances of our method and a diverse possibility for future research.", "comment": " Comments: To appear in CVPR MULA Workshop "}, {"arxiv": {"page": "https://arxiv.org/abs/2104.03149", "id": "2104.03149", "pdf": "https://arxiv.org/pdf/2104.03149", "other": "https://arxiv.org/format/2104.03149"}, "title": "Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering", "author_info": ["Corentin Dancette", "Remi Cadene", "Damien Teney", "Matthieu Cord"], "summary": "We introduce an evaluation methodology for visual question answering (VQA) to better diagnose cases of shortcut learning. These cases happen when a model exploits spurious statistical regularities to produce correct answers but does not actually deploy the desired behavior. There is a need to identify possible shortcuts in a dataset and assess their use before deploying a model in the real world. The research community in VQA has focused exclusively on question-based shortcuts, where a model might, for example, answer \"What is the color of the sky\" with \"blue\" by relying mostly on the question-conditional training prior and give little weight to visual evidence. We go a step further and consider multimodal shortcuts that involve both questions and images. We first identify potential shortcuts in the popular VQA v2 training set by mining trivial predictive rules such as co-occurrences of words and visual elements. We then introduce VQA-CounterExamples (VQA-CE), an evaluation protocol based on our subset of CounterExamples i.e. image-question-answer triplets where our rules lead to incorrect answers. We use this new evaluation in a large-scale study of existing approaches for VQA. We demonstrate that even state-of-the-art models perform poorly and that existing techniques to reduce biases are largely ineffective in this context. Our findings suggest that past work on question-based biases in VQA has only addressed one facet of a complex issue. The code for our method is available at https://github.com/cdancette/detect-shortcuts.", "comment": " Comments: Accepted at ICCV 2021. Code is available at https://github.com/cdancette/detect-shortcuts "}, {"arxiv": {"page": "https://arxiv.org/abs/2104.00107", "id": "2104.00107", "pdf": "https://arxiv.org/pdf/2104.00107", "other": "https://arxiv.org/format/2104.00107"}, "title": "Analysis on Image Set Visual Question Answering", "author_info": ["Abhinav Khattar", "Aviral Joshi", "Har Simrat Singh", "Pulkit Goel", "Rohit Prakash Barnwal"], "summary": "We tackle the challenge of Visual Question Answering in multi-image setting for the ISVQA dataset. Traditional VQA tasks have focused on a single-image setting where the target answer is generated from a single image. Image set VQA, however, comprises of a set of images and requires finding connection between images, relate the objects across images based on these connections and generate a unified answer. In this report, we work with 4 approaches in a bid to improve the performance on the task. We analyse and compare our results with three baseline models - LXMERT, HME-VideoQA and VisualBERT - and show that our approaches can provide a slight improvement over the baselines. In specific, we try to improve on the spatial awareness of the model and help the model identify color using enhanced pre-training, reduce language dependence using adversarial regularization, and improve counting using regression loss and graph based deduplication. We further delve into an in-depth analysis on the language bias in the ISVQA dataset and show how models trained on ISVQA implicitly learn to associate language more strongly with the final answer.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2103.15022", "id": "2103.15022", "pdf": "https://arxiv.org/pdf/2103.15022", "other": "https://arxiv.org/format/2103.15022"}, "title": "'Just because you are right, doesn't mean I am wrong': Overcoming a Bottleneck in the Development and Evaluation of Open-Ended Visual Question Answering (VQA) Tasks", "author_info": ["Man Luo", "Shailaja Keyur Sampat", "Riley Tallman", "Yankai Zeng", "Manuha Vancha", "Akarshan Sajja", "Chitta Baral"], "summary": "GQA (Hudson and Manning, 2019) is a dataset for real-world visual reasoning and compositional question answering. We found that many answers predicted by the best visionlanguage models on the GQA dataset do not match the ground-truth answer but still are semantically meaningful and correct in the given context. In fact, this is the case with most existing visual question answering (VQA) datasets where they assume only one ground-truth answer for each question. We propose Alternative Answer Sets (AAS) of ground-truth answers to address this limitation, which is created automatically using off-the-shelf NLP tools. We introduce a semantic metric based on AAS and modify top VQA solvers to support multiple plausible answers for a question. We implement this approach on the GQA dataset and show the performance improvements.", "comment": " Comments: accepted to EACL 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2103.05568", "id": "2103.05568", "pdf": "https://arxiv.org/pdf/2103.05568", "other": "https://arxiv.org/format/2103.05568"}, "title": "Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering", "author_info": ["Aman Jain", "Mayank Kothyari", "Vishwajeet Kumar", "Preethi Jyothi", "Ganesh Ramakrishnan", "Soumen Chakrabarti"], "summary": "Multimodal IR, spanning text corpus, knowledge graph and images, called outside knowledge visual question answering (OKVQA), is of much recent interest. However, the popular data set has serious limitations. A surprisingly large fraction of queries do not assess the ability to integrate cross-modal information. Instead, some are independent of the image, some depend on speculation, some require OCR or are otherwise answerable from the image alone. To add to the above limitations, frequency-based guessing is very effective because of (unintended) widespread answer overlaps between the train and test folds. Overall, it is hard to determine when state-of-the-art systems exploit these weaknesses rather than really infer the answers, because they are opaque and their 'reasoning' process is uninterpretable. An equally important limitation is that the dataset is designed for the quantitative assessment only of the end-to-end answer retrieval task, with no provision for assessing the correct(semantic) interpretation of the input query. In response, we identify a key structural idiom in OKVQA ,viz., S3 (select, substitute and search), and build a new data set and challenge around it. Specifically, the questioner identifies an entity in the image and asks a question involving that entity which can be answered only by consulting a knowledge graph or corpus passage mentioning the entity. Our challenge consists of (i)OKVQAS3, a subset of OKVQA annotated based on the structural idiom and (ii)S3VQA, a new dataset built from scratch. We also present a neural but structurally transparent OKVQA system, S3, that explicitly addresses our challenge dataset, and outperforms recent competitive baselines.", "comment": " Comments: Accepted at SIGIR 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2103.02937", "id": "2103.02937", "pdf": "https://arxiv.org/pdf/2103.02937", "other": "https://arxiv.org/format/2103.02937"}, "title": "Visual Question Answering: which investigated applications?", "author_info": ["Silvio Barra", "Carmen Bisogni", "Maria De Marsico", "Stefano Ricciardi"], "summary": "Visual Question Answering (VQA) is an extremely stimulating and challenging research area where Computer Vision (CV) and Natural Language Processig (NLP) have recently met. In image captioning and video summarization, the semantic information is completely contained in still images or video dynamics, and it has only to be mined and expressed in a human-consistent way. Differently from this, in VQA semantic information in the same media must be compared with the semantics implied by a question expressed in natural language, doubling the artificial intelligence-related effort. Some recent surveys about VQA approaches have focused on methods underlying either the image-related processing or the verbal-related one, or on the way to consistently fuse the conveyed information. Possible applications are only suggested, and, in fact, most cited works rely on general-purpose datasets that are used to assess the building blocks of a VQA system. This paper rather considers the proposals that focus on real-world applications, possibly using as benchmarks suitable data bound to the application domain. The paper also reports about some recent challenges in VQA research.", "comment": " Journal ref:         Pattern Recognition Letters 2021       "}, {"arxiv": {"page": "https://arxiv.org/abs/2102.10575", "id": "2102.10575", "pdf": "https://arxiv.org/pdf/2102.10575", "other": "https://arxiv.org/format/2102.10575"}, "title": "Learning Compositional Representation for Few-shot Visual Question Answering", "author_info": ["Dalu Guo", "Dacheng Tao"], "summary": "Current methods of Visual Question Answering perform well on the answers with an amount of training data but have limited accuracy on the novel ones with few examples. However, humans can quickly adapt to these new categories with just a few glimpses, as they learn to organize the concepts that have been seen before to figure the novel class, which are hardly explored by the deep learning methods. Therefore, in this paper, we propose to extract the attributes from the answers with enough data, which are later composed to constrain the learning of the few-shot ones. We generate the few-shot dataset of VQA with a variety of answers and their attributes without any human effort. With this dataset, we build our attribute network to disentangle the attributes by learning their features from parts of the image instead of the whole one. Experimental results on the VQA v2.0 validation dataset demonstrate the effectiveness of our proposed attribute network and the constraint between answers and their corresponding attributes, as well as the ability of our method to handle the answers with few training examples.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2102.09542", "id": "2102.09542", "pdf": "https://arxiv.org/pdf/2102.09542", "other": "https://arxiv.org/format/2102.09542"}, "title": "SLAKE: A Semantically-Labeled Knowledge-Enhanced Dataset for Medical Visual Question Answering", "author_info": ["Bo Liu", "Li-Ming Zhan", "Li Xu", "Lin Ma", "Yan Yang", "Xiao-Ming Wu"], "summary": "Medical visual question answering (Med-VQA) has tremendous potential in healthcare. However, the development of this technology is hindered by the lacking of publicly-available and high-quality labeled datasets for training and evaluation. In this paper, we present a large bilingual dataset, SLAKE, with comprehensive semantic labels annotated by experienced physicians and a new structural medical knowledge base for Med-VQA. Besides, SLAKE includes richer modalities and covers more human body parts than the currently available dataset. We show that SLAKE can be used to facilitate the development and evaluation of Med-VQA systems. The dataset can be downloaded from http://www.med-vqa.com/slake.", "comment": " Comments: ISBI 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2102.01916", "id": "2102.01916", "pdf": "https://arxiv.org/pdf/2102.01916", "other": "https://arxiv.org/format/2102.01916"}, "title": "Answer Questions with Right Image Regions: A Visual Attention Regularization Approach", "author_info": ["Yibing Liu", "Yangyang Guo", "Jianhua Yin", "Xuemeng Song", "Weifeng Liu", "Liqiang Nie"], "summary": "Visual attention in Visual Question Answering (VQA) targets at locating the right image regions regarding the answer prediction, offering a powerful technique to promote multi-modal understanding. However, recent studies have pointed out that the highlighted image regions from the visual attention are often irrelevant to the given question and answer, leading to model confusion for correct visual reasoning. To tackle this problem, existing methods mostly resort to aligning the visual attention weights with human attentions. Nevertheless, gathering such human data is laborious and expensive, making it burdensome to adapt well-developed models across datasets. To address this issue, in this paper, we devise a novel visual attention regularization approach, namely AttReg, for better visual grounding in VQA. Specifically, AttReg firstly identifies the image regions which are essential for question answering yet unexpectedly ignored (i.e., assigned with low attention weights) by the backbone model. And then a mask-guided learning scheme is leveraged to regularize the visual attention to focus more on these ignored key regions. The proposed method is very flexible and model-agnostic, which can be integrated into most visual attention-based VQA models and require no human attention supervision. Extensive experiments over three benchmark datasets, i.e., VQA-CP v2, VQA-CP v1, and VQA v2, have been conducted to evaluate the effectiveness of AttReg. As a by-product, when incorporating AttReg into the strong baseline LMH, our approach can achieve a new state-of-the-art accuracy of 60.00% with an absolute performance gain of 7.01% on the VQA-CP v2 benchmark dataset...", "comment": " Comments: ACM TOMM 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2101.08978", "id": "2101.08978", "pdf": "https://arxiv.org/pdf/2101.08978", "other": "https://arxiv.org/format/2101.08978"}, "title": "Visual Question Answering based on Local-Scene-Aware Referring Expression Generation", "author_info": ["Jung-Jun Kim", "Dong-Gyu Lee", "Jialin Wu", "Hong-Gyu Jung", "Seong-Whan Lee"], "summary": "Visual question answering requires a deep understanding of both images and natural language. However, most methods mainly focus on visual concept; such as the relationships between various objects. The limited use of object categories combined with their relationships or simple question embedding is insufficient for representing complex scenes and explaining decisions. To address this limitation, we propose the use of text expressions generated for images, because such expressions have few structural constraints and can provide richer descriptions of images. The generated expressions can be incorporated with visual features and question embedding to obtain the question-relevant answer. A joint-embedding multi-head attention network is also proposed to model three different information modalities with co-attention. We quantitatively and qualitatively evaluated the proposed method on the VQA v2 dataset and compared it with state-of-the-art methods in terms of answer prediction. The quality of the generated expressions was also evaluated on the RefCOCO, RefCOCO+, and RefCOCOg datasets. Experimental results demonstrate the effectiveness of the proposed method and reveal that it outperformed all of the competing methods in terms of both quantitative and qualitative results.", "comment": " Comments: 32 pages, 8 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2101.06399", "id": "2101.06399", "pdf": "https://arxiv.org/pdf/2101.06399", "other": "https://arxiv.org/format/2101.06399"}, "title": "Latent Variable Models for Visual Question Answering", "author_info": ["Zixu Wang", "Yishu Miao", "Lucia Specia"], "summary": "Current work on Visual Question Answering (VQA) explore deterministic approaches conditioned on various types of image and question features. We posit that, in addition to image and question pairs, other modalities are useful for teaching machine to carry out question answering. Hence in this paper, we propose latent variable models for VQA where extra information (e.g. captions and answer categories) are incorporated as latent variables, which are observed during training but in turn benefit question-answering performance at test time. Experiments on the VQA v2.0 benchmarking dataset demonstrate the effectiveness of our proposed models: they improve over strong baselines, especially those that do not rely on extensive language-vision pre-training.", "comment": " Comments: ICCV21 CLVL: 4th Workshop on Closing the Loop Between Vision and Language "}, {"arxiv": {"page": "https://arxiv.org/abs/2101.05479", "id": "2101.05479", "pdf": "https://arxiv.org/pdf/2101.05479", "other": "https://arxiv.org/format/2101.05479"}, "title": "Understanding the Role of Scene Graphs in Visual Question Answering", "author_info": ["Vinay Damodaran", "Sharanya Chakravarthy", "Akshay Kumar", "Anjana Umapathy", "Teruko Mitamura", "Yuta Nakashima", "Noa Garcia", "Chenhui Chu"], "summary": "Visual Question Answering (VQA) is of tremendous interest to the research community with important applications such as aiding visually impaired users and image-based search. In this work, we explore the use of scene graphs for solving the VQA task. We conduct experiments on the GQA dataset which presents a challenging set of questions requiring counting, compositionality and advanced reasoning capability, and provides scene graphs for a large number of images. We adopt image + question architectures for use with scene graphs, evaluate various scene graph generation techniques for unseen images, propose a training curriculum to leverage human-annotated and auto-generated scene graphs, and build late fusion architectures to learn from multiple image representations. We present a multi-faceted study into the use of scene graphs for VQA, making this work the first of its kind.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2012.15484", "id": "2012.15484", "pdf": "https://arxiv.org/pdf/2012.15484", "other": "https://arxiv.org/format/2012.15484"}, "title": "Seeing is Knowing! Fact-based Visual Question Answering using Knowledge Graph Embeddings", "author_info": ["Kiran Ramnath", "Mark Hasegawa-Johnson"], "summary": "Fact-based Visual Question Answering (FVQA), a challenging variant of VQA, requires a QA-system to include facts from a diverse knowledge graph (KG) in its reasoning process to produce an answer. Large KGs, especially common-sense KGs, are known to be incomplete, i.e., not all non-existent facts are always incorrect. Therefore, being able to reason over incomplete KGs for QA is a critical requirement in real-world applications that has not been addressed extensively in the literature. We develop a novel QA architecture that allows us to reason over incomplete KGs, something current FVQA state-of-the-art (SOTA) approaches lack due to their critical reliance on fact retrieval. We use KG Embeddings, a technique widely used for KG completion, for the downstream task of FVQA. We also employ a new image representation technique we call 'Image-as-Knowledge' to enable this capability, alongside a simple one-step CoAttention mechanism to attend to text and image during QA. Our FVQA architecture is faster during inference time, being O(m), as opposed to existing FVQA SOTA methods which are O(N log N), where m = number of vertices, N = number of edges = O(m^2). KG embeddings are shown to hold complementary information to word embeddings: a combination of both metrics permits performance comparable to SOTA methods in the standard answer retrieval task, and significantly better (26% absolute) in the proposed missing-edge reasoning task.", "comment": " Comments: 17 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2012.11528", "id": "2012.11528", "pdf": "https://arxiv.org/pdf/2012.11528", "other": "https://arxiv.org/format/2012.11528"}, "title": "Overcoming Language Priors with Self-supervised Learning for Visual Question Answering", "author_info": ["Xi Zhu", "Zhendong Mao", "Chunxiao Liu", "Peng Zhang", "Bin Wang", "Yongdong Zhang"], "summary": "Most Visual Question Answering (VQA) models suffer from the language prior problem, which is caused by inherent data biases. Specifically, VQA models tend to answer questions (e.g., what color is the banana?) based on the high-frequency answers (e.g., yellow) ignoring image contents. Existing approaches tackle this problem by creating delicate models or introducing additional visual annotations to reduce question dependency while strengthening image dependency. However, they are still subject to the language prior problem since the data biases have not been even alleviated. In this paper, we introduce a self-supervised learning framework to solve this problem. Concretely, we first automatically generate labeled data to balance the biased data, and propose a self-supervised auxiliary task to utilize the balanced data to assist the base VQA model to overcome language priors. Our method can compensate for the data biases by generating balanced data without introducing external annotations. Experimental results show that our method can significantly outperform the state-of-the-art, improving the overall accuracy from 49.50% to 57.59% on the most commonly used benchmark VQA-CP v2. In other words, we can increase the performance of annotation-based methods by 16% without using external annotations.", "comment": " Comments: Accepted by IJCAI 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2012.11134", "id": "2012.11134", "pdf": "https://arxiv.org/pdf/2012.11134", "other": "https://arxiv.org/format/2012.11134"}, "title": "Learning content and context with language bias for Visual Question Answering", "author_info": ["Chao Yang", "Su Feng", "Dongsheng Li", "Huawei Shen", "Guoqing Wang", "Bin Jiang"], "summary": "Visual Question Answering (VQA) is a challenging multimodal task to answer questions about an image. Many works concentrate on how to reduce language bias which makes models answer questions ignoring visual content and language context. However, reducing language bias also weakens the ability of VQA models to learn context prior. To address this issue, we propose a novel learning strategy named CCB, which forces VQA models to answer questions relying on Content and Context with language Bias. Specifically, CCB establishes Content and Context branches on top of a base VQA model and forces them to focus on local key content and global effective context respectively. Moreover, a joint loss function is proposed to reduce the importance of biased samples and retain their beneficial influence on answering questions. Experiments show that CCB outperforms the state-of-the-art methods in terms of accuracy on VQA-CP v2.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2012.02356", "id": "2012.02356", "pdf": "https://arxiv.org/pdf/2012.02356", "other": "https://arxiv.org/format/2012.02356"}, "title": "WeaQA: Weak Supervision via Captions for Visual Question Answering", "author_info": ["Pratyay Banerjee", "Tejas Gokhale", "Yezhou Yang", "Chitta Baral"], "summary": "Methodologies for training visual question answering (VQA) models assume the availability of datasets with human-annotated \\textit{Image-Question-Answer} (I-Q-A) triplets. This has led to heavy reliance on datasets and a lack of generalization to new types of questions and scenes. Linguistic priors along with biases and errors due to annotator subjectivity have been shown to percolate into VQA models trained on such samples. We study whether models can be trained without any human-annotated Q-A pairs, but only with images and their associated textual descriptions or captions. We present a method to train models with synthetic Q-A pairs generated procedurally from captions. Additionally, we demonstrate the efficacy of spatial-pyramid image patches as a simple but effective alternative to dense and costly object bounding box annotations used in existing VQA models. Our experiments on three VQA benchmarks demonstrate the efficacy of this weakly-supervised approach, especially on the VQA-CP challenge, which tests performance under changing linguistic priors.", "comment": " Comments: Accepted in Findings of ACL 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2011.13681", "id": "2011.13681", "pdf": "https://arxiv.org/pdf/2011.13681", "other": "https://arxiv.org/format/2011.13681"}, "title": "Point and Ask: Incorporating Pointing into Visual Question Answering", "author_info": ["Arjun Mani", "Nobline Yoo", "Will Hinthorn", "Olga Russakovsky"], "summary": "Visual Question Answering (VQA) has become one of the key benchmarks of visual recognition progress. Multiple VQA extensions have been explored to better simulate real-world settings: different question formulations, changing training and test distributions, conversational consistency in dialogues, and explanation-based answering. In this work, we further expand this space by considering visual questions that include a spatial point of reference. Pointing is a nearly universal gesture among humans, and real-world VQA is likely to involve a gesture towards the target region.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2011.13406", "id": "2011.13406", "pdf": "https://arxiv.org/pdf/2011.13406", "other": "https://arxiv.org/format/2011.13406"}, "title": "Learning from Lexical Perturbations for Consistent Visual Question Answering", "author_info": ["Spencer Whitehead", "Hui Wu", "Yi Ren Fung", "Heng Ji", "Rogerio Feris", "Kate Saenko"], "summary": "Existing Visual Question Answering (VQA) models are often fragile and sensitive to input variations. In this paper, we propose a novel approach to address this issue based on modular networks, which creates two questions related by linguistic perturbations and regularizes the visual reasoning process between them to be consistent during training. We show that our framework markedly improves consistency and generalization ability, demonstrating the value of controlled linguistic perturbations as a useful and currently underutilized training and regularization tool for VQA models. We also present VQA Perturbed Pairings (VQA P2), a new, low-cost benchmark and augmentation pipeline to create controllable linguistic variations of VQA questions. Our benchmark uniquely draws from large-scale linguistic resources, avoiding human annotation effort while maintaining data quality compared to generative approaches. We benchmark existing VQA models using VQA P2 and provide robustness analysis on each type of linguistic variation.", "comment": " Comments: 14 pages, 8 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2011.12340", "id": "2011.12340", "pdf": "https://arxiv.org/pdf/2011.12340", "other": "https://arxiv.org/format/2011.12340"}, "title": "Zero-Shot Visual Slot Filling as Question Answering", "author_info": ["Larry Heck", "Simon Heck"], "summary": "This paper presents a new approach to visual zero-shot slot filling. The approach extends previous approaches by reformulating the slot filling task as Question Answering. Slot tags are converted to rich natural language questions that capture the semantics of visual information and lexical text on the GUI screen. These questions are paired with the user's utterance and slots are extracted from the utterance using a state-of-the-art ALBERT-based Question Answering system trained on the Stanford Question Answering dataset (SQuaD2). An approach to further refine the model with multi-task training is presented. The multi-task approach facilitates the incorporation of a large number of successive refinements and transfer learning across similar tasks. A new Visual Slot dataset and a visual extension of the popular ATIS dataset is introduced to support research and experimentation on visual slot filling. Results show F1 scores between 0.52 and 0.60 on the Visual Slot and ATIS datasets with no training data (zero-shot).", "comment": " Comments: 5 pages, 6 figures, 4 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2011.10731", "id": "2011.10731", "pdf": "https://arxiv.org/pdf/2011.10731", "other": "https://arxiv.org/format/2011.10731"}, "title": "LRTA: A Transparent Neural-Symbolic Reasoning Framework with Modular Supervision for Visual Question Answering", "author_info": ["Weixin Liang", "Feiyang Niu", "Aishwarya Reganti", "Govind Thattai", "Gokhan Tur"], "summary": "The predominant approach to visual question answering (VQA) relies on encoding the image and question with a \"black-box\" neural encoder and decoding a single token as the answer like \"yes\" or \"no\". Despite this approach's strong quantitative results, it struggles to come up with intuitive, human-readable forms of justification for the prediction process. To address this insufficiency, we reformulate VQA as a full answer generation task, which requires the model to justify its predictions in natural language. We propose LRTA [Look, Read, Think, Answer], a transparent neural-symbolic reasoning framework for visual question answering that solves the problem step-by-step like humans and provides human-readable form of justification at each step. Specifically, LRTA learns to first convert an image into a scene graph and parse a question into multiple reasoning instructions. It then executes the reasoning instructions one at a time by traversing the scene graph using a recurrent neural-symbolic execution module. Finally, it generates a full answer to the given question with natural language justifications. Our experiments on GQA dataset show that LRTA outperforms the state-of-the-art model by a large margin (43.1% v.s. 28.0%) on the full answer generation task. We also create a perturbed GQA test set by removing linguistic cues (attributes and relations) in the questions for analyzing whether a model is having a smart guess with superficial data correlations. We show that LRTA makes a step towards truly understanding the question while the state-of-the-art model tends to learn superficial correlations from the training data.", "comment": " Comments: NeurIPS KR2ML 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2011.10094", "id": "2011.10094", "pdf": "https://arxiv.org/pdf/2011.10094", "other": "https://arxiv.org/format/2011.10094"}, "title": "Logically Consistent Loss for Visual Question Answering", "author_info": ["Anh-Cat Le-Ngo", "Truyen Tran", "Santu Rana", "Sunil Gupta", "Svetha Venkatesh"], "summary": "Given an image, a back-ground knowledge, and a set of questions about an object, human learners answer the questions very consistently regardless of question forms and semantic tasks. The current advancement in neural-network based Visual Question Answering (VQA), despite their impressive performance, cannot ensure such consistency due to identically distribution (i.i.d.) assumption. We propose a new model-agnostic logic constraint to tackle this issue by formulating a logically consistent loss in the multi-task learning framework as well as a data organisation called family-batch and hybrid-batch. To demonstrate usefulness of this proposal, we train and evaluate MAC-net based VQA machines with and without the proposed logically consistent loss and the proposed data organization. The experiments confirm that the proposed loss formulae and introduction of hybrid-batch leads to more consistency as well as better performance. Though the proposed approach is tested with MAC-net, it can be utilised in any other QA methods whenever the logical consistency between answers exist.", "comment": " Comments: 10 pages, 6 figure, 9 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2011.02164", "id": "2011.02164", "pdf": "https://arxiv.org/pdf/2011.02164", "other": "https://arxiv.org/format/2011.02164"}, "title": "An Improved Attention for Visual Question Answering", "author_info": ["Tanzila Rahman", "Shih-Han Chou", "Leonid Sigal", "Giuseppe Carenini"], "summary": "We consider the problem of Visual Question Answering (VQA). Given an image and a free-form, open-ended, question, expressed in natural language, the goal of VQA system is to provide accurate answer to this question with respect to the image. The task is challenging because it requires simultaneous and intricate understanding of both visual and textual information. Attention, which captures intra- and inter-modal dependencies, has emerged as perhaps the most widely used mechanism for addressing these challenges. In this paper, we propose an improved attention-based architecture to solve VQA. We incorporate an Attention on Attention (AoA) module within encoder-decoder framework, which is able to determine the relation between attention results and queries. Attention module generates weighted average for each query. On the other hand, AoA module first generates an information vector and an attention gate using attention results and current context; and then adds another attention to generate final attended information by multiplying the two. We also propose multimodal fusion module to combine both visual and textual information. The goal of this fusion module is to dynamically decide how much information should be considered from each modality. Extensive experiments on VQA-v2 benchmark dataset show that our method achieves the state-of-the-art performance.", "comment": " Comments: 8 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.14953", "id": "2010.14953", "pdf": "https://arxiv.org/pdf/2010.14953", "other": "https://arxiv.org/format/2010.14953"}, "title": "Leveraging Visual Question Answering to Improve Text-to-Image Synthesis", "author_info": ["Stanislav Frolov", "Shailza Jolly", "J\u00f6rn Hees", "Andreas Dengel"], "summary": "Generating images from textual descriptions has recently attracted a lot of interest. While current models can generate photo-realistic images of individual objects such as birds and human faces, synthesising images with multiple objects is still very difficult. In this paper, we propose an effective way to combine Text-to-Image (T2I) synthesis with Visual Question Answering (VQA) to improve the image quality and image-text alignment of generated images by leveraging the VQA 2.0 dataset. We create additional training samples by concatenating question and answer (QA) pairs and employ a standard VQA model to provide the T2I model with an auxiliary learning signal. We encourage images generated from QA pairs to look realistic and additionally minimize an external VQA loss. Our method lowers the FID from 27.84 to 25.38 and increases the R-prec. from 83.82% to 84.79% when compared to the baseline, which indicates that T2I synthesis can successfully be improved using a standard VQA model.", "comment": " Comments: Accepted to the LANTERN workshop at COLING 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.14095", "id": "2010.14095", "pdf": "https://arxiv.org/pdf/2010.14095", "other": "https://arxiv.org/format/2010.14095"}, "title": "MMFT-BERT: Multimodal Fusion Transformer with BERT Encodings for Visual Question Answering", "author_info": ["Aisha Urooj Khan", "Amir Mazaheri", "Niels da Vitoria Lobo", "Mubarak Shah"], "summary": "We present MMFT-BERT(MultiModal Fusion Transformer with BERT encodings), to solve Visual Question Answering (VQA) ensuring individual and combined processing of multiple input modalities. Our approach benefits from processing multimodal data (video and text) adopting the BERT encodings individually and using a novel transformer-based fusion method to fuse them together. Our method decomposes the different sources of modalities, into different BERT instances with similar architectures, but variable weights. This achieves SOTA results on the TVQA dataset. Additionally, we provide TVQA-Visual, an isolated diagnostic subset of TVQA, which strictly requires the knowledge of visual (V) modality based on a human annotator's judgment. This set of questions helps us to study the model's behavior and the challenges TVQA poses to prevent the achievement of super human performance. Extensive experiments show the effectiveness and superiority of our method.", "comment": " Comments: Accepted at Findings of EMNLP 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.12917", "id": "2010.12917", "pdf": "https://arxiv.org/pdf/2010.12917", "other": "https://arxiv.org/format/2010.12917"}, "title": "RUArt: A Novel Text-Centered Solution for Text-Based Visual Question Answering", "author_info": ["Zan-Xia Jin", "Heran Wu", "Chun Yang", "Fang Zhou", "Jingyan Qin", "Lei Xiao", "Xu-Cheng Yin"], "summary": "Text-based visual question answering (VQA) requires to read and understand text in an image to correctly answer a given question. However, most current methods simply add optical character recognition (OCR) tokens extracted from the image into the VQA model without considering contextual information of OCR tokens and mining the relationships between OCR tokens and scene objects. In this paper, we propose a novel text-centered method called RUArt (Reading, Understanding and Answering the Related Text) for text-based VQA. Taking an image and a question as input, RUArt first reads the image and obtains text and scene objects. Then, it understands the question, OCRed text and objects in the context of the scene, and further mines the relationships among them. Finally, it answers the related text for the given question through text semantic matching and reasoning. We evaluate our RUArt on two text-based VQA benchmarks (ST-VQA and TextVQA) and conduct extensive ablation studies for exploring the reasons behind RUArt's effectiveness. Experimental results demonstrate that our method can effectively explore the contextual information of the text and mine the stable relationships between the text and objects.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2010.12852", "id": "2010.12852", "pdf": "https://arxiv.org/pdf/2010.12852", "other": "https://arxiv.org/format/2010.12852"}, "title": "Beyond VQA: Generating Multi-word Answer and Rationale to Visual Questions", "author_info": ["Radhika Dua", "Sai Srinivas Kancheti", "Vineeth N Balasubramanian"], "summary": "Visual Question Answering is a multi-modal task that aims to measure high-level visual understanding. Contemporary VQA models are restrictive in the sense that answers are obtained via classification over a limited vocabulary (in the case of open-ended VQA), or via classification over a set of multiple-choice-type answers. In this work, we present a completely generative formulation where a multi-word answer is generated for a visual query. To take this a step forward, we introduce a new task: ViQAR (Visual Question Answering and Reasoning), wherein a model must generate the complete answer and a rationale that seeks to justify the generated answer. We propose an end-to-end architecture to solve this task and describe how to evaluate it. We show that our model generates strong answers and rationales through qualitative and quantitative evaluation, as well as through a human Turing Test.", "comment": " Comments: MULA Workshop, CVPR 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.12435", "id": "2010.12435", "pdf": "https://arxiv.org/pdf/2010.12435", "other": "https://arxiv.org/format/2010.12435"}, "title": "Pathological Visual Question Answering", "author_info": ["Xuehai He", "Zhuo Cai", "Wenlan Wei", "Yichen Zhang", "Luntian Mou", "Eric Xing", "Pengtao Xie"], "summary": "Is it possible to develop an \"AI Pathologist\" to pass the board-certified examination of the American Board of Pathology (ABP)? To build such a system, three challenges need to be addressed. First, we need to create a visual question answering (VQA) dataset where the AI agent is presented with a pathology image together with a question and is asked to give the correct answer. Due to privacy concerns, pathology images are usually not publicly available. Besides, only well-trained pathologists can understand pathology images, but they barely have time to help create datasets for AI research. The second challenge is: since it is difficult to hire highly experienced pathologists to create pathology visual questions and answers, the resulting pathology VQA dataset may contain errors. Training pathology VQA models using these noisy or even erroneous data will lead to problematic models that cannot generalize well on unseen images. The third challenge is: the medical concepts and knowledge covered in pathology question-answer (QA) pairs are very diverse while the number of QA pairs available for modeling training is limited. How to learn effective representations of diverse medical concepts based on limited data is technically demanding. In this paper, we aim to address these three challenges. To our best knowledge, our work represents the first one addressing the pathology VQA problem. To deal with the issue that a publicly available pathology VQA dataset is lacking, we create PathVQA dataset. To address the second challenge, we propose a learning-by-ignoring approach. To address the third challenge, we propose to use cross-modal self-supervised learning. We perform experiments on our created PathVQA dataset and the results demonstrate the effectiveness of our proposed learning-by-ignoring method and cross-modal self-supervised learning methods.", "comment": " Comments: arXiv admin note: text overlap with arXiv:2003.10286 "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.11997", "id": "2010.11997", "pdf": "https://arxiv.org/pdf/2010.11997", "other": "https://arxiv.org/format/2010.11997"}, "title": "Characterizing Datasets for Social Visual Question Answering, and the New TinySocial Dataset", "author_info": ["Zhanwen Chen", "Shiyao Li", "Roxanne Rashedi", "Xiaoman Zi", "Morgan Elrod-Erickson", "Bryan Hollis", "Angela Maliakal", "Xinyu Shen", "Simeng Zhao", "Maithilee Kunda"], "summary": "Modern social intelligence includes the ability to watch videos and answer questions about social and theory-of-mind-related content, e.g., for a scene in Harry Potter, \"Is the father really upset about the boys flying the car?\" Social visual question answering (social VQA) is emerging as a valuable methodology for studying social reasoning in both humans (e.g., children with autism) and AI agents. However, this problem space spans enormous variations in both videos and questions. We discuss methods for creating and characterizing social VQA datasets, including 1) crowdsourcing versus in-house authoring, including sample comparisons of two new datasets that we created (TinySocial-Crowd and TinySocial-InHouse) and the previously existing Social-IQ dataset; 2) a new rubric for characterizing the difficulty and content of a given video; and 3) a new rubric for characterizing question types. We close by describing how having well-characterized social VQA datasets will enhance the explainability of AI agents and can also inform assessments and educational interventions for people.", "comment": " Comments: To appear in the Joint IEEE International Conference on Development and Learning and on Epigenetic Robotics (ICDL), 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.08708", "id": "2010.08708", "pdf": "https://arxiv.org/pdf/2010.08708", "other": "https://arxiv.org/format/2010.08708"}, "title": "Answer-checking in Context: A Multi-modal FullyAttention Network for Visual Question Answering", "author_info": ["Hantao Huang", "Tao Han", "Wei Han", "Deep Yap", "Cheng-Ming Chiang"], "summary": "Visual Question Answering (VQA) is challenging due to the complex cross-modal relations. It has received extensive attention from the research community. From the human perspective, to answer a visual question, one needs to read the question and then refer to the image to generate an answer. This answer will then be checked against the question and image again for the final confirmation. In this paper, we mimic this process and propose a fully attention based VQA architecture. Moreover, an answer-checking module is proposed to perform a unified attention on the jointly answer, question and image representation to update the answer. This mimics the human answer checking process to consider the answer in the context. With answer-checking modules and transferred BERT layers, our model achieves the state-of-the-art accuracy 71.57\\% using fewer parameters on VQA-v2.0 test-standard split.", "comment": " Comments: Accepted in ICPR2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.04913", "id": "2010.04913", "pdf": "https://arxiv.org/pdf/2010.04913", "other": "https://arxiv.org/format/2010.04913"}, "title": "Interpretable Neural Computation for Real-World Compositional Visual Question Answering", "author_info": ["Ruixue Tang", "Chao Ma"], "summary": "There are two main lines of research on visual question answering (VQA): compositional model with explicit multi-hop reasoning, and monolithic network with implicit reasoning in the latent feature space. The former excels in interpretability and compositionality but fails on real-world images, while the latter usually achieves better performance due to model flexibility and parameter efficiency. We aim to combine the two to build an interpretable framework for real-world compositional VQA. In our framework, images and questions are disentangled into scene graphs and programs, and a symbolic program executor runs on them with full transparency to select the attention regions, which are then iteratively passed to a visual-linguistic pre-trained encoder to predict answers. Experiments conducted on the GQA benchmark demonstrate that our framework outperforms the compositional prior arts and achieves competitive accuracy among monolithic ones. With respect to the validity, plausibility and distribution metrics, our framework surpasses others by a considerable margin.", "comment": " Comments: PRCV 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.03160", "id": "2010.03160", "pdf": "https://arxiv.org/pdf/2010.03160", "other": "https://arxiv.org/format/2010.03160"}, "title": "Vision Skills Needed to Answer Visual Questions", "author_info": ["Xiaoyu Zeng", "Yanan Wang", "Tai-Yin Chiu", "Nilavra Bhattacharya", "Danna Gurari"], "summary": "The task of answering questions about images has garnered attention as a practical service for assisting populations with visual impairments as well as a visual Turing test for the artificial intelligence community. Our first aim is to identify the common vision skills needed for both scenarios. To do so, we analyze the need for four vision skills---object recognition, text recognition, color recognition, and counting---on over 27,000 visual questions from two datasets representing both scenarios. We next quantify the difficulty of these skills for both humans and computers on both datasets. Finally, we propose a novel task of predicting what vision skills are needed to answer a question about an image. Our results reveal (mis)matches between aims of real users of such services and the focus of the AI community. We conclude with a discussion about future directions for addressing the visual question answering task.", "comment": " Comments: To be published on Proceedings of the ACM on Human-Computer Interaction, Vol. 4, No. CSCW2, Article 149. Publication date: October 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.02582", "id": "2010.02582", "pdf": "https://arxiv.org/pdf/2010.02582", "other": "https://arxiv.org/format/2010.02582"}, "title": "Finding the Evidence: Localization-aware Answer Prediction for Text Visual Question Answering", "author_info": ["Wei Han", "Hantao Huang", "Tao Han"], "summary": "Image text carries essential information to understand the scene and perform reasoning. Text-based visual question answering (text VQA) task focuses on visual questions that require reading text in images. Existing text VQA systems generate an answer by selecting from optical character recognition (OCR) texts or a fixed vocabulary. Positional information of text is underused and there is a lack of evidence for the generated answer. As such, this paper proposes a localization-aware answer prediction network (LaAP-Net) to address this challenge. Our LaAP-Net not only generates the answer to the question but also predicts a bounding box as evidence of the generated answer. Moreover, a context-enriched OCR representation (COR) for multimodal fusion is proposed to facilitate the localization task. Our proposed LaAP-Net outperforms existing approaches on three benchmark datasets for the text VQA task by a noticeable margin.", "comment": " Comments: Accepted in COLING2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.01725", "id": "2010.01725", "pdf": "https://arxiv.org/pdf/2010.01725", "other": "https://arxiv.org/format/2010.01725"}, "title": "Attention Guided Semantic Relationship Parsing for Visual Question Answering", "author_info": ["Moshiur Farazi", "Salman Khan", "Nick Barnes"], "summary": "Humans explain inter-object relationships with semantic labels that demonstrate a high-level understanding required to perform complex Vision-Language tasks such as Visual Question Answering (VQA). However, existing VQA models represent relationships as a combination of object-level visual features which constrain a model to express interactions between objects in a single domain, while the model is trying to solve a multi-modal task. In this paper, we propose a general purpose semantic relationship parser which generates a semantic feature vector for each subject-predicate-object triplet in an image, and a Mutual and Self Attention (MSA) mechanism that learns to identify relationship triplets that are important to answer the given question. To motivate the significance of semantic relationships, we show an oracle setting with ground-truth relationship triplets, where our model achieves a ~25% accuracy gain over the closest state-of-the-art model on the challenging GQA dataset. Further, with our semantic parser, we show that our model outperforms other comparable approaches on VQA and GQA datasets.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2009.12770", "id": "2009.12770", "pdf": "https://arxiv.org/pdf/2009.12770", "other": "https://arxiv.org/format/2009.12770"}, "title": "Hierarchical Deep Multi-modal Network for Medical Visual Question Answering", "author_info": ["Deepak Gupta", "Swati Suman", "Asif Ekbal"], "summary": "Visual Question Answering in Medical domain (VQA-Med) plays an important role in providing medical assistance to the end-users. These users are expected to raise either a straightforward question with a Yes/No answer or a challenging question that requires a detailed and descriptive answer. The existing techniques in VQA-Med fail to distinguish between the different question types sometimes complicates the simpler problems, or over-simplifies the complicated ones. It is certainly true that for different question types, several distinct systems can lead to confusion and discomfort for the end-users. To address this issue, we propose a hierarchical deep multi-modal network that analyzes and classifies end-user questions/queries and then incorporates a query-specific approach for answer prediction. We refer our proposed approach as Hierarchical Question Segregation based Visual Question Answering, in short HQS-VQA. Our contributions are three-fold, viz. firstly, we propose a question segregation (QS) technique for VQAMed; secondly, we integrate the QS model to the hierarchical deep multi-modal neural network to generate proper answers to the queries related to medical images; and thirdly, we study the impact of QS in Medical-VQA by comparing the performance of the proposed model with QS and a model without QS. We evaluate the performance of our proposed model on two benchmark datasets, viz. RAD and CLEF18. Experimental results show that our proposed HQS-VQA technique outperforms the baseline models with significant margins. We also conduct a detailed quantitative and qualitative analysis of the obtained results and discover potential causes of errors and their solutions.", "comment": " Comments: Accepted for publication at Expert Systems with Applications "}, {"arxiv": {"page": "https://arxiv.org/abs/2009.11118", "id": "2009.11118", "pdf": "https://arxiv.org/pdf/2009.11118", "other": "https://arxiv.org/format/2009.11118"}, "title": "Multiple interaction learning with question-type prior knowledge for constraining answer search space in visual question answering", "author_info": ["Tuong Do", "Binh X. Nguyen", "Huy Tran", "Erman Tjiputra", "Quang D. Tran", "Thanh-Toan Do"], "summary": "Different approaches have been proposed to Visual Question Answering (VQA). However, few works are aware of the behaviors of varying joint modality methods over question type prior knowledge extracted from data in constraining answer search space, of which information gives a reliable cue to reason about answers for questions asked in input images. In this paper, we propose a novel VQA model that utilizes the question-type prior information to improve VQA by leveraging the multiple interactions between different joint modality methods based on their behaviors in answering questions from different types. The solid experiments on two benchmark datasets, i.e., VQA 2.0 and TDIUC, indicate that the proposed method yields the best performance with the most competitive approaches.", "comment": " Comments: Accepted in ECCV Workshop 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2009.10054", "id": "2009.10054", "pdf": "https://arxiv.org/pdf/2009.10054", "other": "https://arxiv.org/format/2009.10054"}, "title": "Regularizing Attention Networks for Anomaly Detection in Visual Question Answering", "author_info": ["Doyup Lee", "Yeongjae Cheon", "Wook-Shin Han"], "summary": "For stability and reliability of real-world applications, the robustness of DNNs in unimodal tasks has been evaluated. However, few studies consider abnormal situations that a visual question answering (VQA) model might encounter at test time after deployment in the real-world. In this study, we evaluate the robustness of state-of-the-art VQA models to five different anomalies, including worst-case scenarios, the most frequent scenarios, and the current limitation of VQA models. Different from the results in unimodal tasks, the maximum confidence of answers in VQA models cannot detect anomalous inputs, and post-training of the outputs, such as outlier exposure, is ineffective for VQA models. Thus, we propose an attention-based method, which uses confidence of reasoning between input images and questions and shows much more promising results than the previous methods in unimodal tasks. In addition, we show that a maximum entropy regularization of attention networks can significantly improve the attention-based anomaly detection of the VQA models. Thanks to the simplicity, attention-based anomaly detection and the regularization are model-agnostic methods, which can be used for various cross-modal attentions in the state-of-the-art VQA models. The results imply that cross-modal attention in VQA is important to improve not only VQA accuracy, but also the robustness to various anomalies.", "comment": " Comments: 16 pages, 7 figures, Accepted to AAAI-21 "}, {"arxiv": {"page": "https://arxiv.org/abs/2009.08566", "id": "2009.08566", "pdf": "https://arxiv.org/pdf/2009.08566", "other": "https://arxiv.org/format/2009.08566"}, "title": "MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering", "author_info": ["Tejas Gokhale", "Pratyay Banerjee", "Chitta Baral", "Yezhou Yang"], "summary": "While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization. In this paper, we present MUTANT, a training paradigm that exposes the model to perceptually similar, yet semantically distinct mutations of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, MUTANT does not rely on the knowledge about the nature of train and test answer distributions. MUTANT establishes a new state-of-the-art accuracy on VQA-CP with a 10.57% improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.", "comment": " Comments: Accepted to EMNLP 2020, Long Papers "}, {"arxiv": {"page": "https://arxiv.org/abs/2009.00145", "id": "2009.00145", "pdf": "https://arxiv.org/pdf/2009.00145", "other": "https://arxiv.org/format/2009.00145"}, "title": "Cross-modal Knowledge Reasoning for Knowledge-based Visual Question Answering", "author_info": ["Jing Yu", "Zihao Zhu", "Yujing Wang", "Weifeng Zhang", "Yue Hu", "Jianlong Tan"], "summary": "Knowledge-based Visual Question Answering (KVQA) requires external knowledge beyond the visible content to answer questions about an image. This ability is challenging but indispensable to achieve general VQA. One limitation of existing KVQA solutions is that they jointly embed all kinds of information without fine-grained selection, which introduces unexpected noises for reasoning the correct answer. How to capture the question-oriented and information-complementary evidence remains a key challenge to solve the problem. Inspired by the human cognition theory, in this paper, we depict an image by multiple knowledge graphs from the visual, semantic and factual views. Thereinto, the visual graph and semantic graph are regarded as image-conditioned instantiation of the factual graph. On top of these new representations, we re-formulate Knowledge-based Visual Question Answering as a recurrent reasoning process for obtaining complementary evidence from multimodal information. To this end, we decompose the model into a series of memory-based reasoning steps, each performed by a G raph-based R ead, U pdate, and C ontrol ( GRUC ) module that conducts parallel reasoning over both visual and semantic information. By stacking the modules multiple times, our model performs transitive reasoning and obtains question-oriented concept representations under the constrain of different modalities. Finally, we perform graph neural networks to infer the global-optimal answer by jointly considering all the concepts. We achieve a new state-of-the-art performance on three popular benchmark datasets, including FVQA, Visual7W-KB and OK-VQA, and demonstrate the effectiveness and interpretability of our model with extensive experiments.", "comment": " Comments: Accepted at Pattern Recognition. arXiv admin note: substantial text overlap with arXiv:2006.09073 "}, {"arxiv": {"page": "https://arxiv.org/abs/2008.12520", "id": "2008.12520", "pdf": "https://arxiv.org/pdf/2008.12520", "other": "https://arxiv.org/format/2008.12520"}, "title": "A Dataset and Baselines for Visual Question Answering on Art", "author_info": ["Noa Garcia", "Chentao Ye", "Zihua Liu", "Qingtao Hu", "Mayu Otani", "Chenhui Chu", "Yuta Nakashima", "Teruko Mitamura"], "summary": "Answering questions related to art pieces (paintings) is a difficult task, as it implies the understanding of not only the visual information that is shown in the picture, but also the contextual knowledge that is acquired through the study of the history of art. In this work, we introduce our first attempt towards building a new dataset, coined AQUA (Art QUestion Answering). The question-answer (QA) pairs are automatically generated using state-of-the-art question generation methods based on paintings and comments provided in an existing art understanding dataset. The QA pairs are cleansed by crowdsourcing workers with respect to their grammatical correctness, answerability, and answers' correctness. Our dataset inherently consists of visual (painting-based) and knowledge (comment-based) questions. We also present a two-branch model as baseline, where the visual and knowledge questions are handled independently. We extensively compare our baseline model against the state-of-the-art models for question answering, and we provide a comprehensive study about the challenges and potential future directions for visual question answering on art.", "comment": ""}]}