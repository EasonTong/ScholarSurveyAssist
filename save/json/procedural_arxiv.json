{"data": [{"arxiv": {"page": "https://arxiv.org/abs/2202.10783", "id": "2202.10783", "pdf": "https://arxiv.org/pdf/2202.10783", "other": "https://arxiv.org/format/2202.10783"}, "title": "A passive admittance controller to enforce Remote Center of Motion and Tool Spatial constraints with application in hands-on surgical procedures", "author_info": ["Theodora Kastritsi", "Zoe Doulgeri"], "summary": "The restriction of feasible motions of a manipulator link constrained to move through an entry port is a common problem in minimum invasive surgery procedures. Additional spatial restrictions are required to ensure the safety of sensitive regions from unintentional damage. In this work, we design a target admittance model that is proved to enforce robot tool manipulation by a human through a remote center of motion and to guarantee that the tool will never enter or touch forbidden regions. The control scheme is proved passive under the exertion of a human force ensuring manipulation stability. Its performance is demonstrated by experiments with a setup mimicking a hands-on surgical procedure comprising a KUKA LWR4+ and a virtual intraoperative environment.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10639", "id": "2202.10639", "pdf": "https://arxiv.org/pdf/2202.10639", "other": "https://arxiv.org/format/2202.10639"}, "title": "A Heuristic Proof Procedure for Propositional Logic", "author_info": ["Keehang Kwon"], "summary": "Theorem proving is one of the oldest applications which require heuristics to prune the search space. Invertible proof procedures has been the major tool. In this paper, we present a novel and powerful heuristic called nongshim which can be seen as an underlying principle of invertible proof procedures. Using this heuristic, we derive an invertible sequent calculus\\cite{Ketonen,Troe} from sequent calculus for propositional logic.", "comment": " Comments: 5 pages. arXiv admin note: text overlap with arXiv:1712.05665 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03885", "id": "2202.03885", "pdf": "https://arxiv.org/pdf/2202.03885", "other": "https://arxiv.org/format/2202.03885"}, "title": "Computer assisted discharging procedure on planar graphs: application to 2-distance coloring", "author_info": ["Hoang La", "Petru Valicov"], "summary": "Using computational techniques we provide a framework for proving results on subclasses of planar graphs via discharging method. The aim of this paper is to apply these techniques to study the 2-distance coloring of planar subcubic graphs. Applying these techniques we show that every subcubic planar graph G of girth at least 8 has 2-distance chromatic number at most 6.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.02557", "id": "2202.02557", "pdf": "https://arxiv.org/pdf/2202.02557", "other": "https://arxiv.org/format/2202.02557"}, "title": "Lower-bounds on the Bayesian Risk in Estimation Procedures via f-Divergences", "author_info": ["Adrien Vandenbroucque", "Amedeo Roberto Esposito", "Michael Gastpar"], "summary": "We consider the problem of parameter estimation in a Bayesian setting and propose a general lower-bound that includes part of the family of f-Divergences. The results are then applied to specific settings of interest and compared to other notable results in the literature. In particular, we show that the known bounds using Mutual Information can be improved by using, for example, Maximal Leakage, Hellinger divergence, or generalizations of the Hockey-Stick divergence.", "comment": " Comments: Submitted to ISIT 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.13374", "id": "2201.13374", "pdf": "https://arxiv.org/pdf/2201.13374", "other": "https://arxiv.org/format/2201.13374"}, "title": "Deuteron Production in Ultra-Relativistic Heavy-Ion Collisions: A Comparison of the Coalescence and the Minimum Spanning Tree Procedure", "author_info": ["Viktar Kireyeu", "Jan Steinheimer", "J\u00f6rg Aichelin", "Marcus Bleicher", "Elena Bratkovskaya"], "summary": "The formation of deuterons in heavy-ion collisions at relativistic energies is investigated by employing two recently advanced models -- the Minimum Spanning Tree (MST) method and the coalescence model by embedding them in the PHQMD and the UrQMD transport approaches. While the coalescence mechanism combines nucleons into deuterons at the kinetic freeze-out hypersurface, the MST identifies the clusters during the different stages of time evolution. We find that both clustering procedures give very similar results for the deuteron observables in the UrQMD as well as in the PHQMD environment. Moreover, the results agree well with the experimental data on deuteron production in Pb+Pb collisions at sNN\u2212\u2212\u2212\u2212\u221a=8.8 GeV (selected for the comparison of the methods and models in this study). A detailed investigation shows that the coordinate space distribution of the produced deuterons differs from that of the free nucleons and other hadrons. Thus, deuterons are not destroyed by additional rescattering.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.13227", "id": "2201.13227", "pdf": "https://arxiv.org/pdf/2201.13227", "other": "https://arxiv.org/format/2201.13227"}, "title": "A Proof Procedure For Separation Logic With Inductive Definitions and Theory Reasoning", "author_info": ["Mnacho Echenim", "Nicolas Peltier"], "summary": "A proof procedure, in the spirit of the sequent calculus, is proposed to check the validity of entailments between Separation Logic formulas combining inductively defined predicates denoted structures of bounded tree width and theory reasoning. The calculus is sound and complete, in the sense that a sequent is valid iff it admits a (possibly infinite) proof tree. We show that the procedure terminates in the two following cases: (i) When the inductive rules that define the predicates occurring on the left-hand side of the entailment terminate, in which case the proof tree is always finite. (ii) When the theory is empty, in which case every valid sequent admits a rational proof tree, where the total number of pairwise distinct sequents occurring in the proof tree is doubly exponential w.r.t.\\ the size of the end-sequent. We also show that the validity problem is undecidable for a wide class of theories, even with a very low expressive power.", "comment": " MSC Class:           03B70                              ACM Class:           F.4.1                "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.13056", "id": "2201.13056", "pdf": "https://arxiv.org/pdf/2201.13056", "other": "https://arxiv.org/format/2201.13056"}, "title": "The complexity gap in the static analysis of cache accesses grows if procedure calls are added", "author_info": ["David Monniaux"], "summary": "The static analysis of cache accesses consists in correctly predicting which accesses are hits or misses. While there exist good exact and approximate analyses for caches implementing the least recently used (LRU) replacement policy, such analyses were harder to find for other replacement policies. A theoretical explanation was found: for an appropriate setting of analysis over control-flow graphs, cache analysis is PSPACE-complete for all common replacement policies (FIFO, PLRU, NMRU) except for LRU, for which it is only NP-complete. In this paper, we show that if procedure calls are added to the control flow, then the gap widens: analysis remains NP-complete for LRU, but becomes EXPTIME-complete for the three other policies. For this, we improve on earlier results on the complexity of reachability problems on Boolean programs with procedure calls. In addition, for the LRU policy we derive a backtracking algorithm as well as an approach for using it as a last resort after other analyses have failed to conclude.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.13042", "id": "2201.13042", "pdf": "https://arxiv.org/pdf/2201.13042"}, "title": "An efficient procedure to predict the acoustophoresis of axisymmetric irregular particles above ultrasound transducer array", "author_info": ["Tianquan Tang", "Lixi Huang"], "summary": "Acoustic radiation force and torque arising from wave scattering are able to translate and rotate matter without contact. However, the existing research mainly focused on manipulating simple symmetrical geometries, neglecting the significance of geometric features. For the non-spherical geometries, the shape of the object strongly affects its scattering properties, and thus the radiation force and torque as well as the acoustophoretic process. Here, we develop a semi-analytical framework to calculate the radiation force and torque exerted on the axisymmetric particles excited by a user-customized transducer array based on a conformal transformation approach, capturing the significance of the geometric features. The derivation framework is established under the computation coordinate system (CCS), whereas the particle is assumed to be static. For the dynamic processes, the rotation of particle is converted as the opposite rotation of transducer array, achieved by employing a rotation transformation to tune the incident driving field in the CCS. Later, the obtained radiation force and torque in the CCS should be transformed back to the observation coordinate system (OCS) for force and torque analysis. The radiation force and torque exerted on particles with different orientations are validated by comparing the full three-dimensional numerical solution in different phase distributions. It is found that the proposed method presents superior computational accuracy, high geometric adaptivity, and good robustness to various geometric features, while the computational efficiency is more than 100 times higher than that of the full numerical method. Furthermore, it is found that the dynamic trajectories of particles with different geometric features are completely different, indicating that the geometric features can be a potential degree of freedom to tune acoustophoretic process.", "comment": " Comments: 42 pages, 9 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11444", "id": "2201.11444", "pdf": "https://arxiv.org/pdf/2201.11444", "other": "https://arxiv.org/format/2201.11444"}, "title": "A Memetic Procedure for Global Multi-Objective Optimization", "author_info": ["Matteo Lapucci", "Pierluigi Mansueto", "Fabio Schoen"], "summary": "In this paper we consider multi-objective optimization problems over a box. The problem is very relevant and several computational approaches have been proposed in the literature. They broadly fall into two main classes: evolutionary methods, which are usually very good at exploring the feasible region and retrieving good solutions even in the nonconvex case, and descent methods, which excel in efficiently approximating good quality solutions. In this paper, first we confirm, through numerical experiments, the advantages and disadvantages of these approaches. Then we propose a new method which combines the good features of both. The resulting algorithm, which we call Non-dominated Sorting Memetic Algorithm (NSMA), besides enjoying interesting theoretical properties, excels in all of the numerical tests we performed on several, widely employed, test functions.", "comment": " MSC Class:           90C29; 90C30; 68W20                          "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11002", "id": "2201.11002", "pdf": "https://arxiv.org/pdf/2201.11002", "other": "https://arxiv.org/format/2201.11002"}, "title": "A Multi-rater Comparative Study of Automatic Target Localization Methods for Epilepsy Deep Brain Stimulation Procedures", "author_info": ["Han Liu", "Kathryn L. Holloway", "Dario J. Englot", "Benoit M. Dawant"], "summary": "Epilepsy is the fourth most common neurological disorder and affects people of all ages worldwide. Deep Brain Stimulation (DBS) has emerged as an alternative treatment option when anti-epileptic drugs or resective surgery cannot lead to satisfactory outcomes. To facilitate the planning of the procedure and for its standardization, it is desirable to develop an algorithm to automatically localize the DBS stimulation target, i.e., Anterior Nucleus of Thalamus (ANT), which is a challenging target to plan. In this work, we perform an extensive comparative study by benchmarking various localization methods for ANT-DBS. Specifically, the methods involved in this study include traditional registration method and deep-learning-based methods including heatmap matching and differentiable spatial to numerical transform (DSNT). Our experimental results show that the deep-learning (DL)-based localization methods that are trained with pseudo labels can achieve a performance that is comparable to the inter-rater and intra-rater variability and that they are orders of magnitude faster than traditional methods.", "comment": " Comments: Accepted by SPIE Medical Imaging 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.10990", "id": "2201.10990", "pdf": "https://arxiv.org/pdf/2201.10990", "other": "https://arxiv.org/format/2201.10990"}, "title": "Learning To Recognize Procedural Activities with Distant Supervision", "author_info": ["Xudong Lin", "Fabio Petroni", "Gedas Bertasius", "Marcus Rohrbach", "Shih-Fu Chang", "Lorenzo Torresani"], "summary": "In this paper we consider the problem of classifying fine-grained, multi-step activities (e.g., cooking different recipes, making disparate home improvements, creating various forms of arts and crafts) from long videos spanning up to several minutes. Accurately categorizing these activities requires not only recognizing the individual steps that compose the task but also capturing their temporal dependencies. This problem is dramatically different from traditional action classification, where models are typically optimized on videos that span only a few seconds and that are manually trimmed to contain simple atomic actions. While step annotations could enable the training of models to recognize the individual steps of procedural activities, existing large-scale datasets in this area do not include such segment labels due to the prohibitive cost of manually annotating temporal boundaries in long videos. To address this issue, we propose to automatically identify steps in instructional videos by leveraging the distant supervision of a textual knowledge base (wikiHow) that includes detailed descriptions of the steps needed for the execution of a wide variety of complex activities. Our method uses a language model to match noisy, automatically-transcribed speech from the video to step descriptions in the knowledge base. We demonstrate that video models trained to recognize these automatically-labeled steps (without manual supervision) yield a representation that achieves superior generalization performance on four downstream tasks: recognition of procedural activities, step classification, step forecasting and egocentric video classification.", "comment": " Comments: work in progress "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.10334", "id": "2201.10334", "pdf": "https://arxiv.org/pdf/2201.10334", "other": "https://arxiv.org/format/2201.10334"}, "title": "Towards Objective Metrics for Procedurally Generated Video Game Levels", "author_info": ["Michael Beukman", "Steven James", "Christopher Cleghorn"], "summary": "With increasing interest in procedural content generation by academia and game developers alike, it is vital that different approaches can be compared fairly. However, evaluating procedurally generated video game levels is often difficult, due to the lack of standardised, game-independent metrics. In this paper, we introduce two simulation-based evaluation metrics that involve analysing the behaviour of an A* agent to measure the diversity and difficulty of generated levels in a general, game-independent manner. Diversity is calculated by comparing action trajectories from different levels using the edit distance, and difficulty is measured as how much exploration and expansion of the A* search tree is necessary before the agent can solve the level. We demonstrate that our diversity metric is more robust to changes in level size and representation than current methods and additionally measures factors that directly affect playability, instead of focusing on visual information. The difficulty metric shows promise, as it correlates with existing estimates of difficulty in one of the tested domains, but it does face some challenges in the other domain. Finally, to promote reproducibility, we publicly release our evaluation framework.", "comment": " Comments: 7 pages, 10 figures. Code is located at https://github.com/Michael-Beukman/PCGNN "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.10311", "id": "2201.10311", "pdf": "https://arxiv.org/pdf/2201.10311", "other": "https://arxiv.org/format/2201.10311"}, "title": "Multi-purpose open-end monitoring procedures for multivariate observations based on the empirical distribution function", "author_info": ["Mark Holmes", "Ivan Kojadinovic", "Alex Verhoijsen"], "summary": "We propose nonparametric open-end sequential testing procedures that can detect all types of changes in the contemporary distribution function of multivariate observations. Their asymptotic properties are theoretically investigated under stationarity and under alternatives to stationarity. Monte Carlo experiments reveal their good finite-sample behavior in the case of continuous univariate observations. A short data example concludes the work.", "comment": " MSC Class:           62L99; 62E20; 62G10                          "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.10224", "id": "2201.10224", "pdf": "https://arxiv.org/pdf/2201.10224", "other": "https://arxiv.org/format/2201.10224"}, "title": "The Chv\u00e1tal-Gomory Procedure for Integer SDPs with Applications in Combinatorial Optimization", "author_info": ["Frank de Meijer", "Renata Sotirov"], "summary": "In this paper we study the well-known Chv\u00e1tal-Gomory (CG) procedure for the class of integer semidefinite programs (ISDPs). We prove several results regarding the hierarchy of relaxations obtained by iterating this procedure. We also study different formulations of the elementary closure of spectrahedra. A polyhedral description of the elementary closure for a specific type of spectrahedra is derived by exploiting total dual integrality for SDPs. Moreover, we show how to exploit (strengthened) CG cuts in a branch-and-cut framework for ISDPs. Different from existing algorithms in the literature, the separation routine in our approach exploits both the semidefinite and the integrality constraints. We provide separation routines for several common classes of binary SDPs resulting from combinatorial optimization problems. In the second part of the paper we present a comprehensive application of our approach to the quadratic traveling salesman problem (QTSP). Based on the algebraic connectivity of the directed Hamiltonian cycle, two ISDPs that model the QTSP are introduced. We show that the CG cuts resulting from these formulations contain several well-known families of cutting planes. Numerical results illustrate the practical strength of the CG cuts in our branch-and-cut algorithm, which outperforms alternative ISDP solvers and is able to solve large QTSP instances to optimality.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.08977", "id": "2201.08977", "pdf": "https://arxiv.org/pdf/2201.08977", "other": "https://arxiv.org/format/2201.08977"}, "title": "Semi-Supervised Adversarial Recognition of Refined Window Structures for Inverse Procedural Fa\u00e7ade Modeling", "author_info": ["Han Hu", "Xinrong Liang", "Yulin Ding", "Qisen Shang", "Bo Xu", "Xuming Ge", "Min Chen", "Ruofei Zhong", "Qing Zhu"], "summary": "Deep learning methods are notoriously data-hungry, which requires a large number of labeled samples. Unfortunately, the large amount of interactive sample labeling efforts has dramatically hindered the application of deep learning methods, especially for 3D modeling tasks, which require heterogeneous samples. To alleviate the work of data annotation for learned 3D modeling of fa\u00e7ades, this paper proposed a semi-supervised adversarial recognition strategy embedded in inverse procedural modeling. Beginning with textured LOD-2 (Level-of-Details) models, we use the classical convolutional neural networks to recognize the types and estimate the parameters of windows from image patches. The window types and parameters are then assembled into procedural grammar. A simple procedural engine is built inside an existing 3D modeling software, producing fine-grained window geometries. To obtain a useful model from a few labeled samples, we leverage the generative adversarial network to train the feature extractor in a semi-supervised manner. The adversarial training strategy can also exploit unlabeled data to make the training phase more stable. Experiments using publicly available fa\u00e7ade image datasets reveal that the proposed training strategy can obtain about 10% improvement in classification accuracy and 50% improvement in parameter estimation under the same network structure. In addition, performance gains are more pronounced when testing against unseen data featuring different fa\u00e7ade styles.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.08899", "id": "2201.08899", "pdf": "https://arxiv.org/pdf/2201.08899", "other": "https://arxiv.org/format/2201.08899"}, "title": "Scaling limits of loop-erased Markov chains on resistance spaces via a partial loop-erasing procedure", "author_info": ["Shiping Cao"], "summary": "We introduce partial loop-erasing operators. We show that by applying a refinement sequence of partial loop-erasing operators to a finite Markov chain, we get a process equivalent to the chronological loop-erased Markov chain. As an application, we construct loop-erased random paths on bounded domains of resistance spaces as the weak limit of the loop erasure of the Markov chains on a sequence of finite sets approximating the space. The limit is independent of the approximating sequences, and the random paths we constructed are simple paths almost surely. Finally, we show that the scaling limit of the loop-erased random walks on the Sierpi\u0144ski carpet graphs exists, and is equivalent to the loop-erased random paths on the Sierpi\u0144ksi carpet.", "comment": " MSC Class:           60J10; 82B41; 31E05; 60B10; 28A80                          "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.04560", "id": "2201.04560", "pdf": "https://arxiv.org/pdf/2201.04560", "other": "https://arxiv.org/format/2201.04560"}, "title": "Pseudopotential Lattice Boltzmann Method for boiling heat transfer: a mesh refinement procedure", "author_info": ["Alfredo Jaramillo", "Vin\u00edcius Pessoa Mapelli", "Luben Cabezas-G\u00f3mez"], "summary": "Boiling is a complex phenomenon where different non-linear physical interactions take place and for which the quantitative modeling of the mechanism involved is not fully developed yet. In the last years, many works have been published focusing on the numerical analysis of this problem. However, a lack of numerical works assessing quantitatively the sensitivity of these numerical simulations to grid parameters can be identified, especially for the Lattice Boltzmann method (LBM). The main goal of this work is to propose a mesh refinement methodology for simulating phase-change heat transfer problems by means of the pseudopotential LBM. This methodology was based on relating the physical parameters to their lattice counterparts for an arbitrary mesh under the viscous regime (where \u0394t\u221d\u0394x2). A suitable modification of the EOS parameters and the adjusting of thermodynamic consistency and surface tension for a certain \u0394x were the main steps of the proposed methodology. A first ensemble of simple simulations including the droplet vaporization and the Stefan problems was performed to validate the proposed method and to assess the influence of some physical mechanisms. Global norms in space and time were used to evaluate the variations of both the density and temperature fields for pool boiling simulations when the lattice discretization is refined. It was observed that the proposed methodology provides convergent results for all the problems considered, and the convergence orders depend on the complexity of the simulated phenomena.", "comment": " MSC Class:           76T10; 80A19                          "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.04486", "id": "2201.04486", "pdf": "https://arxiv.org/pdf/2201.04486", "other": "https://arxiv.org/format/2201.04486"}, "title": "Gauge embedding procedure: classical and quantum equivalence between dual models", "author_info": ["B. Alves Marques", "B. Z. Felippe", "A. P. Baeta Scarpelli", "L. C. T. Brito", "A. Yu. Petrov"], "summary": "In this paper the gauge embedding procedure of dualization is reassessed through a deeper analysis of the mutual equivalence of vector field models of more generic forms, explicitly, a general modified massive gauge-breaking extension of electrodynamics and its dual gauge-invariant model we derive in the paper. General relations between the vector field propagators and interaction terms of these models are obtained. Further, these models are shown to be equivalent at tree-level and one-loop physical calculations. Finally, we discuss extension of this equivalence to all loop orders.", "comment": " Comments: 13 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.01602", "id": "2201.01602", "pdf": "https://arxiv.org/pdf/2201.01602", "other": "https://arxiv.org/format/2201.01602"}, "title": "Sixth order weighted essentially non-oscillatory schemes with Z-type nonlinear weighting procedure for nonlinear degenerate parabolic equations", "author_info": ["Jiaxi Gu", "Samala Rathan"], "summary": "In this paper we develop new nonlinear weights of sixth order finite difference weighted essentially non-oscillatory (WENO) schemes for nonlinear degenerate parabolic equations. We construct two Z-type nonlinear weights: one is based on the L2 norm and the other depends on the L1 norm, yielding improved WENO schemes with more accurate resolution. We also confirm that the new devised nonlinear weights satisfy the sufficient conditions of sixth order accuracy. Finally, one- and two-dimensional numerical examples are presented to demonstrate the improved behavior of WENO schemes with new weighting procedure.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.01060", "id": "2201.01060", "pdf": "https://arxiv.org/pdf/2201.01060", "other": "https://arxiv.org/format/2201.01060"}, "title": "Trusting Machine Learning Results from Medical Procedures in the Operating Room", "author_info": ["Ali El-Merhi", "Helena Odenstedt Herg\u00e9s", "Linda Block", "Mikael Elam", "Richard Vithal", "Jaquette Liljencrantz", "Miroslaw Staron"], "summary": "Machine learning can be used to analyse physiological data for several purposes. Detection of cerebral ischemia is an achievement that would have high impact on patient care. We attempted to study if collection of continous physiological data from non-invasive monitors, and analysis with machine learning could detect cerebral ischemia in tho different setting, during surgery for carotid endarterectomy and during endovascular thrombectomy in acute stroke. We compare the results from the two different group and one patient from each group in details. While results from CEA-patients are consistent, those from thrombectomy patients are not and frequently contain extreme values such as 1.0 in accuracy. We conlcude that this is a result of short duration of the procedure and abundance of data with bad quality resulting in small data sets. These results can therefore not be trusted.", "comment": " Journal ref:         AAAI workshop on Trustworthy AI for Healthcare 2022       "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.15309", "id": "2112.15309", "pdf": "https://arxiv.org/pdf/2112.15309", "other": "https://arxiv.org/format/2112.15309"}, "title": "An Interface-Driven Adaptive Variational Procedure for Fully Eulerian Fluid-Structure Interaction via Phase-field Modeling", "author_info": ["Biswajeet Rath", "Xiaoyu Mao", "Rajeev K. Jaiman"], "summary": "In this paper, we present a novel interface-driven adaptive variational procedure using a fully Eulerian description of fluid-structure interaction. The proposed fully-Eulerian procedure involves a fixed background unstructured mesh on which the fluid-structure interface is treated implicitly. We model the fluid-structure interaction by the phase-field finite element formulation relying on a partitioned staggered integration of the convective Allen-Cahn equation with the unified momentum equation for both solid and fluid dynamics. We employ the positivity preserving variational scheme for a bounded and stable solution of the Allen-Cahn phase-field equation. To evaluate the solid stresses, the left Cauchy-Green deformation tensor is convected at each time step to trace the evolution of the solid strain in the Eulerian reference frame. We utilize the residual based error indicators and the newest vertex bisection algorithm for the adaptive refinement/coarsening of the unstructured mesh. The proposed nonlinear adaptive partitioned procedure restricts the coarsening step to the last non-linear iteration while simultaneously ensuring convergence properties of the coupled governing equations. We perform a detailed convergence and accuracy analysis via two benchmark problems namely, the pure solid system and a coupled fluid-solid system with an interface in a rectangle domain. We next systematically assess the performance of the adaptive procedure in terms of conservation properties for the increasing complexity of problems. Finally, we demonstrate our fully-Eulerian interface-driven adaptive FSI model to simulate the contact and bouncing phenomenon between an elastic solid and a rigid wall.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.12508", "id": "2112.12508", "pdf": "https://arxiv.org/pdf/2112.12508", "other": "https://arxiv.org/format/2112.12508"}, "title": "From Procedures, Objects, Actors, Components, Services, to Agents -- A Comparative Analysis of the History and Evolution of Programming Abstractions", "author_info": ["Jean-Pierre Briot"], "summary": "The objective of this chapter is to propose some retrospective analysis of the evolution of programming abstractions, from {\\em procedures}, {\\em objects}, {\\em actors}, {\\em components}, {\\em services}, up to {\\em agents}, %have some compare concepts of software component and of agent (and multi-agent system), %The method chosen is to by replacing them within a general historical perspective. Some common referential with three axes/dimensions is chosen: {\\em action selection} at the level of one entity, {\\em coupling flexibility} between entities, and {\\em abstraction level}. We indeed may observe some continuous quest for higher flexibility (through notions such as {\\em late binding}, or {\\em reification} of {\\em connections}) and higher level of {\\em abstraction}. Concepts of components, services and agents have some common objectives (notably, {\\em software modularity and reconfigurability}), with multi-agent systems raising further concepts of {\\em autonomy} and {\\em coordination}. notably through the notion of {\\em auto-organization} and the use of {\\em knowledge}. We hope that this analysis helps at highlighting some of the basic forces motivating the progress of programming abstractions and therefore that it may provide some seeds for the reflection about future programming abstractions.", "comment": " MSC Class:           97P40                              ACM Class:           D.3                "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.11298", "id": "2112.11298", "pdf": "https://arxiv.org/pdf/2112.11298", "other": "https://arxiv.org/format/2112.11298"}, "title": "Evaluation of diagnostic test procedures for SARS-CoV-2 using latent class models: comparison of antigen test kits and sampling for PCR testing based on Danish national data registries", "author_info": ["Jacob St\u00e6rk-\u00d8stergaard", "Carsten Kirkeby", "Lasse Engbo Christiansen", "Michael Asger Andersen", "Camilla Holten M\u00f8ller", "Marianne Voldstedlund", "Matthew J. Denwood"], "summary": "Antigen test kits have been used extensively as a screening tool during the worldwide pandemic of coronavirus (SARS-CoV-2). While it is generally expected that taking samples for analysis with PCR testing gives more reliable results than using antigen test kits, the overall sensitivity and specificity of the two protocols in the field have not yet been estimated without assuming that the PCR test constitutes a gold standard. We use latent class models to estimate the in situ performance of both PCR and antigen testing, using data from the Danish national registries. The results are based on 240,000 paired tests results sub-selected from the 55 million test results that were obtained in Denmark during the period from February 2021 until June 2021.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.09318", "id": "2112.09318", "pdf": "https://arxiv.org/pdf/2112.09318", "other": "https://arxiv.org/format/2112.09318"}, "title": "Procedural Kernel Networks", "author_info": ["Bartlomiej Wronski"], "summary": "In the last decade Convolutional Neural Networks (CNNs) have defined the state of the art for many low level image processing and restoration tasks such as denoising, demosaicking, upscaling, or inpainting. However, on-device mobile photography is still dominated by traditional image processing techniques, and uses mostly simple machine learning techniques or limits the neural network processing to producing low resolution masks. High computational and memory requirements of CNNs, limited processing power and thermal constraints of mobile devices, combined with large output image resolutions (typically 8--12 MPix) prevent their wider application. In this work, we introduce Procedural Kernel Networks (PKNs), a family of machine learning models which generate parameters of image filter kernels or other traditional algorithms. A lightweight CNN processes the input image at a lower resolution, which yields a significant speedup compared to other kernel-based machine learning methods and allows for new applications. The architecture is learned end-to-end and is especially well suited for a wide range of low-level image processing tasks, where it improves the performance of many traditional algorithms. We also describe how this framework unifies some previous work applying machine learning for common image restoration tasks.", "comment": " Comments: 11 pages, technical report "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.08129", "id": "2112.08129", "pdf": "https://arxiv.org/pdf/2112.08129", "other": "https://arxiv.org/format/2112.08129"}, "title": "A combinatorial procedure for tilting mutation", "author_info": ["Didrik Fosse"], "summary": "Tilting mutation is a way of producing new tilting complexes from old ones replacing only one indecomposable summand. In this paper, we give a purely combinatorial procedure for performing tilting mutation of suitable algebras.", "comment": " Comments: 30 pages, v2 corrected some typos "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.06447", "id": "2112.06447", "pdf": "https://arxiv.org/pdf/2112.06447", "other": "https://arxiv.org/format/2112.06447"}, "title": "SVIP: Sequence VerIfication for Procedures in Videos", "author_info": ["Yicheng Qian", "Weixin Luo", "Dongze Lian", "Xu Tang", "Peilin Zhao", "Shenghua Gao"], "summary": "In this paper, we propose a novel sequence verification task that aims to distinguish positive video pairs performing the same action sequence from negative ones with step-level transformations but still conducting the same task. Such a challenging task resides in an open-set setting without prior action detection or segmentation that requires event-level or even frame-level annotations. To that end, we carefully reorganize two publicly available action-related datasets with step-procedure-task structure. To fully investigate the effectiveness of any method, we collect a scripted video dataset enumerating all kinds of step-level transformations in chemical experiments. Besides, a novel evaluation metric Weighted Distance Ratio is introduced to ensure equivalence for different step-level transformations during evaluation. In the end, a simple but effective baseline based on the transformer with a novel sequence alignment loss is introduced to better characterize long-term dependency between steps, which outperforms other action recognition methods. Codes and data will be released.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.04548", "id": "2112.04548", "pdf": "https://arxiv.org/pdf/2112.04548"}, "title": "Relaxation of condition for convergence of dynamic regressor extension and mixing procedure", "author_info": ["Anton Glushchenko", "Konstantin Lastochkin"], "summary": "A generalization of the dynamic regressor extension and mixing procedure is proposed. First of all, it relaxes the requirement of the regressor finite excitation, which is known to be the condition for the mentioned procedure convergence. Secondly, if the weaker requirement of the regressor semi-finite-excitation is met, it guarantees the uniform ultimate boundedness of the parameter error and elementwise monotonicity for transients of some parameters to be identified.", "comment": " Comments: 21 pages. In Russian "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.04406", "id": "2112.04406", "pdf": "https://arxiv.org/pdf/2112.04406", "other": "https://arxiv.org/format/2112.04406"}, "title": "Adapting Procedural Content Generation to Player Personas Through Evolution", "author_info": ["Pedro M. Fernandes", "Jonathan J\u00f8rgensen", "Niels N. T. G. Poldervaart"], "summary": "Automatically adapting game content to players opens new doors for game development. In this paper we propose an architecture using persona agents and experience metrics, which enables evolving procedurally generated levels tailored for particular player personas. Using our game, \"Grave Rave\", we demonstrate that this approach successfully adapts to four rule-based persona agents over three different experience metrics. Furthermore, the adaptation is shown to be specific in nature, meaning that the levels are persona-conscious, and not just general optimizations with regard to the selected metric.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.02377", "id": "2112.02377", "pdf": "https://arxiv.org/pdf/2112.02377", "other": "https://arxiv.org/format/2112.02377"}, "title": "On the stability and performance of the solution of sparse linear systems by partitioned procedures", "author_info": ["Abal-Kassim Cheik Ahamed", "Frederic Magoules"], "summary": "In this paper, we present, evaluate and analyse the performance of parallel synchronous Jacobi algorithms by different partitioned procedures including band-row splitting, band-row sparsity pattern splitting and substructuring splitting, when solving sparse large linear systems. Numerical experiments performed on a set of academic 3D Laplace equation and on a real gravity matrices arising from the Chicxulub crater are exhibited, and show the impact of splitting on parallel synchronous iterations when solving sparse large linear systems. The numerical results clearly show the interest of substructuring methods compared to band-row splitting strategies.", "comment": " Comments: arXiv admin note: text overlap with arXiv:2108.13162 "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.13172", "id": "2111.13172", "pdf": "https://arxiv.org/pdf/2111.13172", "other": "https://arxiv.org/format/2111.13172"}, "title": "Security Threats and Cellular Network Procedures for Unmanned Aircraft Systems", "author_info": ["Aly Sabri Abdalla", "Vuk Marojevic"], "summary": "This paper discusses cellular network security for unmanned aircraft systems (UASs) and provides insights into the ongoing Third Generation Partnership Project (3GPP) standardization efforts with respect to authentication and authorization, location information privacy, and command and control signaling. We introduce the 3GPP reference architecture for network connected UAS and the new network functions as part of the 5G core network, discuss introduce the three security contexts, potential threats, and the 3GPP procedures. The paper identifies research opportunities for UAS communications security and recommends critical security features and processes to be considered for standardization.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.11716", "id": "2111.11716", "pdf": "https://arxiv.org/pdf/2111.11716"}, "title": "Robust Time-Varying Parameters Estimation Based on I-DREM Procedure", "author_info": ["Anton Glushchenko", "Konstantin Lastochkin"], "summary": "We consider a class of systems with time-varying parameters, which are written as linear regressions with bounded disturbances. The task is to estimate such parameters under the condition that the regressor is finitely exciting (FE). Considering such a problem statement, a new robust method is proposed to identify the time-varying parameters with bounded error, which could be reduced to the limit by the adjustment of such method parameters. For this purpose, the function of the system unknown parameters, which depends on time, is expanded into a Taylor series in order to turn the considered problem into the identification of the regression with piecewise-constant parameters. This results in the increase of the dimensionality of the problem to be solved. Then, the I-DREM procedure with exponential forgetting, resetting, and normalization of the regressor, which has been proposed earlier by the authors, is applied to the obtained regression. This allows one, in contrast to the known solutions, to get the dimensionality of the problem back to the initial one and provide the required exponential convergence of the parameter error to a bounded set with adjustable bound under the condition that the regressor is FE. In addition, this method guarantees that the parameter error is bounded beyond the regressor excitation interval. The above properties are proved analytically and shown via numerical simulations.", "comment": " Comments: 6 pages, 2 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.11687", "id": "2111.11687", "pdf": "https://arxiv.org/pdf/2111.11687"}, "title": "Development of a GPU-accelerated Monte Carlo dose calculation module for nuclear medicine, ARCHER-NM: Demonstration for a PET/CT imaging procedure", "author_info": ["Zhao Peng", "Yu Lu", "Yao Xu", "Yongzhe Li", "Bo Cheng", "Ming Ni", "Zhi Chen", "Xi Pei", "Qiang Xie", "Shicun Wang", "X. George Xu"], "summary": "This paper describes the development and validation of a Monte Carlo (MC) dose computing module dedicated to organ dose calculations of patients undergoing nuclear medicine (NM) internal radiation exposures involving 18F-FDG PET/CT examination. This new module extends the more-than-10-years-long ARCHER project that developed a GPU-accelerated MC dose engine by adding dedicated NM source-definition features. To validate the code, we compared dose distributions from the 0.511-MeV point photon source calculated for a water phantom as well as a patient PET/CT phantom against a well-tested MC code, GATE. The water-phantom results show excellent agreement, suggesting that the radiation physics module in the new NM code is adequate. To demonstrate the clinical utility and advantage of ARCHER-NM, one set of PET/CT data for an adult male NM patient is calculated using the new code. Radiosensitive organs in the CT dataset are segmented using a CNN-based tool called DeepViewer. The PET image intensity maps are converted to radioactivity distributions to allow for MC radiation transport dose calculations at the voxel level. The dose rate maps and corresponding statistical uncertainties were calculated for the duration of PET image acquisition. The dose rate results of the 18F-FDG PET imaging patient show that ARCHER-NM's results agree very well with those of the GATE within 0.58% to 4.11%. Most impressively, ARCHER-NM obtains such results in less than 0.5 minutes while it takes GATE as much as 376 minutes. This is the first study presenting GPU-accelerated patient-specific MC internal radiation dose rate calculations for clinically realistic 18F-FDG PET/CT imaging cases involving auto-segmentation of whole-body PET/CT images. This study suggests that modern computing tools -- ARCHER-NM and DeepViewer -- are accurate and fast enough for routine internal dosimetry in NM clinics.", "comment": " Comments: 17 pages, 5 figures, 1 table "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.02372", "id": "2111.02372", "pdf": "https://arxiv.org/pdf/2111.02372", "other": "https://arxiv.org/format/2111.02372"}, "title": "Parameter Estimation Procedures for Exponential-Family Random Graph Models on Count-Valued Networks: A Comparative Simulation Study", "author_info": ["Peng Huang", "Carter T. Butts"], "summary": "The exponential-family random graph models (ERGMs) have emerged as an important framework for modeling social and other networks. ERGMs for valued networks are less well-studied than their unvalued counterparts, and pose particular computational challenges. Networks with edge values on the non-negative integers (count-valued networks) are an important such case, with applications ranging from migration and trade flow data to data on frequency of interactions and encounters. Here, we propose an efficient maximum pseudo-likelihood estimation (MPLE) scheme for count-valued ERGMs, and compare its performance with existing Contrastive Divergence (CD) and Monte Carlo Maximum Likelihood Estimation (MCMLE) approaches via a simulation study based on migration flow networks in two U.S states. Our results suggest that edge value variance is a key factor in method performance, with high-variance edges posing a particular challenge for CD. MCMLE can work well but requires careful seeding in the high-variance case, and the MPLE itself performs well when edge variance is high.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.01977", "id": "2111.01977", "pdf": "https://arxiv.org/pdf/2111.01977", "other": "https://arxiv.org/format/2111.01977"}, "title": "Autonomous Magnetic Navigation Framework for Active Wireless Capsule Endoscopy Inspired by Conventional Colonoscopy Procedures", "author_info": ["Yangxin Xu", "Keyu Li", "Ziqi Zhao", "Max Q. -H. Meng"], "summary": "In recent years, simultaneous magnetic actuation and localization (SMAL) for active wireless capsule endoscopy (WCE) has been intensively studied to improve the efficiency and accuracy of the examination. In this paper, we propose an autonomous magnetic navigation framework for active WCE that mimics the \"insertion\" and \"withdrawal\" procedures performed by an expert physician in conventional colonoscopy, thereby enabling efficient and accurate navigation of a robotic capsule endoscope in the intestine with minimal user effort. First, the capsule is automatically propelled through the unknown intestinal environment and generate a viable path to represent the environment. Then, the capsule is autonomously navigated towards any point selected on the intestinal trajectory to allow accurate and repeated inspections of suspicious lesions. Moreover, we implement the navigation framework on a robotic system incorporated with advanced SMAL algorithms, and validate it in the navigation in various tubular environments using phantoms and an ex-vivo pig colon. Our results demonstrate that the proposed autonomous navigation framework can effectively navigate the capsule in unknown, complex tubular environments with a satisfactory accuracy, repeatability and efficiency compared with manual operation.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.01587", "id": "2111.01587", "pdf": "https://arxiv.org/pdf/2111.01587", "other": "https://arxiv.org/format/2111.01587"}, "title": "Procedural Generalization by Planning with Self-Supervised World Models", "author_info": ["Ankesh Anand", "Jacob Walker", "Yazhe Li", "Eszter V\u00e9rtes", "Julian Schrittwieser", "Sherjil Ozair", "Th\u00e9ophane Weber", "Jessica B. Hamrick"], "summary": "One of the key promises of model-based reinforcement learning is the ability to generalize using an internal model of the world to make predictions in novel environments and tasks. However, the generalization ability of model-based agents is not well understood because existing work has focused on model-free agents when benchmarking generalization. Here, we explicitly measure the generalization ability of model-based agents in comparison to their model-free counterparts. We focus our analysis on MuZero (Schrittwieser et al., 2020), a powerful model-based agent, and evaluate its performance on both procedural and task generalization. We identify three factors of procedural generalization -- planning, self-supervised representation learning, and procedural data diversity -- and show that by combining these techniques, we achieve state-of-the art generalization performance and data efficiency on Procgen (Cobbe et al., 2019). However, we find that these factors do not always provide the same benefits for the task generalization benchmarks in Meta-World (Yu et al., 2019), indicating that transfer remains a challenge and may require different approaches than procedural generalization. Overall, we suggest that building generalizable agents requires moving beyond the single-task, model-free paradigm and towards self-supervised model-based agents that are trained in rich, procedural, multi-task environments.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.13894", "id": "2110.13894", "pdf": "https://arxiv.org/pdf/2110.13894", "other": "https://arxiv.org/format/2110.13894"}, "title": "Fitting procedure for estimating interstellar extinction at high galactic latitudes", "author_info": ["Aleksandra Avdeeva", "Dana Kovaleva", "Oleg Malkov", "Alexey Nekrasov"], "summary": "We determine the interstellar extinction in the selected high-latitude areas of the sky based on Gaia EDR3 astrometry and photometry and spectroscopic data from RAVE survey. We approximate the results with the cosecant law in each area thus deriving the parameters of the barometric formula for different lines of sight. The distribution of the parameters over the entire sky is described using spherical harmonics. As a result, we get a mathematical description of the interstellar visual extinction for different lines of sight and distances from the Sun which can be used for estimating interstellar extinction.", "comment": " Comments: 11 pages, 6 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.13551", "id": "2110.13551", "pdf": "https://arxiv.org/pdf/2110.13551", "other": "https://arxiv.org/format/2110.13551"}, "title": "BuffetFS: Serve Yourself Permission Checks without Remote Procedure Calls", "author_info": ["Yanliang Zou", "Bin Yang", "Jian Zhang", "Wei Xue", "Shu Yin"], "summary": "The remote procedure call (a.k.a. RPC) latency becomes increasingly significant in a distributed file system. We propose BuffetFS, a user-level file system that optimizes I/O performance by eliminating the RPCs caused by \\texttt{open()} operation. By leveraging \\texttt{open()} from file servers to clients, BuffetFS can restrain the procedure calls for permission checks locally, hence avoid RPCs during the initial stage to access a file. BuffetFS can further reduce response time when users are accessing a large number of small files. We implement a BuffetFS prototype and integrate it into a storage cluster. Our preliminary evaluation results show that BuffetFS can offer up to 70\\% performance gain compared to the Lustre file system.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.13233", "id": "2110.13233", "pdf": "https://arxiv.org/pdf/2110.13233", "other": "https://arxiv.org/format/2110.13233"}, "title": "Decomposed Inductive Procedure Learning", "author_info": ["Daniel Weitekamp", "Christopher MacLellan", "Erik Harpstead", "Kenneth Koedinger"], "summary": "Recent advances in machine learning have made it possible to train artificially intelligent agents that perform with super-human accuracy on a great diversity of complex tasks. However, the process of training these capabilities often necessitates millions of annotated examples -- far more than humans typically need in order to achieve a passing level of mastery on similar tasks. Thus, while contemporary methods in machine learning can produce agents that exhibit super-human performance, their rate of learning per opportunity in many domains is decidedly lower than human-learning. In this work we formalize a theory of Decomposed Inductive Procedure Learning (DIPL) that outlines how different forms of inductive symbolic learning can be used in combination to build agents that learn educationally relevant tasks such as mathematical, and scientific procedures, at a rate similar to human learners. We motivate the construction of this theory along Marr's concepts of the computational, algorithmic, and implementation levels of cognitive modeling, and outline at the computational-level six learning capacities that must be achieved to accurately model human learning. We demonstrate that agents built along the DIPL theory are amenable to satisfying these capacities, and demonstrate, both empirically and theoretically, that DIPL enables the creation of agents that exhibit human-like learning performance.", "comment": " Comments: 38 pages, 7 figures, submitted to Journal of Artificial Intelligence "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.12879", "id": "2110.12879", "pdf": "https://arxiv.org/pdf/2110.12879", "other": "https://arxiv.org/format/2110.12879"}, "title": "Information efficient learning of complexly structured preferences: Elicitation procedures and their application to decision making under uncertainty", "author_info": ["Christoph Jansen", "Hannah Blocher", "Thomas Augustin", "Georg Schollmeyer"], "summary": "In this paper we propose efficient methods for elicitation of complexly structured preferences and utilize these in problems of decision making under (severe) uncertainty. Based on the general framework introduced in Jansen, Schollmeyer and Augustin (2018, Int. J. Approx. Reason), we now design elicitation procedures and algorithms that enable decision makers to reveal their underlying preference system (i.e. two relations, one encoding the ordinal, the other the cardinal part of the preferences) while having to answer as few as possible simple ranking questions. Here, two different approaches are followed. The first approach directly utilizes the collected ranking data for obtaining the ordinal part of the preferences, while their cardinal part is constructed implicitly by measuring meta data on the decision maker's consideration times. In contrast, the second approach explicitly elicits also the cardinal part of the decision maker's preference system, however, only an approximate version of it. This approximation is obtained by additionally collecting labels of preference strength during the elicitation procedure. For both approaches, we give conditions under which they produce the decision maker's true preference system and investigate how their efficiency can be improved. For the latter purpose, besides data-free approaches, we also discuss ways for effectively guiding the elicitation procedure if data from previous elicitation rounds is available. Finally, we demonstrate how the proposed elicitation methods can be utilized in problems of decision under (severe) uncertainty. Precisely, we show that under certain conditions optimal decisions can be found without fully specifying the preference system.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.11899", "id": "2110.11899", "pdf": "https://arxiv.org/pdf/2110.11899", "other": "https://arxiv.org/format/2110.11899"}, "title": "Challenges in Procedural Multimodal Machine Comprehension:A Novel Way To Benchmark", "author_info": ["Pritish Sahu", "Karan Sikka", "Ajay Divakaran"], "summary": "We focus on Multimodal Machine Reading Comprehension (M3C) where a model is expected to answer questions based on given passage (or context), and the context and the questions can be in different modalities. Previous works such as RecipeQA have proposed datasets and cloze-style tasks for evaluation. However, we identify three critical biases stemming from the question-answer generation process and memorization capabilities of large deep models. These biases makes it easier for a model to overfit by relying on spurious correlations or naive data patterns. We propose a systematic framework to address these biases through three Control-Knobs that enable us to generate a test bed of datasets of progressive difficulty levels. We believe that our benchmark (referred to as Meta-RecipeQA) will provide, for the first time, a fine grained estimate of a model's generalization capabilities. We also propose a general M3C model that is used to realize several prior SOTA models and motivate a novel hierarchical transformer based reasoning network (HTRN). We perform a detailed evaluation of these models with different language and visual features on our benchmark. We observe a consistent improvement with HTRN over SOTA (~18% in Visual Cloze task and ~13% in average over all the tasks). We also observe a drop in performance across all the models when testing on RecipeQA and proposed Meta-RecipeQA (e.g. 83.6% versus 67.1% for HTRN), which shows that the proposed dataset is relatively less biased. We conclude by highlighting the impact of the control knobs with some quantitative results.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.11554", "id": "2110.11554", "pdf": "https://arxiv.org/pdf/2110.11554", "other": "https://arxiv.org/format/2110.11554"}, "title": "Effect of the Atomic Dipole-Dipole Interaction on the Phase Diagrams of Field-Matter Interactions I: Variational procedure", "author_info": ["Sergio Cordero", "Octavio Casta\u00f1os", "Ram\u00f3n L\u00f3pez-Pe\u00f1a", "Eduardo Nahmad-Achar"], "summary": "We establish, within the second quantization method, the general dipole-dipole Hamiltonian interaction of a system of n-level atoms. The variational energy surface of the n-level atoms interacting with \u2113-mode fields and under the Van Der Waals forces is calculated with respect the tensorial product of matter and electromagnetic field coherent states. This is used to determine the quantum phase diagram associated to the ground state of the system and quantify the effect of the dipole-dipole Hamiltonian interaction. By considering real induced electric dipole moments, we find the quantum phase transitions for 2- and 3-level atomic systems interacting with 1- and 2- modes of the electromagnetic field, respectively. The corresponding order of the transitions is established by means of Ehrenfest classification; for some undetermined cases, we propose two procedures: the difference of the expectation value of the Casimir operators of the 2-level subsystems, and by maximizing the Bures distance between neighbor variational solutions.", "comment": " Comments: 17 pages, 9 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.10272", "id": "2110.10272", "pdf": "https://arxiv.org/pdf/2110.10272", "other": "https://arxiv.org/format/2110.10272"}, "title": "An application of a small area procedure with correlation between measurement error and sampling error to the Conservation Effects Assessment Project", "author_info": ["Emily Berg", "Sepideh Mosaferi"], "summary": "County level estimates of mean sheet and rill erosion from the Conservation Effects Assessment Project (CEAP) survey are useful for program development and evaluation. As a result of small county sample sizes, small area estimation procedures are needed. One variable that is related to sheet and rill erosion is the quantity of water runoff. The runoff is collected in the CEAP survey but is unavailable for the full population. We use an estimate of mean runoff from the CEAP survey as a covariate in a small area model for sheet and rill erosion. The measurement error in the covariate is important, as is the correlation between the measurement error and the sampling error. We conduct a detailed investigation of small area estimation in the presence of a correlation between the measurement error in the covariate and the sampling error in the response. The proposed methodology has a genuine need in CEAP, where the same survey that supplies the response also provides auxiliary information. In simulations, the proposed predictor is superior to small area predictors that assume the response and covariate are uncorrelated or that ignore the measurement error entirely. We conclude with practical recommendations.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.08486", "id": "2110.08486", "pdf": "https://arxiv.org/pdf/2110.08486", "other": "https://arxiv.org/format/2110.08486"}, "title": "Understanding Procedural Knowledge by Sequencing Multimodal Instructional Manuals", "author_info": ["Te-Lin Wu", "Alex Spangher", "Pegah Alipoormolabashi", "Marjorie Freedman", "Ralph Weischedel", "Nanyun Peng"], "summary": "The ability to sequence unordered events is an essential skill to comprehend and reason about real world task procedures, which often requires thorough understanding of temporal common sense and multimodal information, as these procedures are often communicated through a combination of texts and images. Such capability is essential for applications such as sequential task planning and multi-source instruction summarization. While humans are capable of reasoning about and sequencing unordered multimodal procedural instructions, whether current machine learning models have such essential capability is still an open question. In this work, we benchmark models' capability of reasoning over and sequencing unordered multimodal instructions by curating datasets from popular online instructional manuals and collecting comprehensive human annotations. We find models not only perform significantly worse than humans but also seem incapable of efficiently utilizing the multimodal information. To improve machines' performance on multimodal event sequencing, we propose sequentiality-aware pretraining techniques that exploit the sequential alignment properties of both texts and images, resulting in > 5% significant improvements.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.08464", "id": "2110.08464", "pdf": "https://arxiv.org/pdf/2110.08464", "other": "https://arxiv.org/format/2110.08464"}, "title": "Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems", "author_info": ["Zhongli Li", "Wenxuan Zhang", "Chao Yan", "Qingyu Zhou", "Chao Li", "Hongzhi Liu", "Yunbo Cao"], "summary": "Math Word Problem (MWP) solving needs to discover the quantitative relationships over natural language narratives. Recent work shows that existing models memorize procedures from context and rely on shallow heuristics to solve MWPs. In this paper, we look at this issue and argue that the cause is a lack of overall understanding of MWP patterns. We first investigate how a neural network understands patterns only from semantics, and observe that, if the prototype equations are the same, most problems get closer representations and those representations apart from them or close to other prototypes tend to produce wrong solutions. Inspired by it, we propose a contrastive learning approach, where the neural network perceives the divergence of patterns. We collect contrastive examples by converting the prototype equation into a tree and seeking similar tree structures. The solving model is trained with an auxiliary objective on the collected examples, resulting in the representations of problems with similar prototypes being pulled closer. We conduct experiments on the Chinese dataset Math23k and the English dataset MathQA. Our method greatly improves the performance in monolingual and multilingual settings.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.06091", "id": "2110.06091", "pdf": "https://arxiv.org/pdf/2110.06091", "other": "https://arxiv.org/format/2110.06091"}, "title": "Translation procedures in descriptive inner model theory", "author_info": ["Grigor Sargsyan"], "summary": "We develop a basic translation procedure that translates a hod mouse to an equivalent mouse. Unlike the translation procedure used by Steel and Zhu, our procedure works in a coarse setting without assuming AD. Nevertheless, the procedure resembles the one developed by Steel. We use the translation procedures to answer a question of Trevor Wilson. Namely, we show that if there is a stationary class of lambda such that lambda is a limit of Woodin cardinals and the derived model at lambda satisfies AD the first member of the Solovay sequence is is less than Theta then there is a transitive model M such that M contains the ordinals and satisfies there is a proper class of Woodin cardinals and a strong cardinal.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.05895", "id": "2110.05895", "pdf": "https://arxiv.org/pdf/2110.05895", "other": "https://arxiv.org/format/2110.05895"}, "title": "Adjusting Queries to Statistical Procedures Under Differential Privacy", "author_info": ["Tomer Shoham", "Yosef Rinott"], "summary": "We consider a dataset S held by an agency, and a vector query of interest, f(S)\u2208Rk, to be posed by an analyst, which contains the information required for certain planned statistical inference. The agency releases the requested vector query with noise that guarantees a given level of Differential Privacy -- DP(\u03b5,\u03b4) -- using the well-known Gaussian mechanism. The analyst can choose to pose the vector query f(S) or to adjust it by a suitable transformation that can make the agency's response more informative. For any given level of privacy DP(\u03b5,\u03b4) decided by the agency, we study natural situations where the analyst can achieve better statistical inference by adjusting the query with a suitable simple explicit transformation.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.04146", "id": "2110.04146", "pdf": "https://arxiv.org/pdf/2110.04146", "other": "https://arxiv.org/format/2110.04146"}, "title": "Arachnophobia Exposure Therapy using Experience-driven Procedural Content Generation via Reinforcement Learning (EDPCGRL)", "author_info": ["Athar Mahmoudi-Nejad", "Matthew Guzdial", "Pierre Boulanger"], "summary": "Personalized therapy, in which a therapeutic practice is adapted to an individual patient, leads to better health outcomes. Typically, this is accomplished by relying on a therapist's training and intuition along with feedback from a patient. While there exist approaches to automatically adapt therapeutic content to a patient, they rely on hand-authored, pre-defined rules, which may not generalize to all individuals. In this paper, we propose an approach to automatically adapt therapeutic content to patients based on physiological measures. We implement our approach in the context of arachnophobia exposure therapy, and rely on experience-driven procedural content generation via reinforcement learning (EDPCGRL) to generate virtual spiders to match an individual patient. In this initial implementation, and due to the ongoing pandemic, we make use of virtual or artificial humans implemented based on prior arachnophobia psychology research. Our EDPCGRL method is able to more quickly adapt to these virtual humans with high accuracy in comparison to existing, search-based EDPCG approaches.", "comment": " Journal ref:         Proceedings of the 17th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment 2021 (AIIDE-21)       "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.03262", "id": "2110.03262", "pdf": "https://arxiv.org/pdf/2110.03262", "other": "https://arxiv.org/format/2110.03262"}, "title": "Situated Dialogue Learning through Procedural Environment Generation", "author_info": ["Prithviraj Ammanabrolu", "Renee Jia", "Mark O. Riedl"], "summary": "We teach goal-driven agents to interactively act and speak in situated environments by training on generated curriculums. Our agents operate in LIGHT (Urbanek et al. 2019) -- a large-scale crowd-sourced fantasy text adventure game wherein an agent perceives and interacts with the world through textual natural language. Goals in this environment take the form of character-based quests, consisting of personas and motivations. We augment LIGHT by learning to procedurally generate additional novel textual worlds and quests to create a curriculum of steadily increasing difficulty for training agents to achieve such goals. In particular, we measure curriculum difficulty in terms of the rarity of the quest in the original training distribution -- an easier environment is one that is more likely to have been found in the unaugmented dataset. An ablation study shows that this method of learning from the tail of a distribution results in significantly higher generalization abilities as measured by zero-shot performance on never-before-seen quests.", "comment": " Comments: Preprint. Under Review "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.03181", "id": "2110.03181", "pdf": "https://arxiv.org/pdf/2110.03181", "other": "https://arxiv.org/format/2110.03181"}, "title": "Tile Embedding: A General Representation for Procedural Level Generation via Machine Learning", "author_info": ["Mrunal Jadhav", "Matthew Guzdial"], "summary": "In recent years, Procedural Level Generation via Machine Learning (PLGML) techniques have been applied to generate game levels with machine learning. These approaches rely on human-annotated representations of game levels. Creating annotated datasets for games requires domain knowledge and is time-consuming. Hence, though a large number of video games exist, annotated datasets are curated only for a small handful. Thus current PLGML techniques have been explored in limited domains, with Super Mario Bros. as the most common example. To address this problem, we present tile embeddings, a unified, affordance-rich representation for tile-based 2D games. To learn this embedding, we employ autoencoders trained on the visual and semantic information of tiles from a set of existing, human-annotated games. We evaluate this representation on its ability to predict affordances for unseen tiles, and to serve as a PLGML representation for annotated and unannotated games.", "comment": " Journal ref:         Proceedings of the 17th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment 2021 (AIIDE-21)       "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.02763", "id": "2110.02763", "pdf": "https://arxiv.org/pdf/2110.02763", "other": "https://arxiv.org/format/2110.02763"}, "title": "Quantum Blockchain based on Dimensional Lifting Generalized Gram-Schmidt Procedure", "author_info": ["Kumar Nilesh", "P. K. Panigrahi"], "summary": "The advancement of quantum computers undermines the security of classical blockchain, necessitating either a post-quantum upgrade of the existing architecture or creation of an inherently quantum blockchain. Here we propose a practically realizable model of a fully quantum blockchain based on generalized Gram-Schmidt procedure utilizing dimensional lifting. In this model, information of transactions stored in a multi-qubit state are subsequently encoded using the generalized Gram-Schmidt process. The chain is generated as a result of the reliance of orthogonalized state on the sequence of states preceding it. Various forking scenarios and their countermeasures are considered for the proposed model. It is shown to be secure even against quantum computing attacks using the no-cloning theorem and non-democratic nature of Generalized Gram-Schmidt orthogonalization. Finally, we outline a framework for a quantum token built on the same architecture as our blockchain.", "comment": " Comments: 16 pages, revised version "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.02715", "id": "2110.02715", "pdf": "https://arxiv.org/pdf/2110.02715", "other": "https://arxiv.org/format/2110.02715"}, "title": "Variance function estimation in regression model via aggregation procedures", "author_info": ["Ahmed Zaoui"], "summary": "In the regression problem, we consider the problem of estimating the variance function by the means of aggregation methods. We focus on two particular aggregation setting: Model Selection aggregation (MS) and Convex aggregation (C) where the goal is to select the best candidate and to build the best convex combination of candidates respectively among a collection of candidates. In both cases, the construction of the estimator relies on a two-step procedure and requires two independent samples. The first step exploits the first sample to build the candidate estimators for the variance function by the residual-based method and then the second dataset is used to perform the aggregation step. We show the consistency of the proposed method with respect to the L 2error both for MS and C aggregations. We evaluate the performance of these two methods in the heteroscedastic model and illustrate their interest in the regression problem with reject option.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.01822", "id": "2110.01822", "pdf": "https://arxiv.org/pdf/2110.01822", "other": "https://arxiv.org/format/2110.01822"}, "title": "Verified eigenvalue and eigenvector computations using complex moments and the Rayleigh-Ritz procedure for generalized Hermitian eigenvalue problems", "author_info": ["Akira Imakura", "Keiichi Morikuni", "Akitoshi Takayasu"], "summary": "We propose a verified computation method for eigenvalues in a region and the corresponding eigenvectors of generalized Hermitian eigenvalue problems. The proposed method uses complex moments to extract the eigencomponents of interest from a random matrix and uses the Rayleigh--Ritz procedure to project a given eigenvalue problem into a reduced eigenvalue problem. The complex moment is given by contour integral and approximated by using numerical quadrature. We split the error in the complex moment into the truncation error of the quadrature and rounding errors and evaluate each. This idea for error evaluation inherits our previous Hankel matrix approach, whereas the proposed method requires half the number of quadrature points for the previous approach to reduce the truncation error to the same order. Moreover, the Rayleigh--Ritz procedure approach forms a transformation matrix that enables verification of the eigenvectors. Numerical experiments show that the proposed method is faster than previous methods while maintaining verification performance.", "comment": " MSC Class:           65F15; 65G20; 65G50                          "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.01770", "id": "2110.01770", "pdf": "https://arxiv.org/pdf/2110.01770", "other": "https://arxiv.org/format/2110.01770"}, "title": "Procedure Planning in Instructional Videos via Contextual Modeling and Model-based Policy Learning", "author_info": ["Jing Bi", "Jiebo Luo", "Chenliang Xu"], "summary": "Learning new skills by observing humans' behaviors is an essential capability of AI. In this work, we leverage instructional videos to study humans' decision-making processes, focusing on learning a model to plan goal-directed actions in real-life videos. In contrast to conventional action recognition, goal-directed actions are based on expectations of their outcomes requiring causal knowledge of potential consequences of actions. Thus, integrating the environment structure with goals is critical for solving this task. Previous works learn a single world model will fail to distinguish various tasks, resulting in an ambiguous latent space; planning through it will gradually neglect the desired outcomes since the global information of the future goal degrades quickly as the procedure evolves. We address these limitations with a new formulation of procedure planning and propose novel algorithms to model human behaviors through Bayesian Inference and model-based Imitation Learning. Experiments conducted on real-world instructional videos show that our method can achieve state-of-the-art performance in reaching the indicated goals. Furthermore, the learned contextual information presents interesting features for planning in a latent space.", "comment": " Comments: ICCV 2021 Oral "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.01391", "id": "2110.01391", "pdf": "https://arxiv.org/pdf/2110.01391", "other": "https://arxiv.org/format/2110.01391"}, "title": "An Efficient Procedure for Mining Egocentric Temporal Motifs", "author_info": ["Antonio Longa", "Giulia Cencetti", "Bruno Lepri", "Andrea Passerini"], "summary": "Temporal graphs are structures which model relational data between entities that change over time. Due to the complex structure of data, mining statistically significant temporal subgraphs, also known as temporal motifs, is a challenging task. In this work, we present an efficient technique for extracting temporal motifs in temporal networks. Our method is based on the novel notion of egocentric temporal neighborhoods, namely multi-layer structures centered on an ego node. Each temporal layer of the structure consists of the first-order neighborhood of the ego node, and corresponding nodes in sequential layers are connected by an edge. The strength of this approach lies in the possibility of encoding these structures into a unique bit vector, thus bypassing the problem of graph isomorphism in searching for temporal motifs. This allows our algorithm to mine substantially larger motifs with respect to alternative approaches. Furthermore, by bringing the focus on the temporal dynamics of the interactions of a specific node, our model allows to mine temporal motifs which are visibly interpretable. Experiments on a number of complex networks of social interactions confirm the advantage of the proposed approach over alternative non-egocentric solutions. The egocentric procedure is indeed more efficient in revealing similarities and discrepancies among different social environments, independently of the different technologies used to collect data, which instead affect standard non-egocentric measures.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.01066", "id": "2110.01066", "pdf": "https://arxiv.org/pdf/2110.01066", "other": "https://arxiv.org/format/2110.01066"}, "title": "A Unified 3D Beam Training and Tracking Procedure for Terahertz Communication", "author_info": ["Boyu Ning", "Zhi Chen", "Zhongbao Tian", "Chong Han", "Shaoqian Li"], "summary": "Terahertz (THz) communication is considered as an attractive way to overcome the bandwidth bottleneck and satisfy the ever-increasing capacity demand in the future. Due to the high directivity and propagation loss of THz waves, a massive MIMO system using beamforming is envisioned as a promising technology in THz communication to realize high-gain and directional transmission. However, pilots, which are the fundamentals for many beamforming schemes, are challenging to be accurately detected in the THz band owing to the severe propagation loss. In this paper, a unified 3D beam training and tracking procedure is proposed to effectively realize the beamforming in THz communications, by considering the line-of-sight (LoS) propagation. In particular, a novel quadruple-uniform planar array (QUPA) architecture is analyzed to enlarge the signal coverage, increase the beam gain, and reduce the beam squint loss. Then, a new 3D grid-based (GB) beam training is developed with low complexity, including the design of the 3D codebook and training protocol. Finally, a simple yet effective grid-based hybrid (GBH) beam tracking is investigated to support THz beamforming in an efficient manner. The communication framework based on this procedure can dynamically trigger beam training/tracking depending on the real-time quality of service. Numerical results are presented to demonstrate the superiority of our proposed beam training and tracking over the benchmark methods.", "comment": " Comments: in IEEE Transactions on Wireless Communications, 2021. arXiv admin note: text overlap with arXiv:2104.02885 "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.00476", "id": "2110.00476", "pdf": "https://arxiv.org/pdf/2110.00476", "other": "https://arxiv.org/format/2110.00476"}, "title": "ResNet strikes back: An improved training procedure in timm", "author_info": ["Ross Wightman", "Hugo Touvron", "Herv\u00e9 J\u00e9gou"], "summary": "The influential Residual Networks designed by He et al. remain the gold-standard architecture in numerous scientific publications. They typically serve as the default architecture in studies, or as baselines when new architectures are proposed. Yet there has been significant progress on best practices for training neural networks since the inception of the ResNet architecture in 2015. Novel optimization & data-augmentation have increased the effectiveness of the training recipes. In this paper, we re-evaluate the performance of the vanilla ResNet-50 when trained with a procedure that integrates such advances. We share competitive training settings and pre-trained models in the timm open-source library, with the hope that they will serve as better baselines for future work. For instance, with our more demanding training setting, a vanilla ResNet-50 reaches 80.4% top-1 accuracy at resolution 224x224 on ImageNet-val without extra data or distillation. We also report the performance achieved with popular models with our training procedure.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.00322", "id": "2110.00322", "pdf": "https://arxiv.org/pdf/2110.00322"}, "title": "A Ratio to Evaluate Harvest Procedures Management in an Economic System where Resources Dynamics is ruled by an Ornstein- Uhlenbeck Process", "author_info": ["Manuel Alberto M. Ferreira", "Jos\u00e9 Ant\u00f3nio Filipe"], "summary": "The assessing resources dynamics problem, in the context of an economic system with Gaussian consumption and deterministic productivity, is considered in this paper. Basically it is presented a discrete time recursive equation that supports the recourse to the Ornstein-Uhlenbeck diffusion process. Some assumptions on the regeneration of the process are made, in order to observe the system equilibrium in what concerns the resources depreciation or accumulation. The objective of this work is to present a result on the sign of a ratio that can be used to evaluate harvest procedures in this context.", "comment": " Comments: 6 pages and no figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.12356", "id": "2109.12356", "pdf": "https://arxiv.org/pdf/2109.12356", "other": "https://arxiv.org/format/2109.12356"}, "title": "Detailed Comparison of Renormalization Scale-Setting Procedures based on the Principle of Maximum Conformality", "author_info": ["Xu-Dong Huang", "Jiang Yan", "Hong-Hao Ma", "Leonardo Di Giustino", "Jian-Ming Shen", "Xing-Gang Wu", "Stanley J. Brodsky"], "summary": "It has become conventional to simply guess the renormalization scale and choose an arbitrary range of uncertainty when making perturbative QCD (pQCD) predictions. However, this {\\it ad hoc} assignment of the renormalization scale and the estimate of the size of the resulting uncertainty leads to anomalous renormalization scheme-and-scale dependences. In fact, relations between physical observables must be independent of the theorist's choice of the renormalization scheme, and the renormalization scale in any given scheme at any given order of pQCD is not ambiguous. The {\\it Principle of Maximum Conformality} (PMC), which generalizes the conventional Gell-Mann-Low method for scale-setting in perturbative QED to non-Abelian QCD, provides a rigorous method for achieving unambiguous scheme-independent, fixed-order predictions for observables consistent with the principles of the renormalization group. The renormalization scale in the PMC is fixed such that all \u03b2 terms are eliminated from the perturbative series and resumed into the running coupling; this procedure results in a convergent, scheme-independent conformal series without factorial renormalon divergences. In this paper, we will give a detailed comparison of these PMC approaches by comparing their predictions for three important quantities Re+e\u2212, R\u03c4, and \u0393(H\u2192bb\u00af) up to four-loop pQCD corrections. Our numerical results show that the single-scale PMCs method, which involves a somewhat simpler analysis, can serve as a reliable substitute for the full multi-scale PMCm method, and that it leads to more precise pQCD predictions with less residual scale dependence.", "comment": " Comments: 15 pages, 3 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.10217", "id": "2109.10217", "pdf": "https://arxiv.org/pdf/2109.10217", "other": "https://arxiv.org/format/2109.10217"}, "title": "Shape Inference and Grammar Induction for Example-based Procedural Generation", "author_info": ["Gillis Hermans", "Thomas Winters", "Luc De Raedt"], "summary": "Designers increasingly rely on procedural generation for automatic generation of content in various industries. These techniques require extensive knowledge of the desired content, and about how to actually implement such procedural methods. Algorithms for learning interpretable generative models from example content could alleviate both difficulties. We propose SIGI, a novel method for inferring shapes and inducing a shape grammar from grid-based 3D building examples. This interpretable grammar is well-suited for co-creative design. Applied to Minecraft buildings, we show how the shape grammar can be used to automatically generate new buildings in a similar style.", "comment": " Journal ref:         International Conference on Computational Creativity 12 (2021) 342-349       "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.06395", "id": "2109.06395", "pdf": "https://arxiv.org/pdf/2109.06395", "other": "https://arxiv.org/format/2109.06395"}, "title": "An Inverse Procedural Modeling Pipeline for SVBRDF Maps", "author_info": ["Yiwei Hu", "Chengan He", "Valentin Deschaintre", "Julie Dorsey", "Holly Rushmeier"], "summary": "Procedural modeling is now the de facto standard of material modeling in industry. Procedural models can be edited and are easily extended, unlike pixel-based representations of captured materials. In this paper, we present a semi-automatic pipeline for general material proceduralization. Given Spatially-Varying Bidirectional Reflectance Distribution Functions (SVBRDFs) represented as sets of pixel maps, our pipeline decomposes them into a tree of sub-materials whose spatial distributions are encoded by their associated mask maps. This semi-automatic decomposition of material maps progresses hierarchically, driven by our new spectrum-aware material matting and instance-based decomposition methods. Each decomposed sub-material is proceduralized by a novel multi-layer noise model to capture local variations at different scales. Spatial distributions of these sub-materials are modeled either by a by-example inverse synthesis method recovering Point Process Texture Basis Functions (PPTBF) or via random sampling. To reconstruct procedural material maps, we propose a differentiable rendering-based optimization that recomposes all generated procedures together to maximize the similarity between our procedural models and the input material pixel maps. We evaluate our pipeline on a variety of synthetic and real materials. We demonstrate our method's capacity to process a wide range of material types, eliminating the need for artist designed material graphs required in previous work. As fully procedural models, our results expand to arbitrary resolution and enable high level user control of appearance.", "comment": " ACM Class:           I.3                "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.05954", "id": "2109.05954", "pdf": "https://arxiv.org/pdf/2109.05954", "other": "https://arxiv.org/format/2109.05954"}, "title": "Distributed Control of Descriptor Networks: A Convex Procedure for Augmented Sparsity", "author_info": ["Andrei Speril\u0103", "Cristian Oar\u0103", "Bogdan D. Ciubotaru", "\u015eerban Sab\u0103u"], "summary": "For networks of systems, not restricted to having proper transfer function matrices, we present a design framework which enables H\u221e optimization while imposing sparsity constraints on the controller's coprime factors. We propose a convex and iterative optimization procedure with guaranteed convergence to obtain distributed controllers. By exploiting the robustness-oriented nature of the approach, we discuss the means to ensure sparse representations of our control laws that are not supported by the network's nominal model.", "comment": " Comments: 8 pages, 2 figures, 1 table "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.04869", "id": "2109.04869", "pdf": "https://arxiv.org/pdf/2109.04869", "other": "https://arxiv.org/format/2109.04869"}, "title": "PlaTe: Visually-Grounded Planning with Transformers in Procedural Tasks", "author_info": ["Jiankai Sun", "De-An Huang", "Bo Lu", "Yun-Hui Liu", "Bolei Zhou", "Animesh Garg"], "summary": "In this work, we study the problem of how to leverage instructional videos to facilitate the understanding of human decision-making processes, focusing on training a model with the ability to plan a goal-directed procedure from real-world videos. Learning structured and plannable state and action spaces directly from unstructured videos is the key technical challenge of our task. There are two problems: first, the appearance gap between the training and validation datasets could be large for unstructured videos; second, these gaps lead to decision errors that compound over the steps. We address these limitations with Planning Transformer (PlaTe), which has the advantage of circumventing the compounding prediction errors that occur with single-step models during long model-based rollouts. Our method simultaneously learns the latent state and action information of assigned tasks and the representations of the decision-making process from human demonstrations. Experiments conducted on real-world instructional videos and an interactive environment show that our method can achieve a better performance in reaching the indicated goal than previous algorithms. We also validated the possibility of applying procedural tasks on a UR-5 platform.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2109.03361", "id": "2109.03361", "pdf": "https://arxiv.org/pdf/2109.03361", "other": "https://arxiv.org/format/2109.03361"}, "title": "On the CUSUM procedure for phase-type distributions: a L\u00e9vy fluctuation theory approach", "author_info": ["Jevgenijs Ivanovs", "Kazutoshi Yamazaki"], "summary": "We introduce a new method analyzing the cumulative sum (CUSUM) procedure in sequential change-point detection. When observations are phase-type distributed and the post-change distribution is given by exponential tilting of its pre-change distribution, the first passage analysis of the CUSUM statistic is reduced to that of a certain Markov additive process. By using the theory of the so-called scale matrix and further developing it, we derive exact expressions of the average run length, average detection delay, and false alarm probability under the CUSUM procedure. The proposed method is robust and applicable in a general setting with non-i.i.d. observations. Numerical results also are given.", "comment": " MSC Class:           60G51; 62L10; 62L15; 62M05; 94C12                          "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.02665", "id": "2109.02665", "pdf": "https://arxiv.org/pdf/2109.02665", "other": "https://arxiv.org/format/2109.02665"}, "title": "The TESS Mission Target Selection Procedure", "author_info": ["Michael Fausnaugh", "Ed Morgan", "Roland Vanderspek", "Joshua Pepper", "Christopher J. Burke", "Alan M. Levine", "Alexander Rudat", "Jesus Noel S. Villase\u00f1or", "Michael Vezie", "Robert F. Goeke", "George R. Ricker", "David W. Latham", "S. Seager", "Joshua N. Winn", "Jon M. Jenkins", "G. A. Bakos", "Thomas Barclay", "Zachory K. Berta-thompson", "Luke G. Bouma", "Patricia T. Boyd", "C. E. Brasseur", "Jennifer Burt", "Douglas A. Caldwell", "David Charbonneau", "J. Christensen-dalsgaard"], "summary": "We describe the target selection procedure by which stars are selected for 2-minute and 20-second observations by TESS. We first list the technical requirements of the TESS instrument and ground systems processing that limit the total number of target slots. We then describe algorithms used by the TESS Payload Operation Center (POC) to merge candidate targets requested by the various TESS mission elements (the Target Selection Working Group, TESS Asteroseismic Science Consortium, and Guest Investigator office). Lastly, we summarize the properties of the observed TESS targets over the two-year primary TESS mission. We find that the POC target selection algorithm results in 2.1 to 3.4 times as many observed targets as target slots allocated for each mission element. We also find that the sky distribution of observed targets is different from the sky distributions of candidate targets due to technical constraints that require a relatively even distribution of targets across the TESS fields of view. We caution researchers exploring statistical analyses of TESS planet-host stars that the population of observed targets cannot be characterized by any simple set of criteria applied to the properties of the input Candidate Target Lists.", "comment": " Comments: 15 pages, 6 figures, accepted for publication in PASP "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.00171", "id": "2109.00171", "pdf": "https://arxiv.org/pdf/2109.00171"}, "title": "A generalized bootstrap procedure of the standard error and confidence interval estimation for inverse probability of treatment weighting", "author_info": ["Tenglong Li", "Jordan Lawson"], "summary": "The inverse probability of treatment weighting (IPTW) approach is commonly used in propensity score analysis to infer causal effects in regression models. Due to oversized IPTW weights and errors associated with propensity score estimation, the IPTW approach can underestimate the standard error of causal effect. To remediate this, bootstrap standard errors have been recommended to replace the IPTW standard error, but the ordinary bootstrap (OB) procedure might still result in underestimation of the standard error because of its inefficient sampling algorithm and un-stabilized weights. In this paper, we develop a generalized bootstrap (GB) procedure for estimating the standard error of the IPTW approach. Compared with the OB procedure, the GB procedure has much lower risk of underestimating the standard error and is more efficient for both point and standard error estimates. The GB procedure also has smaller risk of standard error underestimation than the ordinary bootstrap procedure with trimmed weights, with comparable efficiencies. We demonstrate the effectiveness of the GB procedure via a simulation study and a dataset from the National Educational Longitudinal Study-1988 (NELS-88).", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2108.12623", "id": "2108.12623", "pdf": "https://arxiv.org/pdf/2108.12623", "other": "https://arxiv.org/format/2108.12623"}, "title": "ZAP: Z-value Adaptive Procedures for False Discovery Rate Control with Side Information", "author_info": ["Dennis Leung", "Wenguang Sun"], "summary": "Adaptive multiple testing with covariates is an important research direction that has gained major attention in recent years. It has been widely recognized that leveraging side information provided by auxiliary covariates can improve the power of false discovery rate (FDR) procedures. Currently, most such procedures are devised with p-values as their main statistics. However, for two-sided hypotheses, the usual data processing step that transforms the primary statistics, known as z-values, into p-values not only leads to a loss of information carried by the main statistics, but can also undermine the ability of the covariates to assist with the FDR inference. We develop a z-value based covariate-adaptive (ZAP) methodology that operates on the intact structural information encoded jointly by the z-values and covariates. It seeks to emulate the oracle z-value procedure via a working model, and its rejection regions significantly depart from those of the p-value adaptive testing approaches. The key strength of ZAP is that the FDR control is guaranteed with minimal assumptions, even when the working model is misspecified. We demonstrate the state-of-the-art performance of ZAP using both simulated and real data, which shows that the efficiency gain can be substantial in comparison with p-value based methods. Our methodology is implemented in the R package zap.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2108.12600", "id": "2108.12600", "pdf": "https://arxiv.org/pdf/2108.12600", "other": "https://arxiv.org/format/2108.12600"}, "title": "A robust fusion-extraction procedure with summary statistics in the presence of biased sources", "author_info": ["Ruoyu Wang", "Qihua Wang", "Wang Miao"], "summary": "Information from various data sources is increasingly available nowadays. However, some of the data sources may produce biased estimation due to commonly encountered biased sampling, population heterogeneity, or model misspecification. This calls for statistical methods to combine information in the presence of biased sources. In this paper, a robust data fusion-extraction method is proposed. The method can produce a consistent estimator of the parameter of interest even if many of the data sources are biased. The proposed estimator is easy to compute and only employs summary statistics, and hence can be applied to many different fields, e.g. meta-analysis, Mendelian randomisation and distributed system. Moreover, the proposed estimator is asymptotically equivalent to the oracle estimator that only uses data from unbiased sources under some mild conditions. Asymptotic normality of the proposed estimator is also established. In contrast to the existing meta-analysis methods, the theoretical properties are guaranteed even if both the number of data sources and the dimension of the parameter diverge as the sample size increases, which ensures the performance of the proposed method over a wide range. The robustness and oracle property is also evaluated via simulation studies. The proposed method is applied to a meta-analysis data set to evaluate the surgical treatment for the moderate periodontal disease, and a Mendelian randomization data set to study the risk factors of head and neck cancer.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2108.12448", "id": "2108.12448", "pdf": "https://arxiv.org/pdf/2108.12448", "other": "https://arxiv.org/format/2108.12448"}, "title": "Classical Artificial Neural Network Training Using Quantum Walks as a Search Procedure", "author_info": ["Luciano S. de Souza", "Jonathan H. A. de Carvalho", "Tiago A. E. Ferreira"], "summary": "This paper proposes a computational procedure that applies a quantum algorithm to train classical artificial neural networks. The goal of the procedure is to apply quantum walk as a search algorithm in a complete graph to find all synaptic weights of a classical artificial neural network. Each vertex of this complete graph represents a possible synaptic weight set in the w-dimensional search space, where w is the number of weights of the neural network. To know the number of iterations required \\textit{a priori} to obtain the solutions is one of the main advantages of the procedure. Another advantage is that the proposed method does not stagnate in local minimums. Thus, it is possible to use the quantum walk search procedure as an alternative to the backpropagation algorithm. The proposed method was employed for a XOR problem to prove the proposed concept. To solve this problem, the proposed method trained a classical artificial neural network with nine weights. However, the procedure can find solutions for any number of dimensions. The results achieved demonstrate the viability of the proposal, contributing to machine learning and quantum computing researches.", "comment": " Journal ref:         IEEE Transactions on Computers, 13 January 2021       "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.12266", "id": "2108.12266", "pdf": "https://arxiv.org/pdf/2108.12266"}, "title": "MRI-compatible electromagnetic servomotors for image-guided robotic procedures", "author_info": ["Lorne W. Hofstetter", "Rock Hadley", "Robb Merrill", "Huy Pham", "Gabriel C. Fine", "Dennis L. Parker"], "summary": "Combining the unmatched soft-tissue imaging capabilities of magnetic resonance imaging (MRI) with high precision robotics has the potential to improve the accuracy, precision, and safety of a wide range of image-guided medical procedures. However, the goal of highly functional MRI-compatible robotic systems has not yet been realized because conventional electromagnetic servomotors used by medical robots can become dangerous projectiles near the strong magnetic field of an MRI scanner. Here we report a novel electromagnetic servomotor design that is constructed from non-magnetic components and can operate within the patient area of clinical scanners. We show that this design enables high-torque and precisely controlled rotary actuation during imaging. Using this servomotor design, an MRI-compatible robot was constructed and tested. The robot demonstrated that the linear forces required to manipulate large diameter surgical instruments in tissues could be achieved during simultaneous imaging with MRI. This work presents the first fully functional electromagnetic servomotor that can be safely operated (while imaging) in the patient area of a 3 Tesla clinical MRI scanner.", "comment": " Comments: 20 pages, 5 figures, 1 table "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.11007", "id": "2108.11007", "pdf": "https://arxiv.org/pdf/2108.11007", "other": "https://arxiv.org/format/2108.11007"}, "title": "Proper Plasma Analysis Practice (PPAP), an Integrated Procedure of the Extinction Correction and Plasma Diagnostics: a Demo with an HST/WFC3 Image Set of NGC6720", "author_info": ["Toshiya Ueta", "Masaaki Otsuka"], "summary": "In this work, we propose a proper plasma analysis practice (PPAP), an updated procedure of plasma diagnostics in the era of spatially-resolved spectroscopy. In particular, we emphasize the importance of performing both of the extinction correction and the direct method of plasma diagnostics simultaneously as an integrated process. This approach is motivated by the reciprocal dependence between critical parameters in these analyses, which can be resolved by iteratively seeking a converged solution. The use of PPAP allows us to eliminate unnecessary assumptions that prevent us from obtaining an exact solution at each element of the spectral imaging data. Using a suite of HST/WFC3 narrowband images of the planetary nebula, NGC 6720, we validate PPAP by (1) simultaneously and self-consistently deriving the extinction, c(Hb), and electron density/temperature distribution, (n_e, T_e), maps that are consistent with each other, and (2) obtaining identical metal abundance distribution maps, (n(N^+)/n(H^+), n(S^+)/n(H^+)), from multiple emission line maps at different wavelengths/transition energies. We also determine that the derived c(Hb) consists both of the ISM and circumsource components and that the ionized gas-to-dust mass ratio in the main ring is at least 437 and as high as about 1600. We find that, unless we deliberately seek self-consistency, uncertainties at tens of per cent can easily arise in outcomes, making it impossible to discern actual spatial variations that occurs at the same level, defeating the purpose of conducting spatially resolved spectroscopic observations.", "comment": " Comments: 27 pages, 18 figures, to be published as a Tutorial in the Publication of the Astronomical Society of the Pacific "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.10595", "id": "2108.10595", "pdf": "https://arxiv.org/pdf/2108.10595", "other": "https://arxiv.org/format/2108.10595"}, "title": "A Generalized Knockoff Procedure for FDR Control in Structural Change Detection", "author_info": ["Jingyuan Liu", "Ao Sun", "Yuan Ke"], "summary": "Controlling false discovery rate (FDR) is crucial for variable selection, multiple testing, among other signal detection problems. In literature, there is certainly no shortage of FDR control strategies when selecting individual features. Yet lack of relevant work has been done regarding structural change detection, including, but not limited to change point identification, profile analysis for piecewise constant coefficients, and integration analysis with multiple data sources. In this paper, we propose a generalized knockoff procedure (GKnockoff) for FDR control under such problem settings. We prove that the GKnockoff possesses pairwise exchangeability, and is capable of controlling the exact FDR under finite sample sizes. We further explore GKnockoff under high dimensionality, by first introducing a new screening method to filter the high-dimensional potential structural changes. We adopt a data splitting technique to first reduce the dimensionality via screening and then conduct GKnockoff on the refined selection set. Numerical comparisons with other methods show the superior performance of GKnockoff, in terms of both FDR control and power. We also implement the proposed method to analyze a macroeconomic dataset for detecting change points in the consumer price index, as well as the unemployment rate.", "comment": " Comments: 45 pages, 11 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.08762", "id": "2108.08762", "pdf": "https://arxiv.org/pdf/2108.08762", "other": "https://arxiv.org/format/2108.08762"}, "title": "Dynamic Difficulty Adjustment in Virtual Reality Exergames through Experience-driven Procedural Content Generation", "author_info": ["Tobias Huber", "Silvan Mertes", "Stanislava Rangelova", "Simon Flutura", "Elisabeth Andr\u00e9"], "summary": "Virtual Reality (VR) games that feature physical activities have been shown to increase players' motivation to do physical exercise. However, for such exercises to have a positive healthcare effect, they have to be repeated several times a week. To maintain player motivation over longer periods of time, games often employ Dynamic Difficulty Adjustment (DDA) to adapt the game's challenge according to the player's capabilities. For exercise games, this is mostly done by tuning specific in-game parameters like the speed of objects. In this work, we propose to use experience-driven Procedural Content Generation for DDA in VR exercise games by procedurally generating levels that match the player's current capabilities. Not only finetuning specific parameters but creating completely new levels has the potential to decrease repetition over longer time periods and allows for the simultaneous adaptation of the cognitive and physical challenge of the exergame. As a proof-of-concept, we implement an initial prototype in which the player must traverse a maze that includes several exercise rooms, whereby the generation of the maze is realized by a neural network. Passing those exercise rooms requires the player to perform physical activities. To match the player's capabilities, we use Deep Reinforcement Learning to adjust the structure of the maze and to decide which exercise rooms to include in the maze. We evaluate our prototype in an exploratory user study utilizing both biodata and subjective questionnaires.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2108.08129", "id": "2108.08129", "pdf": "https://arxiv.org/pdf/2108.08129", "other": "https://arxiv.org/format/2108.08129"}, "title": "Quantitative Uniform Stability of the Iterative Proportional Fitting Procedure", "author_info": ["George Deligiannidis", "Valentin De Bortoli", "Arnaud Doucet"], "summary": "We establish the uniform in time stability, w.r.t. the marginals, of the Iterative Proportional Fitting Procedure, also known as Sinkhorn algorithm, used to solve entropy-regularised Optimal Transport problems. Our result is quantitative and stated in terms of the 1-Wasserstein metric. As a corollary we establish a quantitative stability result for Schr\u00f6dinger bridges.", "comment": " Comments: 14 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.07552", "id": "2108.07552", "pdf": "https://arxiv.org/pdf/2108.07552", "other": "https://arxiv.org/format/2108.07552"}, "title": "A simple equilibration procedure leading to polynomial-degree-robust a posteriori error estimators for the curl-curl problem", "author_info": ["T. Chaumont-Frelet"], "summary": "We introduce two a posteriori error estimators for N\u00e9d\u00e9lec finite element discretizations of the curl-curl problem. These estimators pertain to a new Prager-Synge identity and an associated equilibration procedure. They are reliable and efficient, and the error estimates are polynomial-degree-robust. In addition, when the domain is convex, the reliability constants are fully computable. The proposed error estimators are also cheap and easy to implement, as they are computed by solving divergence-constrained minimization problems over edge patches. Numerical examples highlight our key findings, and show that both estimators are suited to drive adaptive refinement algorithms. Besides, these examples seem to indicate that guaranteed upper bounds can be achieved even in non-convex domains.", "comment": " Report number:           hal-03323859                                    "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.06136", "id": "2108.06136", "pdf": "https://arxiv.org/pdf/2108.06136"}, "title": "On the evaluation of research software: the CDUR procedure", "author_info": ["Teresa Gomez-Diaz", "Tomas Recio"], "summary": "Background: Evaluation of the quality of research software is a challenging and relevant issue, still not sufficiently addressed by the scientific community.", "comment": " Journal ref:         F1000Research 2019, 8:1353       "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.04776", "id": "2108.04776", "pdf": "https://arxiv.org/pdf/2108.04776", "other": "https://arxiv.org/format/2108.04776"}, "title": "BioCode: A Data-Driven Procedure to Learn the Growth of Biological Networks", "author_info": ["Emre Sefer"], "summary": "Probabilistic biological network growth models have been utilized for many tasks including but not limited to capturing mechanism and dynamics of biological growth activities, null model representation, capturing anomalies, etc. Well-known examples of these probabilistic models are Kronecker model, preferential attachment model, and duplication-based model. However, we should frequently keep developing new models to better fit and explain the observed network features while new networks are being observed. Additionally, it is difficult to develop a growth model each time we study a new network. In this paper, we propose BioCode, a framework to automatically discover novel biological growth models matching user-specified graph attributes in directed and undirected biological graphs. BioCode designs a basic set of instructions which are common enough to model a number of well-known biological graph growth models. We combine such instruction-wise representation with a genetic algorithm based optimization procedure to encode models for various biological networks. We mainly evaluate the performance of BioCode in discovering models for biological collaboration networks, gene regulatory networks, metabolic networks, and protein interaction networks which features such as assortativity, clustering coefficient, degree distribution closely match with the true ones in the corresponding real biological networks. As shown by the tests on the simulated graphs, the variance of the distributions of biological networks generated by BioCode is similar to the known models' variance for these biological network types.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2108.04409", "id": "2108.04409", "pdf": "https://arxiv.org/pdf/2108.04409", "other": "https://arxiv.org/format/2108.04409"}, "title": "On Procedural Adversarial Noise Attack And Defense", "author_info": ["Jun Yan", "Xiaoyang Deng", "Huilin Yin", "Wancheng Ge"], "summary": "Deep Neural Networks (DNNs) are vulnerable to adversarial examples which would inveigle neural networks to make prediction errors with small perturbations on the input images. Researchers have been devoted to promoting the research on the universal adversarial perturbations (UAPs) which are gradient-free and have little prior knowledge on data distributions. Procedural adversarial noise attack is a data-free universal perturbation generation method. In this paper, we propose two universal adversarial perturbation (UAP) generation methods based on procedural noise functions: Simplex noise and Worley noise. In our framework, the shading which disturbs visual classification is generated with rendering technology. Without changing the semantic representations, the adversarial examples generated via our methods show superior performance on the attack.", "comment": " Comments: Remove theoretical analysis and focus on the empirical study "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.01126", "id": "2108.01126", "pdf": "https://arxiv.org/pdf/2108.01126", "other": "https://arxiv.org/format/2108.01126"}, "title": "Universal renormalization procedure for higher curvature gravities in D\u22645", "author_info": ["Ignacio J. Araya", "Jose D. Edelstein", "Alberto Rivadulla Sanchez", "David Vazquez Rodriguez", "Alejandro Vilar Lopez"], "summary": "We implement a universal method for renormalizing AdS gravity actions applicable to arbitrary higher curvature theories in up to five dimensions. The renormalization procedure considers the extrinsic counterterm for Einstein-AdS gravity given by the Kounterterms scheme, but with a theory-dependent coupling constant that is fixed by the requirement of renormalization for the vacuum solution. This method is shown to work for a generic higher curvature gravity with arbitrary couplings except for a zero measure subset, which includes well-known examples where the asymptotic behavior is modified and the AdS vacua are degenerate, such as Chern-Simons gravity in 5D, Conformal Gravity in 4D and New Massive Gravity in 3D. In order to show the universality of the scheme, we perform a decomposition of the equations of motion into their normal and tangential components with respect to the Poincare coordinate and study the Fefferman-Graham expansion of the metric. We verify the cancellation of divergences of the on-shell action and the well-posedness of the variational principle.", "comment": " Comments: 33 pages, minor amendments and references added "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.00056", "id": "2108.00056", "pdf": "https://arxiv.org/pdf/2108.00056", "other": "https://arxiv.org/format/2108.00056"}, "title": "Procedural Generation of 3D Maps with Snappable Meshes", "author_info": ["Rafael C. e Silva", "Nuno Fachada", "Diogo de Andrade", "N\u00e9lio C\u00f3dices"], "summary": "In this paper we present a technique for procedurally generating 3D maps using a set of premade meshes which snap together based on designer-specified visual constraints. The proposed approach avoids size and layout limitations, offering the designer control over the look and feel of the generated maps, as well as immediate feedback on a given map's navigability. A prototype implementation of the method, developed in the Unity game engine, is discussed, and a number of case studies are analyzed. These include a multiplayer game where the method was used, together with a number of illustrative examples which highlight various parameterizations and piece selection methods. The technique can be used as a designer-centric map composition method and/or as a prototyping system in 3D level design, opening the door for quality map and level creation in a fraction of the time of a fully human-based approach.", "comment": " MSC Class:           68T99                              ACM Class:           I.2.1                "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.12513", "id": "2107.12513", "pdf": "https://arxiv.org/pdf/2107.12513"}, "title": "Next generation combined sonic-hotfilm anemometer: wind alignment and automated calibration procedure using deep learning", "author_info": ["Roni H. Goldshmid", "Ewelina Winiarska", "Dan Liberzon"], "summary": "The study of naturally occurring turbulent flows requires ability to collect empirical data down to the fine scales. While hotwire anemometry offers such ability, the open field studies are uncommon due to the cumbersome calibration procedure and operational requirements of hotwire anemometry, e.g., constant ambient properties and steady flow conditions. The combo probe-the combined sonic-hotfilm anemometer developed and tested over the last decade-has demonstrated its ability to overcome this hurdle. The old-er generation had a limited wind alignment range of 120 degrees and the in-situ calibration procedure was human decision based. This study presents the next generation of the combo probe design, and the new fully automated in-situ calibration procedure implementing deep learning. The elegant new design now enables measurements of the incoming wind flow in a 360-degree range. The improved calibration procedure is shown to have the robustness necessary for operation in everchanging open field flow and environmental conditions. This is especially useful with diurnally changing environments or non-stationary measuring stations, i.e., probes placed on moving platforms like boats, drones, and weather balloons. Together, the updated design and the new calibration procedure, allow for continuous field measurements with minimal to no human interaction, enabling near real-time monitoring of fine-scale turbulent fluctuations. Integration of these probes will contribute toward generation of a large pool of field data to be collected to unravel the intricacies of all scales of turbulent flows occurring in natural setups.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2107.11931", "id": "2107.11931", "pdf": "https://arxiv.org/pdf/2107.11931", "other": "https://arxiv.org/format/2107.11931"}, "title": "Max-Type and Sum-Type Procedures for Online Change-Point Detection in the Mean of High-Dimensional Data", "author_info": ["Jun Li"], "summary": "We propose two procedures to detect a change in the mean of high-dimensional online data. One is based on a max-type U-statistic and another is based on a sum-type U-statistic. Theoretical properties of the two procedures are explored in the high dimensional setting. More precisely, we derive their average run lengths (ARLs) when there is no change point, and expected detection delays (EDDs) when there is a change point. Accuracy of the theoretical results is confirmed by simulation studies. The practical use of the proposed procedures is demonstrated by detecting an abrupt change in PM2.5 concentrations. The current study attempts to extend the results of the CUSUM and Shiryayev-Roberts procedures previously established in the univariate setting.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2107.11560", "id": "2107.11560", "pdf": "https://arxiv.org/pdf/2107.11560", "other": "https://arxiv.org/format/2107.11560"}, "title": "A Fast Temporal Decomposition Procedure for Long-horizon Nonlinear Dynamic Programming", "author_info": ["Sen Na", "Mihai Anitescu", "Mladen Kolar"], "summary": "We propose a fast temporal decomposition procedure for solving long-horizon nonlinear dynamic programs. The core of the procedure is sequential quadratic programming (SQP), with a differentiable exact augmented Lagrangian being the merit function. Within each SQP iteration, we solve the Newton system approximately using an overlapping temporal decomposition. We show that the approximated search direction is still a descent direction of the augmented Lagrangian, provided the overlap size and penalty parameters are suitably chosen, which allows us to establish global convergence. Moreover, we show that a unit stepsize is accepted locally for the approximated search direction, and further establish a uniform, local linear convergence over stages. Our local convergence rate matches the rate of the recent Schwarz scheme \\cite{Na2020Overlapping}. However, the Schwarz scheme has to solve nonlinear subproblems to optimality in each iteration, while we only perform one Newton step instead. Numerical experiments validate our theories and demonstrate the superiority of our method.", "comment": " Comments: 35 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.09996", "id": "2107.09996", "pdf": "https://arxiv.org/pdf/2107.09996", "other": "https://arxiv.org/format/2107.09996"}, "title": "MarsExplorer: Exploration of Unknown Terrains via Deep Reinforcement Learning and Procedurally Generated Environments", "author_info": ["Dimitrios I. Koutras", "Athanasios Ch. Kapoutsis", "Angelos A. Amanatiadis", "Elias B. Kosmatopoulos"], "summary": "This paper is an initial endeavor to bridge the gap between powerful Deep Reinforcement Learning methodologies and the problem of exploration/coverage of unknown terrains. Within this scope, MarsExplorer, an openai-gym compatible environment tailored to exploration/coverage of unknown areas, is presented. MarsExplorer translates the original robotics problem into a Reinforcement Learning setup that various off-the-shelf algorithms can tackle. Any learned policy can be straightforwardly applied to a robotic platform without an elaborate simulation model of the robot's dynamics to apply a different learning/adaptation phase. One of its core features is the controllable multi-dimensional procedural generation of terrains, which is the key for producing policies with strong generalization capabilities. Four different state-of-the-art RL algorithms (A3C, PPO, Rainbow, and SAC) are trained on the MarsExplorer environment, and a proper evaluation of their results compared to the average human-level performance is reported. In the follow-up experimental analysis, the effect of the multi-dimensional difficulty setting on the learning capabilities of the best-performing algorithm (PPO) is analyzed. A milestone result is the generation of an exploration policy that follows the Hilbert curve without providing this information to the environment or rewarding directly or indirectly Hilbert-curve-like trajectories. The experimental analysis is concluded by evaluating PPO learned policy algorithm side-by-side with frontier-based exploration strategies. A study on the performance curves revealed that PPO-based policy was capable of performing adaptive-to-the-unknown-terrain sweeping without leaving expensive-to-revisit areas uncovered, underlying the capability of RL-based methodologies to tackle exploration tasks efficiently. The source code can be found at: https://github.com/dimikout3/MarsExplorer.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2107.06638", "id": "2107.06638", "pdf": "https://arxiv.org/pdf/2107.06638", "other": "https://arxiv.org/format/2107.06638"}, "title": "Procedural Content Generation using Behavior Trees (PCGBT)", "author_info": ["Anurag Sarkar", "Seth Cooper"], "summary": "Behavior trees (BTs) are a popular method for modeling NPC and enemy AI behavior and have been widely used in commercial games. In this work, rather than use BTs to model game playing agents, we use them for modeling game design agents, defining behaviors as content generation tasks rather than in-game actions. Similar to how traditional BTs enable modeling behaviors in a modular and dynamic manner, BTs for PCG enable simple subtrees for generating parts of levels to be combined modularly to form complex trees for generating whole levels as well as generators that can dynamically vary the generated content. We refer to this approach as Procedural Content Generation using Behavior Trees, or PCGBT, and demonstrate it by using BTs to model generators for Super Mario Bros., Mega Man and Metroid levels as well as dungeon layouts and discuss several ways in which this paradigm could be applied and extended in the future.", "comment": " Comments: Accepted to EXAG 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.03206", "id": "2107.03206", "pdf": "https://arxiv.org/pdf/2107.03206", "other": "https://arxiv.org/format/2107.03206"}, "title": "A reconstruction procedure for near horizon extensive air showers based on radio signals", "author_info": ["Valentin Decoene", "Olivier Martineau-Huynh", "Mat\u00ecas Tueros", "Simon Chiche"], "summary": "Very inclined extensive air showers (EAS), with both down-going and up-going trajectories, are particularly targeted by the next generation of extended radio arrays, such as GRAND. Methods to reconstruct the incoming direction, core position, primary energy and composition of showers with these specific geometries, remain to be developed. Towards that goal, we present a new reconstruction procedure based on the arrival times and the amplitudes of the radio signal, measured at each antenna station. This hybrid reconstruction method, harnesses the fact that the emission is observed, at the antenna level, far away from the emission region, thus allowing for a point-like emission description. Thanks to this assumption, the arrival times are modelled following a spherical wavefront emission, which offers the possibility to reconstruct the radio emission zone as a fixed point along the shower axis. From that point the amplitude distribution at the antenna level is described through an Angular Distribution Function (ADF) taking into account at once all geo-magnetic asymmetries and early late effects as well as additional signal asymmetries featured by very inclined EAS. This method shows promising results in terms of arrival direction reconstruction, within the 0.1\u00b0 range, even when taking into account experimental uncertainties, and interesting potential for the energy reconstruction and primary composition identification.", "comment": " Journal ref:         Proceedings of Science (PoS), ICRC2021, 211, 2021       "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.01274", "id": "2107.01274", "pdf": "https://arxiv.org/pdf/2107.01274", "other": "https://arxiv.org/format/2107.01274"}, "title": "Unbiasing Procedures for Scale-invariant Multi-reference Alignment", "author_info": ["Matthew Hirn", "Anna Little"], "summary": "This article discusses a generalization of the 1-dimensional multi-reference alignment problem. The goal is to recover a hidden signal from many noisy observations, where each noisy observation includes a random translation and random dilation of the hidden signal, as well as high additive noise. We propose a method that recovers the power spectrum of the hidden signal by applying a data-driven, nonlinear unbiasing procedure, and thus the hidden signal is obtained up to an unknown phase. An unbiased estimator of the power spectrum is defined, whose error depends on the sample size and noise levels, and we precisely quantify the convergence rate of the proposed estimator. The unbiasing procedure relies on knowledge of the dilation distribution, and we implement an optimization procedure to learn the dilation variance when this parameter is unknown. Our theoretical work is supported by extensive numerical experiments on a wide range of signals.", "comment": " Comments: 12 pages, 5 figures. Code reproducing numerical results at https://bitbucket.org/annavlittle/inversion-unbiasing/src/master/ "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.00832", "id": "2107.00832", "pdf": "https://arxiv.org/pdf/2107.00832"}, "title": "Production and measuring methods and procedures in precision optical cavities production", "author_info": ["Jiri Benes", "Frantisek Prochaska", "Zdenek Rail", "David Tomka", "Lenka Pradova", "Ondrej Cip"], "summary": "This work presents the development of the production process of a Fabry Perot type laser resonator. It describes the acquired knowledge in the field of production of very precise optical standards, cavities, and lenses. In addition, it describes the measurement methods used in production. The work is intended for industry and science.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2107.00077", "id": "2107.00077", "pdf": "https://arxiv.org/pdf/2107.00077", "other": "https://arxiv.org/format/2107.00077"}, "title": "Learning to communicate about shared procedural abstractions", "author_info": ["William P. McCarthy", "Robert D. Hawkins", "Haoliang Wang", "Cameron Holdaway", "Judith E. Fan"], "summary": "Many real-world tasks require agents to coordinate their behavior to achieve shared goals. Successful collaboration requires not only adopting the same communicative conventions, but also grounding these conventions in the same task-appropriate conceptual abstractions. We investigate how humans use natural language to collaboratively solve physical assembly problems more effectively over time. Human participants were paired up in an online environment to reconstruct scenes containing two block towers. One participant could see the target towers, and sent assembly instructions for the other participant to reconstruct. Participants provided increasingly concise instructions across repeated attempts on each pair of towers, using higher-level referring expressions that captured each scene's hierarchical structure. To explain these findings, we extend recent probabilistic models of ad-hoc convention formation with an explicit perceptual learning mechanism. These results shed light on the inductive biases that enable intelligent agents to coordinate upon shared procedural abstractions.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2106.15439", "id": "2106.15439", "pdf": "https://arxiv.org/pdf/2106.15439", "other": "https://arxiv.org/format/2106.15439"}, "title": "Random Access Procedure over Non-Terrestrial Networks: From Theory to Practice", "author_info": ["O. Kodheli", "A. Astro", "J. Querol", "M. Gholamian", "S. Kumar", "N. Maturo", "S. Chatzinotas"], "summary": "Non-terrestrial Networks (NTNs) have become an appealing concept over the last few years and they are foreseen as a cornerstone for the next generations of mobile communication systems. Despite opening up new market opportunities and use cases for the future, the novel impairments caused by the signal propagation over the NTN channel compromises several procedures of the current cellular standards. One of the first and most important procedures impacted is the random access (RA) procedure, which is mainly utilized for achieving uplink synchronization among users in several standards, such as the fourth and fifth generation of mobile communication (4 & 5G) and narrowband internet of things (NB-IoT). In this work, we analyse the challenges imposed by the considerably increased delay in the communication link on the RA procedure and propose new solutions to overcome those challenges. A trade-off analysis of various solutions is provided taking into account also the already existing ones in the literature. In order to broaden the scope of applicability, we keep the analysis general targeting 4G, 5G and NB-IoT systems since the RA procedure is quasi-identical among these technologies. Last but not least, we go one step further and validate our techniques in an experimental setup, consisting of a user and a base station implemented in open air interface (OAI), and an NTN channel implemented in hardware that emulates the signal propagation delay. The laboratory test-bed built in this work, not only enables us to validate various solutions, but also plays a crucial role in identifying novel challenges not previously treated in the literature. Finally, an important key performance indicator (KPI) of the RA procedure over NTN is shown, which is the time that a single user requires to establish a connection with the base station.", "comment": " Comments: This work has been submitted to the IEEE Access for possible publication "}, {"arxiv": {"page": "https://arxiv.org/abs/2106.15309", "id": "2106.15309", "pdf": "https://arxiv.org/pdf/2106.15309", "other": "https://arxiv.org/format/2106.15309"}, "title": "Multimodal Semantic Scene Graphs for Holistic Modeling of Surgical Procedures", "author_info": ["Ege \u00d6zsoy", "Evin P\u0131nar \u00d6rnek", "Ulrich Eck", "Federico Tombari", "Nassir Navab"], "summary": "From a computer science viewpoint, a surgical domain model needs to be a conceptual one incorporating both behavior and data. It should therefore model actors, devices, tools, their complex interactions and data flow. To capture and model these, we take advantage of the latest computer vision methodologies for generating 3D scene graphs from camera views. We then introduce the Multimodal Semantic Scene Graph (MSSG) which aims at providing a unified symbolic, spatiotemporal and semantic representation of surgical procedures. This methodology aims at modeling the relationship between different components in surgical domain including medical staff, imaging systems, and surgical devices, opening the path towards holistic understanding and modeling of surgical procedures. We then use MSSG to introduce a dynamically generated graphical user interface tool for surgical procedure analysis which could be used for many applications including process optimization, OR design and automatic report generation. We finally demonstrate that the proposed MSSGs could also be used for synchronizing different complex surgical procedures. While the system still needs to be integrated into real operating rooms before getting validated, this conference paper aims mainly at providing the community with the basic principles of this novel concept through a first prototypal partial realization based on MVOR dataset.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2106.12844", "id": "2106.12844", "pdf": "https://arxiv.org/pdf/2106.12844", "other": "https://arxiv.org/format/2106.12844"}, "title": "Bootstrap confidence intervals for multiple change points based on moving sum procedures", "author_info": ["Haeran Cho", "Claudia Kirch"], "summary": "The problem of quantifying uncertainty about the locations of multiple change points by means of confidence intervals is addressed. The asymptotic distribution of the change point estimators obtained as the local maximisers of moving sum statistics is derived, where the limit distributions differ depending on whether the corresponding size of changes is local, i.e. tends to zero as the sample size increases, or fixed. A bootstrap procedure for confidence interval generation is proposed which adapts to the unknown magnitude of changes and guarantees asymptotic validity both for local and fixed changes. Simulation studies show good performance of the proposed bootstrap procedure, and some discussions about how it can be extended to serially dependent errors is provided.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2106.11962", "id": "2106.11962", "pdf": "https://arxiv.org/pdf/2106.11962", "other": "https://arxiv.org/format/2106.11962"}, "title": "Analysis of Executional and Procedural Errors in Dry-lab Robotic Surgery Experiments", "author_info": ["Kay Hutchinson", "Zongyu Li", "Leigh A. Cantrell", "Noah S. Schenkman", "Homa Alemzadeh"], "summary": "Background Analyzing kinematic and video data can help identify potentially erroneous motions that lead to sub-optimal surgeon performance and safety-critical events in robot-assisted surgery.", "comment": " Comments: 18 pages, 14 figures, 6 tables. Submitted to The International Journal of Medical Robotics and Computer Assisted Surgery (IJMRCAS). Code and supplementary video files are available at https://github.com/UVA-DSA/ExecProc_Error_Analysis "}, {"arxiv": {"page": "https://arxiv.org/abs/2106.07587", "id": "2106.07587", "pdf": "https://arxiv.org/pdf/2106.07587", "other": "https://arxiv.org/format/2106.07587"}, "title": "Limited-Information Maximum Likelihood based Model Selection Procedures for Binary Outcomes", "author_info": ["Shunichiro Orihara"], "summary": "Unmeasured covariates constitute one of the important problems in causal inference. Even if there are some unmeasured covariates, some instrumental variable methods such as a two-stage residual inclusion (2SRI) estimator, or a limited-information maximum likelihood (LIML) estimator can obtain an unbiased estimate for causal effects despite there being nonlinear outcomes such as binary outcomes; however, it requires that we specify not only a correct outcome model but also a correct treatment model. Therefore, detecting correct models is an important process. In this paper, we propose two model selection procedures: AIC-type and BIC-type, and confirm their properties. The proposed model selection procedures are based on a LIML estimator. We prove that a proposed BIC-type model selection procedure has model selection consistency, and confirm their properties of the proposed model selection procedures through simulation datasets.", "comment": " Comments: Keywords: Causal inference, Consistency, Limited-information maximum lilkelihood, Model selection, Two-stage residual inclusion, Unmeasured covariates "}, {"arxiv": {"page": "https://arxiv.org/abs/2106.03197", "id": "2106.03197", "pdf": "https://arxiv.org/pdf/2106.03197", "other": "https://arxiv.org/format/2106.03197"}, "title": "Robustness of Parameter Estimation Procedures for Bulk-Heterojunction Organic Solar Cells", "author_info": ["Alexis Prel", "Abir Rezgui", "Anne-Sophie Cordan", "Yann Leroy"], "summary": "Parameter estimation procedures provide valuable guidance in the understanding and improvement of organic solar cells and other devices. They often rely on one-dimensional models, but in the case of bulk-heterojunction (BHJ) designs, it is not straightforward that these models' parameters have a consistent physical interpretation. Indeed, contrarily to two- or three-dimensional models, the BHJ morphology is not explicitly described in one-dimensional models and must be implicitly expressed through effective parameters. In order to inform experimental decisions, a helpful parameter estimation method must establish that one can correctly interpret the provided parameters. However, only a few works have been undertaken to reach that objective in the context of BHJ organic solar cells. In this work, a realistic two-dimensional model of BHJ solar cells is used to investigate the behavior of state-of-the-art parameter estimation procedures in situations that emulate experimental conditions. We demonstrate that fitting solely current-voltage characteristics by an effective medium one-dimensional model can yield nonsensical results, which may lead to counter-productive decisions about future design choices. In agreement with previously published literature, we explicitly demonstrate that fitting several characterization results together can drastically improve the robustness of the parameter estimation. Based on a detailed analysis of parameter estimation results, a set of recommendations is formulated to avoid the most problematic pitfalls and increase awareness about the limitations that cannot be circumvented.", "comment": " Comments: 13 pages, 6 figures, 6 tables, 38 references "}, {"arxiv": {"page": "https://arxiv.org/abs/2106.03158", "id": "2106.03158", "pdf": "https://arxiv.org/pdf/2106.03158", "other": "https://arxiv.org/format/2106.03158"}, "title": "Learning Video Models from Text: Zero-Shot Anticipation for Procedural Actions", "author_info": ["Fadime Sener", "Rishabh Saraf", "Angela Yao"], "summary": "Can we teach a robot to recognize and make predictions for activities that it has never seen before? We tackle this problem by learning models for video from text. This paper presents a hierarchical model that generalizes instructional knowledge from large-scale text-corpora and transfers the knowledge to video. Given a portion of an instructional video, our model recognizes and predicts coherent and plausible actions multiple steps into the future, all in rich natural language. To demonstrate the capabilities of our model, we introduce the \\emph{Tasty Videos Dataset V2}, a collection of 4022 recipes for zero-shot learning, recognition and anticipation. Extensive experiments with various evaluation metrics demonstrate the potential of our method for generalization, given limited video data for training models.", "comment": " Comments: arXiv admin note: text overlap with arXiv:1812.02501 "}, {"arxiv": {"page": "https://arxiv.org/abs/2106.02585", "id": "2106.02585", "pdf": "https://arxiv.org/pdf/2106.02585", "other": "https://arxiv.org/format/2106.02585"}, "title": "A Procedural World Generation Framework for Systematic Evaluation of Continual Learning", "author_info": ["Timm Hess", "Martin Mundt", "Iuliia Pliushch", "Visvanathan Ramesh"], "summary": "Several families of continual learning techniques have been proposed to alleviate catastrophic interference in deep neural network training on non-stationary data. However, a comprehensive comparison and analysis of limitations remains largely open due to the inaccessibility to suitable datasets. Empirical examination not only varies immensely between individual works, it further currently relies on contrived composition of benchmarks through subdivision and concatenation of various prevalent static vision datasets. In this work, our goal is to bridge this gap by introducing a computer graphics simulation framework that repeatedly renders only upcoming urban scene fragments in an endless real-time procedural world generation process. At its core lies a modular parametric generative model with adaptable generative factors. The latter can be used to flexibly compose data streams, which significantly facilitates a detailed analysis and allows for effortless investigation of various continual learning schemes.", "comment": " Comments: Published in Neural Information Processing Systems, Dataset and Benchmarks Track 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2106.01254", "id": "2106.01254", "pdf": "https://arxiv.org/pdf/2106.01254", "other": "https://arxiv.org/format/2106.01254"}, "title": "Survey Equivalence: A Procedure for Measuring Classifier Accuracy Against Human Labels", "author_info": ["Paul Resnick", "Yuqing Kong", "Grant Schoenebeck", "Tim Weninger"], "summary": "In many classification tasks, the ground truth is either noisy or subjective. Examples include: which of two alternative paper titles is better? is this comment toxic? what is the political leaning of this news article? We refer to such tasks as survey settings because the ground truth is defined through a survey of one or more human raters. In survey settings, conventional measurements of classifier accuracy such as precision, recall, and cross-entropy confound the quality of the classifier with the level of agreement among human raters. Thus, they have no meaningful interpretation on their own. We describe a procedure that, given a dataset with predictions from a classifier and K ratings per item, rescales any accuracy measure into one that has an intuitive interpretation. The key insight is to score the classifier not against the best proxy for the ground truth, such as a majority vote of the raters, but against a single human rater at a time. That score can be compared to other predictors' scores, in particular predictors created by combining labels from several other human raters. The survey equivalence of any classifier is the minimum number of raters needed to produce the same expected score as that found for the classifier.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2105.14780", "id": "2105.14780", "pdf": "https://arxiv.org/pdf/2105.14780", "other": "https://arxiv.org/format/2105.14780"}, "title": "Procedural Content Generation: Better Benchmarks for Transfer Reinforcement Learning", "author_info": ["Matthias M\u00fcller-Brockhausen", "Mike Preuss", "Aske Plaat"], "summary": "The idea of transfer in reinforcement learning (TRL) is intriguing: being able to transfer knowledge from one problem to another problem without learning everything from scratch. This promises quicker learning and learning more complex methods. To gain an insight into the field and to detect emerging trends, we performed a database search. We note a surprisingly late adoption of deep learning that starts in 2018. The introduction of deep learning has not yet solved the greatest challenge of TRL: generalization. Transfer between different domains works well when domains have strong similarities (e.g. MountainCar to Cartpole), and most TRL publications focus on different tasks within the same domain that have few differences. Most TRL applications we encountered compare their improvements against self-defined baselines, and the field is still missing unified benchmarks. We consider this to be a disappointing situation. For the future, we note that: (1) A clear measure of task similarity is needed. (2) Generalization needs to improve. Promising approaches merge deep learning with planning via MCTS or introduce memory through LSTMs. (3) The lack of benchmarking tools will be remedied to enable meaningful comparison and measure progress. Already Alchemy and Meta-World are emerging as interesting benchmark suites. We note that another development, the increase in procedural content generation (PCG), can improve both benchmarking and generalization in TRL.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2105.14357", "id": "2105.14357", "pdf": "https://arxiv.org/pdf/2105.14357", "other": "https://arxiv.org/format/2105.14357"}, "title": "Constructing Flow Graphs from Procedural Cybersecurity Texts", "author_info": ["Kuntal Kumar Pal", "Kazuaki Kashihara", "Pratyay Banerjee", "Swaroop Mishra", "Ruoyu Wang", "Chitta Baral"], "summary": "Following procedural texts written in natural languages is challenging. We must read the whole text to identify the relevant information or identify the instruction flows to complete a task, which is prone to failures. If such texts are structured, we can readily visualize instruction-flows, reason or infer a particular step, or even build automated systems to help novice agents achieve a goal. However, this structure recovery task is a challenge because of such texts' diverse nature. This paper proposes to identify relevant information from such texts and generate information flows between sentences. We built a large annotated procedural text dataset (CTFW) in the cybersecurity domain (3154 documents). This dataset contains valuable instructions regarding software vulnerability analysis experiences. We performed extensive experiments on CTFW with our LM-GNN model variants in multiple settings. To show the generalizability of both this task and our method, we also experimented with procedural texts from two other domains (Maintenance Manual and Cooking), which are substantially different from cybersecurity. Our experiments show that Graph Convolution Network with BERT sentence embeddings outperforms BERT in all three domains", "comment": " Comments: 13 pages, 5 pages, accepted in the Findings of ACL 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2105.11236", "id": "2105.11236", "pdf": "https://arxiv.org/pdf/2105.11236", "other": "https://arxiv.org/format/2105.11236"}, "title": "Parallel adaptive procedure for CFD simulations", "author_info": ["Imad Kissami", "Souhail Maazioui", "Fayssal Benkhaldoun"], "summary": "The present paper describes a parallel unstructured-mesh Plasma simulation code based on Finite Volume method. The code dynamically refines and coarses mesh for accurate resolution of the different features regarding the electron density. Our purpose is to examine the performance of a new Parallel Adaptive Mesh Refinement (PAMR) procedure introduced on the ADAPT platform, which resolves of a relatively complicated system coupling the flow partial differential equations to the Poisson's equation. The implementation deals with the MUMPS parallel multi-frontal direct solver and mesh partitioning methods using METIS to improve the performance of the framework. The standard MPI is used to establish communication between processors. Performance analysis of the PAMR procedure shows the efficiency and the potential of the method for the propagation equations of ionization waves.", "comment": ""}]}