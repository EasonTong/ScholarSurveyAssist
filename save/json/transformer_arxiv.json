{"data": [{"arxiv": {"page": "https://arxiv.org/abs/2202.11031", "id": "2202.11031", "pdf": "https://arxiv.org/pdf/2202.11031", "other": "https://arxiv.org/format/2202.11031"}, "title": "A Unified Nonparametric Test of Transformations on Distribution Functions with Nuisance Parameters", "author_info": ["Xingyu Li", "Xiaojun Song", "Zhenting Sun"], "summary": "This paper proposes a simple unified approach to testing transformations on cumulative distribution functions (CDFs) with nuisance parameters. We consider testing general parametric transformations on two CDFs, and then generalize the test for multiple CDFs. We construct the test using a numerical bootstrap method which can easily be implemented. The proposed test is shown to be asymptotically size controlled and consistent. Monte Carlo simulations and an empirical application show that the test performs well on finite samples.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10931", "id": "2202.10931", "pdf": "https://arxiv.org/pdf/2202.10931", "other": "https://arxiv.org/format/2202.10931"}, "title": "Convergence Analysis of Structure-Preserving Numerical Methods Based on Slotboom Transformation for the Poisson--Nernst--Planck Equations", "author_info": ["Jie Ding", "Cheng Wang", "Shenggao Zhou"], "summary": "The analysis of structure-preserving numerical methods for the Poisson--Nernst--Planck (PNP) system has attracted growing interests in recent years. In this work, we provide an optimal rate convergence analysis and error estimate for finite difference schemes based on the Slotboom reformulation. Different options of mobility average at the staggered mesh points are considered in the finite-difference spatial discretization, such as the harmonic mean, geometric mean, arithmetic mean, and entropic mean. A semi-implicit temporal discretization is applied, which in turn results in a non-constant coefficient, positive-definite linear system at each time step. A higher order asymptotic expansion is applied in the consistency analysis, and such a higher order consistency estimate is necessary to control the discrete maximum norm of the concentration variables. In convergence estimate, the harmonic mean for the mobility average, which turns out to bring lots of convenience in the theoretical analysis, is taken for simplicity, while other options of mobility average would also lead to the desired error estimate, with more technical details involved. As a result, an optimal rate convergence analysis on concentrations, electric potential, and ionic fluxes is derived, which is the first such results for the structure-preserving numerical schemes based on the Slotboom reformulation. It is remarked that the convergence analysis leads to a theoretical justification of the conditional energy dissipation analysis, which relies on the maximum norm bounds of the concentration and the gradient of the electric potential. Some numerical results are also presented to demonstrate the accuracy and structure-preserving performance of the associated schemes.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10930", "id": "2202.10930", "pdf": "https://arxiv.org/pdf/2202.10930", "other": "https://arxiv.org/format/2202.10930"}, "title": "Transformation Coding: Simple Objectives for Equivariant Representations", "author_info": ["Mehran Shakerinava", "Arnab Kumar Mondal", "Siamak Ravanbakhsh"], "summary": "We present a simple non-generative approach to deep representation learning that seeks equivariant deep embedding through simple objectives. In contrast to existing equivariant networks, our transformation coding approach does not constrain the choice of the feed-forward layer or the architecture and allows for an unknown group action on the input space. We introduce several such transformation coding objectives for different Lie groups such as the Euclidean, Orthogonal and the Unitary groups. When using product groups, the representation is decomposed and disentangled. We show that the presence of additional information on different transformations improves disentanglement in transformation coding. We evaluate the representations learnt by transformation coding both qualitatively and quantitatively on downstream tasks, including reinforcement learning.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10774", "id": "2202.10774", "pdf": "https://arxiv.org/pdf/2202.10774"}, "title": "Social Computational Design Method for Generating Product Shapes with GAN and Transformer Models", "author_info": ["Maolin Yang", "Pingyu Jiang"], "summary": "A social computational design method is established, aiming at taking advantages of the fast-developing artificial intelligence technologies for intelligent product design. Supported with multi-agent system, shape grammar, Generative adversarial network, Bayesian network, Transformer, etc., the method is able to define the design solution space, prepare training samples, and eventually acquire an intelligent model that can recommend design solutions according to incomplete solutions for given design tasks. Product shape design is used as entry point to demonstrate the method, however, the method can be applied to tasks rather than shape design when the solutions can be properly coded.", "comment": " Comments: 6pages, 6 figures, conference paper "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10690", "id": "2202.10690", "pdf": "https://arxiv.org/pdf/2202.10690"}, "title": "An Energy-concentrated Wavelet Transform for Time Frequency Analysis of Transient Signals", "author_info": ["Haoran Dong", "Gang Yu"], "summary": "Transient signals are often composed of a series of modes that have multivalued time-dependent instantaneous frequency (IF), which brings challenges to the development of signal processing technology. Fortunately, the group delay (GD) of such signal can be well expressed as a single valued function of frequency. By considering the frequency-domain signal model, we present a postprocessing method called wavelet transform (WT)-based time-reassigned synchrosqueezing transform (WTSST). Our proposed method embeds a two-dimensional GD operator into a synchrosqueezing framework to generate a time-frequency representation (TFR) of transient signal with high energy concentration and allows to retrieve the whole or part of the signal. The theoretical analyses of the WTSST are provided, including the analysis of GD candidate accuracy and signal reconstruction accuracy. Moreover, based on WTSST, the WT-based time-reassigned multisynchrosqueezing transform (WTMSST) is proposed by introducing a stepwise refinement scheme, which further improves the drawback that the WTSST method is unable to deal with strong frequency-varying signal. Simulation and real signal analysis illustrate that the proposed methods have the capacity to appropriately describe the features of transient signals.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10581", "id": "2202.10581", "pdf": "https://arxiv.org/pdf/2202.10581", "other": "https://arxiv.org/format/2202.10581"}, "title": "Unleashing the Power of Transformer for Graphs", "author_info": ["Lingbing Guo", "Qiang Zhang", "Huajun Chen"], "summary": "Despite recent successes in natural language processing and computer vision, Transformer suffers from the scalability problem when dealing with graphs. The computational complexity is unacceptable for large-scale graphs, e.g., knowledge graphs. One solution is to consider only the near neighbors, which, however, will lose the key merit of Transformer to attend to the elements at any distance. In this paper, we propose a new Transformer architecture, named dual-encoding Transformer (DET). DET has a structural encoder to aggregate information from connected neighbors and a semantic encoder to focus on semantically useful distant nodes. In comparison with resorting to multi-hop neighbors, DET seeks the desired distant neighbors via self-supervised training. We further find these two encoders can be incorporated to boost each others' performance. Our experiments demonstrate DET has achieved superior performance compared to the respective state-of-the-art methods in dealing with molecules, networks and knowledge graphs with various sizes.", "comment": " Comments: 8 pages, under review "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10455", "id": "2202.10455", "pdf": "https://arxiv.org/pdf/2202.10455", "other": "https://arxiv.org/format/2202.10455"}, "title": "A Clustering Preserving Transformation for k-Means Algorithm Output", "author_info": ["Mieczys\u0142aw A. K\u0142opotek"], "summary": "This note introduces a novel clustering preserving transformation of cluster sets obtained from k-means algorithm. This transformation may be used to generate new labeled data{}sets from existent ones. It is more flexible that Kleinberg axiom based consistency transformation because data points in a cluster can be moved away and datapoints between clusters may come closer together.", "comment": " Comments: 11 pages, 5 figures. arXiv admin note: substantial text overlap with arXiv:2202.06015 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10447", "id": "2202.10447", "pdf": "https://arxiv.org/pdf/2202.10447", "other": "https://arxiv.org/format/2202.10447"}, "title": "Transformer Quality in Linear Time", "author_info": ["Weizhe Hua", "Zihang Dai", "Hanxiao Liu", "Quoc V. Le"], "summary": "We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head attention with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH, matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9\u00d7 on Wiki-40B and 12.1\u00d7 on PG-19 for auto-regressive language modeling, and 4.8\u00d7 on C4 for masked language modeling.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10394", "id": "2202.10394", "pdf": "https://arxiv.org/pdf/2202.10394", "other": "https://arxiv.org/format/2202.10394"}, "title": "On The Generalisation of Henstock-Kurzweil Fourier Transform", "author_info": ["S. Mahanta", "S. Ray"], "summary": "In this paper, a generalised integral called the Laplace integral is defined on unbounded intervals, and some of its properties, including necessary and sufficient condition for differentiating under the integral sign, are discussed. It is also shown that this integral is more general than the Henstock-Kurzweil integral. Finally, the Fourier transform is defined using the Laplace integral, and its well-known properties are established.", "comment": " MSC Class:           42A38; 26A39                          "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10280", "id": "2202.10280", "pdf": "https://arxiv.org/pdf/2202.10280", "other": "https://arxiv.org/format/2202.10280"}, "title": "On the Information-theoretic Security of Combinatorial All-or-nothing Transforms", "author_info": ["Yujie Gu", "Sonata Akao", "Navid Nasr Esfahani", "Ying Miao", "Kouichi Sakurai"], "summary": "All-or-nothing transforms (AONT) were proposed by Rivest as a message preprocessing technique for encrypting data to protect against brute-force attacks, and have numerous applications in cryptography and information security. Later the unconditionally secure AONT and their combinatorial characterization were introduced by Stinson. Informally, a combinatorial AONT is an array with the unbiased requirements and its security properties in general depend on the prior probability distribution on the inputs s-tuples. Recently, it was shown by Esfahani and Stinson that a combinatorial AONT has perfect security provided that all the inputs s-tuples are equiprobable, and has weak security provided that all the inputs s-tuples are with non-zero probability.", "comment": " Comments: 16 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10242", "id": "2202.10242", "pdf": "https://arxiv.org/pdf/2202.10242", "other": "https://arxiv.org/format/2202.10242"}, "title": "On The Low Speed Limits of Lorentz's Transformation", "author_info": ["Hao Chen", "Wei E. I. Sha", "Xi Dai", "Yue Yu"], "summary": "This article contains a digest of the theory of electromagnetism and a review of the transformation between inertial frames, especially under low speed limits. The covariant nature of the Maxwell's equations is explained using the conventional language. We show that even under low speed limits, the relativistic effects should not be neglected to get a self-consistent theory of the electromagnetic fields, unless the intrinsic dynamics of these fields has been omitted completely. The quasi-static limits, where the relativistic effects can be partly neglected are also reviewed, to clarify some common misunderstandings and imprecise use of the theory in presence of moving media and other related situations. The discussion presented in this paper provide a clear view of why classical electromagnetic theory is relativistic in its essence.", "comment": " Comments: 8+10 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10139", "id": "2202.10139", "pdf": "https://arxiv.org/pdf/2202.10139", "other": "https://arxiv.org/format/2202.10139"}, "title": "S3T: Self-Supervised Pre-training with Swin Transformer for Music Classification", "author_info": ["Hang Zhao", "Chen Zhang", "Belei Zhu", "Zejun Ma", "Kejun Zhang"], "summary": "In this paper, we propose S3T, a self-supervised pre-training method with Swin Transformer for music classification, aiming to learn meaningful music representations from massive easily accessible unlabeled music data. S3T introduces a momentum-based paradigm, MoCo, with Swin Transformer as its feature extractor to music time-frequency domain. For better music representations learning, S3T contributes a music data augmentation pipeline and two specially designed pre-processors. To our knowledge, S3T is the first method combining the Swin Transformer with a self-supervised learning method for music classification. We evaluate S3T on music genre classification and music tagging tasks with linear classifiers trained on learned representations. Experimental results show that S3T outperforms the previous self-supervised method (CLMR) by 12.5 percents top-1 accuracy and 4.8 percents PR-AUC on two tasks respectively, and also surpasses the task-specific state-of-the-art supervised methods. Besides, S3T shows advances in label efficiency using only 10% labeled data exceeding CLMR on both tasks with 100% labeled data.", "comment": " Comments: Accepted by ICASSP2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10108", "id": "2202.10108", "pdf": "https://arxiv.org/pdf/2202.10108", "other": "https://arxiv.org/format/2202.10108"}, "title": "ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond", "author_info": ["Qiming Zhang", "Yufei Xu", "Jing Zhang", "Dacheng Tao"], "summary": "Vision transformers have shown great potential in various computer vision tasks owing to their strong capability to model long-range dependency using the self-attention mechanism. Nevertheless, they treat an image as a 1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual structures and dealing with scale variance, which is instead learned implicitly from large-scale training data with longer training schedules. In this paper, we propose a Vision Transformer Advanced by Exploring intrinsic IB from convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid reduction modules to downsample and embed the input image into tokens with rich multi-scale context using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and can learn robust feature representation for objects at various scales. Moreover, in each transformer layer, ViTAE has a convolution block parallel to the multi-head self-attention module, whose features are fused and fed into the feed-forward network. Consequently, it has the intrinsic locality IB and is able to learn local features and global dependencies collaboratively. The proposed two kinds of cells are stacked in both isotropic and multi-stage manners to formulate two families of ViTAE models, i.e., the vanilla ViTAE and ViTAEv2. Experiments on the ImageNet dataset as well as downstream tasks on the MS COCO, ADE20K, and AP10K datasets validate the superiority of our models over the baseline transformer models and concurrent works. Besides, we scale up our ViTAE model to 644M parameters and obtain the state-of-the-art classification performance, i.e., 88.5% Top-1 classification accuracy on ImageNet validation set and the best 91.2% Top-1 accuracy on ImageNet real validation set, without using extra private data.", "comment": " Comments: An extended version of the Neurips paper \"ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias\". arXiv admin note: substantial text overlap with arXiv:2106.03348 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10101", "id": "2202.10101", "pdf": "https://arxiv.org/pdf/2202.10101", "other": "https://arxiv.org/format/2202.10101"}, "title": "BERT WEAVER: Using WEight AVERaging to Enable Lifelong Learning for Transformer-based Models", "author_info": ["Lisa Langnickel", "Alexander Schulz", "Barbara Hammer", "Juliane Fluck"], "summary": "Recent developments in transfer learning have boosted the advancements in natural language processing tasks. The performance is, however, dependent on high-quality, manually annotated training data. Especially in the biomedical domain, it has been shown that one training corpus is not enough to learn generic models that are able to efficiently predict on new data. Therefore, state-of-the-art models need the ability of lifelong learning in order to improve performance as soon as new data are available - without the need of retraining the whole model from scratch. We present WEAVER, a simple, yet efficient post-processing method that infuses old knowledge into the new model, thereby reducing catastrophic forgetting. We show that applying WEAVER in a sequential manner results in similar word embedding distributions as doing a combined training on all data at once, while being computationally more efficient. Because there is no need of data sharing, the presented method is also easily applicable to federated learning settings and can for example be beneficial for the mining of electronic health records from different clinics.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09979", "id": "2202.09979", "pdf": "https://arxiv.org/pdf/2202.09979", "other": "https://arxiv.org/format/2202.09979"}, "title": "Audio Visual Scene-Aware Dialog Generation with Transformer-based Video Representations", "author_info": ["Yoshihiro Yamazaki", "Shota Orihashi", "Ryo Masumura", "Mihiro Uchida", "Akihiko Takashima"], "summary": "There have been many attempts to build multimodal dialog systems that can respond to a question about given audio-visual information, and the representative task for such systems is the Audio Visual Scene-Aware Dialog (AVSD). Most conventional AVSD models adopt the Convolutional Neural Network (CNN)-based video feature extractor to understand visual information. While a CNN tends to obtain both temporally and spatially local information, global information is also crucial for boosting video understanding because AVSD requires long-term temporal visual dependency and whole visual information. In this study, we apply the Transformer-based video feature that can capture both temporally and spatially global representations more efficiently than the CNN-based feature. Our AVSD model with its Transformer-based feature attains higher objective performance scores for answer generation. In addition, our model achieves a subjective score close to that of human answers in DSTC10. We observed that the Transformer-based visual feature is beneficial for the AVSD task because our model tends to correctly answer the questions that need a temporally and spatially broad range of visual information.", "comment": " Comments: Accepted at DSTC10 Workshop at AAAI 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09925", "id": "2202.09925", "pdf": "https://arxiv.org/pdf/2202.09925", "other": "https://arxiv.org/format/2202.09925"}, "title": "Tuning the entanglement growth in matrix-product-state evolution of quantum systems by nonunitary similarity transformations", "author_info": ["Kai T. Liu", "Feng-feng Song", "Peng Zhang", "David N. Beratan"], "summary": "The possibility of using similarity transformations to alter dynamical entanglement growth in matrix-product-state simulations of quantum systems is explored. By appropriately choosing the similarity transformation, the entanglement growth rate is suppressed, improving the efficiency of numerical simulations of quantum systems. The transformation can be applied to general quantum-many-body systems.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09907", "id": "2202.09907", "pdf": "https://arxiv.org/pdf/2202.09907", "other": "https://arxiv.org/format/2202.09907"}, "title": "towards automatic transcription of polyphonic electric guitar music:a new dataset and a multi-loss transformer model", "author_info": ["Yu-Hua Chen", "Wen-Yi Hsiao", "Tsu-Kuang Hsieh", "Jyh-Shing Roger Jang", "Yi-Hsuan Yang"], "summary": "In this paper, we propose a new dataset named EGDB, that con-tains transcriptions of the electric guitar performance of 240 tab-latures rendered with different tones. Moreover, we benchmark theperformance of two well-known transcription models proposed orig-inally for the piano on this dataset, along with a multi-loss Trans-former model that we newly propose. Our evaluation on this datasetand a separate set of real-world recordings demonstrate the influenceof timbre on the accuracy of guitar sheet transcription, the potentialof using multiple losses for Transformers, as well as the room forfurther improvement for this task.", "comment": " Comments: to be published at ICASSP 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09789", "id": "2202.09789", "pdf": "https://arxiv.org/pdf/2202.09789", "other": "https://arxiv.org/format/2202.09789"}, "title": "SOTitle: A Transformer-based Post Title Generation Approach for Stack Overflow", "author_info": ["Ke Liu", "Guang Yang", "Xiang Chen", "Chi Yu"], "summary": "On Stack Overflow, developers can not only browse question posts to solve their programming problems but also gain expertise from the question posts to help improve their programming skills. Therefore, improving the quality of question posts in Stack Overflow has attracted the wide attention of researchers. A concise and precise title can play an important role in helping developers understand the key information of the question post, which can improve the post quality. However, the quality of the generated title is not high due to the lack of professional knowledge related to their questions or the poor presentation ability of developers. A previous study aimed to automatically generate the title by analyzing the code snippets in the question post. However, this study ignored the useful information in the corresponding problem description. Therefore, we propose an approach SOTitle for automatic post title generation by leveraging the code snippets and the problem description in the question post (i.e., the multi-modal input). SOTitle follows the Transformer structure, which can effectively capture long-term dependencies through a multi-head attention mechanism. To verify the effectiveness of SOTitle, we construct a large-scale high-quality corpus from Stack Overflow, which includes 1,168,257 high-quality question posts for four popular programming languages. Experimental results show that SOTitle can significantly outperform six state-of-the-art baselines in both automatic evaluation and human evaluation. To encourage follow-up studies, we make our corpus and approach publicly available", "comment": " Comments: Accepted in SANER 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09785", "id": "2202.09785", "pdf": "https://arxiv.org/pdf/2202.09785", "other": "https://arxiv.org/format/2202.09785"}, "title": "DualSC: Automatic Generation and Summarization of Shellcode via Transformer and Dual Learning", "author_info": ["Guang Yang", "Xiang Chen", "Yanlin Zhou", "Chi Yu"], "summary": "A shellcode is a small piece of code and it is executed to exploit a software vulnerability, which allows the target computer to execute arbitrary commands from the attacker through a code injection attack. Similar to the purpose of automated vulnerability generation techniques, the automated generation of shellcode can generate attack instructions, which can be used to detect vulnerabilities and implement defensive measures. While the automated summarization of shellcode can help users unfamiliar with shellcode and network information security understand the intent of shellcode attacks. In this study, we propose a novel approach DualSC to solve the automatic shellcode generation and summarization tasks. Specifically, we formalize automatic shellcode generation and summarization as dual tasks, use a shallow Transformer for model construction, and design a normalization method Adjust QKNorm to adapt these low-resource tasks (i.e., insufficient training data). Finally, to alleviate the out-of-vocabulary problem, we propose a rulebased repair component to improve the performance of automatic shellcode generation. In our empirical study, we select a highquality corpus Shellcode IA32 as our empirical subject. This corpus was gathered from two real-world projects based on the line-by-line granularity. We first compare DualSC with six state-of-the-art baselines from the code generation and code summarization domains in terms of four performance measures. The comparison results show the competitiveness of DualSC. Then, we verify the effectiveness of the component setting in DualSC. Finally, we conduct a human study to further verify the effectiveness of DualSC.", "comment": " Comments: Accepted in SANER 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09662", "id": "2202.09662", "pdf": "https://arxiv.org/pdf/2202.09662", "other": "https://arxiv.org/format/2202.09662"}, "title": "Reward Modeling for Mitigating Toxicity in Transformer-based Language Models", "author_info": ["Farshid Faal", "Jia Yuan Yu", "Ketra Schmitt"], "summary": "Transformer-based language models are able to generate fluent text and be efficiently adapted across various natural language generation tasks. However, language models that are pretrained on large unlabeled web text corpora have been shown to suffer from degenerating toxic content and social bias behaviors, consequently hindering their safe deployment. Various detoxification methods were proposed to mitigate the language model's toxicity; however, these methods struggled to detoxify language models when conditioned on prompts that contain specific social identities related to gender, race, or religion. In this study, we propose Reinforce-Detoxify; A reinforcement learning-based method for mitigating toxicity in language models. We address the challenge of safety in language models and propose a new reward model that is able to detect toxic content and mitigate unintended bias towards social identities in toxicity prediction. The experiments demonstrate that the Reinforce-Detoxify method for language model detoxification outperforms existing detoxification approaches in automatic evaluation metrics, indicating the ability of our approach in language model detoxification and less prone to unintended bias toward social identities in generated content.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09580", "id": "2202.09580", "pdf": "https://arxiv.org/pdf/2202.09580", "other": "https://arxiv.org/format/2202.09580"}, "title": "Image-to-Graph Transformers for Chemical Structure Recognition", "author_info": ["Sanghyun Yoo", "Ohyun Kwon", "Hoshik Lee"], "summary": "For several decades, chemical knowledge has been published in written text, and there have been many attempts to make it accessible, for example, by transforming such natural language text to a structured format. Although the discovered chemical itself commonly represented in an image is the most important part, the correct recognition of the molecular structure from the image in literature still remains a hard problem since they are often abbreviated to reduce the complexity and drawn in many different styles. In this paper, we present a deep learning model to extract molecular structures from images. The proposed model is designed to transform the molecular image directly into the corresponding graph, which makes it capable of handling non-atomic symbols for abbreviations. Also, by end-to-end learning approach it can fully utilize many open image-molecule pair data from various sources, and hence it is more robust to image style variation than other tools. The experimental results show that the proposed model outperforms the existing models with 17.1 % and 12.8 % relative improvement for well-known benchmark datasets and large molecular images that we collected from literature, respectively.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09507", "id": "2202.09507", "pdf": "https://arxiv.org/pdf/2202.09507", "other": "https://arxiv.org/format/2202.09507"}, "title": "PMP-Net++: Point Cloud Completion by Transformer-Enhanced Multi-step Point Moving Paths", "author_info": ["Xin Wen", "Peng Xiang", "Zhizhong Han", "Yan-Pei Cao", "Pengfei Wan", "Wen Zheng", "Yu-Shen Liu"], "summary": "Point cloud completion concerns to predict missing part for incomplete 3D shapes. A common strategy is to generate complete shape according to incomplete input. However, unordered nature of point clouds will degrade generation of high-quality 3D shapes, as detailed topology and structure of unordered points are hard to be captured during the generative process using an extracted latent code. We address this problem by formulating completion as point cloud deformation process. Specifically, we design a novel neural network, named PMP-Net++, to mimic behavior of an earth mover. It moves each point of incomplete input to obtain a complete point cloud, where total distance of point moving paths (PMPs) should be the shortest. Therefore, PMP-Net++ predicts unique PMP for each point according to constraint of point moving distances. The network learns a strict and unique correspondence on point-level, and thus improves quality of predicted complete shape. Moreover, since moving points heavily relies on per-point features learned by network, we further introduce a transformer-enhanced representation learning network, which significantly improves completion performance of PMP-Net++. We conduct comprehensive experiments in shape completion, and further explore application on point cloud up-sampling, which demonstrate non-trivial improvement of PMP-Net++ over state-of-the-art point cloud completion/up-sampling methods.", "comment": " Comments: 16 pages, 17 figures. Minor revision under TPAMI. Journel extension of CVPR 2021 paper PMP-Net(arXiv:2012.03408). arXiv admin note: substantial text overlap with arXiv:2012.03408 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09481", "id": "2202.09481", "pdf": "https://arxiv.org/pdf/2202.09481", "other": "https://arxiv.org/format/2202.09481"}, "title": "TransDreamer: Reinforcement Learning with Transformer World Models", "author_info": ["Chang Chen", "Yi-Fu Wu", "Jaesik Yoon", "Sungjin Ahn"], "summary": "The Dreamer agent provides various benefits of Model-Based Reinforcement Learning (MBRL) such as sample efficiency, reusable knowledge, and safe planning. However, its world model and policy networks inherit the limitations of recurrent neural networks and thus an important question is how an MBRL framework can benefit from the recent advances of transformers and what the challenges are in doing so. In this paper, we propose a transformer-based MBRL agent, called TransDreamer. We first introduce the Transformer State-Space Model, a world model that leverages a transformer for dynamics predictions. We then share this world model with a transformer-based policy network and obtain stability in training a transformer-based RL agent. In experiments, we apply the proposed model to 2D visual RL and 3D first-person visual RL tasks both requiring long-range memory access for memory-based reasoning. We show that the proposed model outperforms Dreamer in these complex tasks.", "comment": " Comments: Deep RL Workshop NeurIPS 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09428", "id": "2202.09428", "pdf": "https://arxiv.org/pdf/2202.09428"}, "title": "Microstructure and phase transformation of nickel-titanium shape memory alloy fabricated by directed energy deposition with in-situ heat treatment", "author_info": ["Shiming Gao", "Ojo Philip Bodunde", "Mian Qin", "Wei-Hsin Liao", "Ping Guo"], "summary": "Additive manufacturing has been vastly applied to fabricate various structures of nickel-titanium (NiTi) shape memory alloys due to its flexibility to create complex structures with minimal defects. However, the microstructure heterogeneity and secondary phase formation are two main problems that impede the further application of NiTi alloys. Although post-heat treatment is usually adopted to improve or manipulate NiTi alloy properties, it cannot realize the spatial control of thermal and/or mechanical properties of NiTi alloys. To overcome the limitations of uniform post-heat treatment, this study proposes an in-situ heat treatment strategy that is integrated into the directed energy deposition of NiTi alloys. The proposed method will potentially lead to new manufacturing capabilities to achieve location-dependent performance or property manipulation. The influences of in-situ heat treatment on the thermal and mechanical properties of printed NiTi structures were investigated. The investigations were carried out in terms of thermal cycling, microstructure evolution, and mechanical properties by 3D finite element simulations and experimental characterizations. A low-power laser beam was adopted to localize the in-situ heat treatment only to the current printed layer, facilitating a reverse peritectic reaction and a transient high solution treatment successively. The proposed in-situ heat treatment on the specimen results in a more obvious phase transformation peak in the differential scanning calorimetry curves, about 50%~70% volume reduction for the Ti2Ni phase, and approximately 35 HV reduction on microhardness.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09367", "id": "2202.09367", "pdf": "https://arxiv.org/pdf/2202.09367", "other": "https://arxiv.org/format/2202.09367"}, "title": "Snowflake Point Deconvolution for Point Cloud Completion and Generation with Skip-Transformer", "author_info": ["Peng Xiang", "Xin Wen", "Yu-Shen Liu", "Yan-Pei Cao", "Pengfei Wan", "Wen Zheng", "Zhizhong Han"], "summary": "Most existing point cloud completion methods suffered from discrete nature of point clouds and unstructured prediction of points in local regions, which makes it hard to reveal fine local geometric details. To resolve this issue, we propose SnowflakeNet with Snowflake Point Deconvolution (SPD) to generate the complete point clouds. SPD models the generation of complete point clouds as the snowflake-like growth of points, where the child points are progressively generated by splitting their parent points after each SPD. Our insight of revealing detailed geometry is to introduce skip-transformer in SPD to learn point splitting patterns which can fit local regions the best. Skip-transformer leverages attention mechanism to summarize the splitting patterns used in previous SPD layer to produce the splitting in current SPD layer. The locally compact and structured point clouds generated by SPD precisely reveal the structure characteristic of 3D shape in local patches, which enables us to predict highly detailed geometries. Moreover, since SPD is a general operation, which is not limited to completion, we further explore the applications of SPD on other generative tasks, including point cloud auto-encoding, generation, single image reconstruction and upsampling. Our experimental results outperform the state-of-the-art methods under widely used benchmarks.", "comment": " Comments: IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), Dec. 2021, under review. This work is a journal extension of our ICCV 2021 paper arXiv:2108.04444 . The first two authors contributed equally "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09206", "id": "2202.09206", "pdf": "https://arxiv.org/pdf/2202.09206", "other": "https://arxiv.org/format/2202.09206"}, "title": "Spatio-Temporal Outdoor Lighting Aggregation on Image Sequences using Transformer Networks", "author_info": ["Haebom Lee", "Christian Homeyer", "Robert Herzog", "Jan Rexilius", "Carsten Rother"], "summary": "In this work, we focus on outdoor lighting estimation by aggregating individual noisy estimates from images, exploiting the rich image information from wide-angle cameras and/or temporal image sequences. Photographs inherently encode information about the scene's lighting in the form of shading and shadows. Recovering the lighting is an inverse rendering problem and as that ill-posed. Recent work based on deep neural networks has shown promising results for single image lighting estimation, but suffers from robustness. We tackle this problem by combining lighting estimates from several image views sampled in the angular and temporal domain of an image sequence. For this task, we introduce a transformer architecture that is trained in an end-2-end fashion without any statistical post-processing as required by previous work. Thereby, we propose a positional encoding that takes into account the camera calibration and ego-motion estimation to globally register the individual estimates when computing attention between visual words. We show that our method leads to improved lighting estimation while requiring less hyper-parameters compared to the state-of-the-art.", "comment": " Comments: 11 pages, 7 figures, 1 table, currently under a review process "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09144", "id": "2202.09144", "pdf": "https://arxiv.org/pdf/2202.09144", "other": "https://arxiv.org/format/2202.09144"}, "title": "Modelling the semantics of text in complex document layouts using graph transformer networks", "author_info": ["Thomas Roland Barillot", "Jacob Saks", "Polena Lilyanova", "Edward Torgas", "Yachen Hu", "Yuanqing Liu", "Varun Balupuri", "Paul Gaskell"], "summary": "Representing structured text from complex documents typically calls for different machine learning techniques, such as language models for paragraphs and convolutional neural networks (CNNs) for table extraction, which prohibits drawing links between text spans from different content types. In this article we propose a model that approximates the human reading pattern of a document and outputs a unique semantic representation for every text span irrespective of the content type they are found in. We base our architecture on a graph representation of the structured text, and we demonstrate that not only can we retrieve semantically similar information across documents but also that the embedding space we generate captures useful semantic information, similar to language models that work only on text sequences.", "comment": " MSC Class:           I.2.7                          "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09124", "id": "2202.09124", "pdf": "https://arxiv.org/pdf/2202.09124", "other": "https://arxiv.org/format/2202.09124"}, "title": "Multi-view and Multi-modal Event Detection Utilizing Transformer-based Multi-sensor fusion", "author_info": ["Masahiro Yasuda", "Yasunori Ohishi", "Shoichiro Saito", "Noboru Harada"], "summary": "We tackle a challenging task: multi-view and multi-modal event detection that detects events in a wide-range real environment by utilizing data from distributed cameras and microphones and their weak labels. In this task, distributed sensors are utilized complementarily to capture events that are difficult to capture with a single sensor, such as a series of actions of people moving in an intricate room, or communication between people located far apart in a room. For sensors to cooperate effectively in such a situation, the system should be able to exchange information among sensors and combines information that is useful for identifying events in a complementary manner. For such a mechanism, we propose a Transformer-based multi-sensor fusion (MultiTrans) which combines multi-sensor data on the basis of the relationships between features of different viewpoints and modalities. In the experiments using a dataset newly collected for this task, our proposed method using MultiTrans improved the event detection performance and outperformed comparatives.", "comment": " Comments: 5 pages, 5 figures, to appear in IEEE ICASSP 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08945", "id": "2202.08945", "pdf": "https://arxiv.org/pdf/2202.08945"}, "title": "Direct Measurement of Storage and Loss Behavior in AFM Force-Distance Experiments Using the Modified Fourier Transformation", "author_info": ["Berkin Uluutku", "Marshall Richards McCraw", "Santiago D. Solares"], "summary": "Force-distance curve experiments are commonly performed in Atomic Force Microscopy (AFM) to obtain the viscoelastic characteristics of materials, such as the storage and loss moduli or compliances. The classic methods used to obtain these characteristics consist of fitting a viscoelastic material model to the experimentally obtained AFM data. Here, we demonstrate a new method that utilizes the modified discrete Fourier transform to approximate the storage and loss behavior of a material directly from the data, without the need for a fit. Additionally, one may still fit a model to the resulting storage and loss behavior if a parameterized description of the material is desired. In contrast to fitting the data to a model chosen a priori, departing from a model-free description of the material's frequency behavior guides the selection of the model, such that the user may choose the one that is most appropriate for the particular material under study. To this end, we also include modified Fourier domain descriptions of commonly used viscoelastic models.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08708", "id": "2202.08708", "pdf": "https://arxiv.org/pdf/2202.08708", "other": "https://arxiv.org/format/2202.08708"}, "title": "Learning stochastic dynamics and predicting emergent behavior using transformers", "author_info": ["Corneel Casert", "Isaac Tamblyn", "Stephen Whitelam"], "summary": "We show that a neural network originally designed for language processing can learn the dynamical rules of a stochastic system by observation of a single dynamical trajectory of the system, and can accurately predict its emergent behavior under conditions not observed during training. We consider a lattice model of active matter undergoing continuous-time Monte Carlo dynamics, simulated at a density at which its steady state comprises small, dispersed clusters. We train a neural network called a transformer on a single trajectory of the model. The transformer, which we show has the capacity to represent dynamical rules that are numerous and nonlocal, learns that the dynamics of this model consists of a small number of processes. Forward-propagated trajectories of the trained transformer, at densities not encountered during training, exhibit motility-induced phase separation and so predict the existence of a nonequilibrium phase transition. Transformers have the flexibility to learn dynamical rules from observation without explicit enumeration of rates or coarse-graining of configuration space, and so the procedure used here can be applied to a wide range of physical systems, including those with large and complex dynamical generators.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08514", "id": "2202.08514", "pdf": "https://arxiv.org/pdf/2202.08514", "other": "https://arxiv.org/format/2202.08514"}, "title": "Survey on Self-supervised Representation Learning Using Image Transformations", "author_info": ["Muhammad Ali", "Sayed Hashim"], "summary": "Deep neural networks need huge amount of training data, while in real world there is a scarcity of data available for training purposes. To resolve these issues, self-supervised learning (SSL) methods are used. SSL using geometric transformations (GT) is a simple yet powerful technique used in unsupervised representation learning. Although multiple survey papers have reviewed SSL techniques, there is none that only focuses on those that use geometric transformations. Furthermore, such methods have not been covered in depth in papers where they are reviewed. Our motivation to present this work is that geometric transformations have shown to be powerful supervisory signals in unsupervised representation learning. Moreover, many such works have found tremendous success, but have not gained much attention. We present a concise survey of SSL approaches that use geometric transformations. We shortlist six representative models that use image transformations including those based on predicting and autoencoding transformations. We review their architecture as well as learning methodologies. We also compare the performance of these models in the object recognition task on CIFAR-10 and ImageNet datasets. Our analysis indicates the AETv2 performs the best in most settings. Rotation with feature decoupling also performed well in some settings. We then derive insights from the observed results. Finally, we conclude with a summary of the results and insights as well as highlighting open problems to be addressed and indicating various future directions.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08510", "id": "2202.08510", "pdf": "https://arxiv.org/pdf/2202.08510", "other": "https://arxiv.org/format/2202.08510"}, "title": "A hybrid 2-stage vision transformer for AI-assisted 5 class pathologic diagnosis of gastric endoscopic biopsies", "author_info": ["Yujin Oh", "Go Eun Bae", "Kyung-Hee Kim", "Min-Kyung Yeo", "Jong Chul Ye"], "summary": "Gastric endoscopic screening is an effective way to decide appropriate gastric cancer (GC) treatment at an early stage, reducing GC-associated mortality rate. Although artificial intelligence (AI) has brought a great promise to assist pathologist to screen digitalized whole slide images, automatic classification systems for guiding proper GC treatment based on clinical guideline are still lacking. Here, we propose an AI system classifying 5 classes of GC histology, which can be perfectly matched to general treatment guidance. The AI system, mimicking the way pathologist understand slides through multi-scale self-attention mechanism using a 2-stage Vision Transformer, demonstrates clinical capability by achieving diagnostic sensitivity of above 85% for both internal and external cohort analysis. Furthermore, AI-assisted pathologists showed significantly improved diagnostic sensitivity by 10% within 18% saved screening time compared to human pathologists. Our AI system has a great potential for providing presumptive pathologic opinion for deciding proper treatment for early GC patients.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08455", "id": "2202.08455", "pdf": "https://arxiv.org/pdf/2202.08455", "other": "https://arxiv.org/format/2202.08455"}, "title": "Transformer for Graphs: An Overview from Architecture Perspective", "author_info": ["Erxue Min", "Runfa Chen", "Yatao Bian", "Tingyang Xu", "Kangfei Zhao", "Wenbing Huang", "Peilin Zhao", "Junzhou Huang", "Sophia Ananiadou", "Yu Rong"], "summary": "Recently, Transformer model, which has achieved great success in many artificial intelligence fields, has demonstrated its great potential in modeling graph-structured data. Till now, a great variety of Transformers has been proposed to adapt to the graph-structured data. However, a comprehensive literature review and systematical evaluation of these Transformer variants for graphs are still unavailable. It's imperative to sort out the existing Transformer models for graphs and systematically investigate their effectiveness on various graph tasks. In this survey, we provide a comprehensive review of various Graph Transformer models from the architectural design perspective. We first disassemble the existing models and conclude three typical ways to incorporate the graph information into the vanilla Transformer: 1) GNNs as Auxiliary Modules, 2) Improved Positional Embedding from Graphs, and 3) Improved Attention Matrix from Graphs. Furthermore, we implement the representative components in three groups and conduct a comprehensive comparison on various kinds of famous graph data benchmarks to investigate the real performance gain of each component. Our experiments confirm the benefits of current graph-specific modules on Transformer and reveal their advantages on different kinds of graph tasks.", "comment": " Comments: 8 pages, 1 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08453", "id": "2202.08453", "pdf": "https://arxiv.org/pdf/2202.08453", "other": "https://arxiv.org/format/2202.08453"}, "title": "TraSeTR: Track-to-Segment Transformer with Contrastive Query for Instance-level Instrument Segmentation in Robotic Surgery", "author_info": ["Zixu Zhao", "Yueming Jin", "Pheng-Ann Heng"], "summary": "Surgical instrument segmentation -- in general a pixel classification task -- is fundamentally crucial for promoting cognitive intelligence in robot-assisted surgery (RAS). However, previous methods are struggling with discriminating instrument types and instances. To address the above issues, we explore a mask classification paradigm that produces per-segment predictions. We propose TraSeTR, a novel Track-to-Segment Transformer that wisely exploits tracking cues to assist surgical instrument segmentation. TraSeTR jointly reasons about the instrument type, location, and identity with instance-level predictions i.e., a set of class-bbox-mask pairs, by decoding query embeddings. Specifically, we introduce the prior query that encoded with previous temporal knowledge, to transfer tracking signals to current instances via identity matching. A contrastive query learning strategy is further applied to reshape the query feature space, which greatly alleviates the tracking difficulty caused by large temporal variations. The effectiveness of our method is demonstrated with state-of-the-art instrument type segmentation results on three public datasets, including two RAS benchmarks from EndoVis Challenges and one cataract surgery dataset CaDIS.", "comment": " Comments: Accepted by ICRA 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08446", "id": "2202.08446", "pdf": "https://arxiv.org/pdf/2202.08446", "other": "https://arxiv.org/format/2202.08446"}, "title": "MeNTT: A Compact and Efficient Processing-in-Memory Number Theoretic Transform (NTT) Accelerator", "author_info": ["Dai Li", "Akhil Pakala", "Kaiyuan Yang"], "summary": "Lattice-based cryptography (LBC) exploiting Learning with Errors (LWE) problems is a promising candidate for post-quantum cryptography. Number theoretic transform (NTT) is the latency- and energy- dominant process in the computation of LWE problems. This paper presents a compact and efficient in-MEmory NTT accelerator, named MeNTT, which explores optimized computation in and near a 6T SRAM array. Specifically-designed peripherals enable fast and efficient modular operations. Moreover, a novel mapping strategy reduces the data flow between NTT stages into a unique pattern, which greatly simplifies the routing among processing units (i.e., SRAM column in this work), reducing energy and area overheads. The accelerator achieves significant latency and energy reductions over prior arts.", "comment": " Comments: This paper has been accepted to IEEE Transactions on Very Large Scale Integration (TVLSI) "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08203", "id": "2202.08203", "pdf": "https://arxiv.org/pdf/2202.08203", "other": "https://arxiv.org/format/2202.08203"}, "title": "Fast inverse transform sampling of non-Gaussian distribution functions in space plasmas", "author_info": ["Xin An", "Anton Artemyev", "Vassilis Angelopoulos", "San Lu", "Philip Pritchett", "Viktor Decyk"], "summary": "Non-Gaussian distributions are commonly observed in collisionless space plasmas. Generating samples from non-Gaussian distributions is critical for the initialization of particle-in-cell simulations that investigate their driven and undriven dynamics. To this end, we report a computationally efficient, robust tool, Chebsampling, to sample general distribution functions in one and two dimensions. This tool is based on inverse transform sampling with function approximation by Chebyshev polynomials. We demonstrate practical uses of Chebsampling through sampling typical distribution functions in space plasmas.", "comment": " Comments: 15 pages, 4 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07959", "id": "2202.07959", "pdf": "https://arxiv.org/pdf/2202.07959", "other": "https://arxiv.org/format/2202.07959"}, "title": "EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation", "author_info": ["Tao Ge", "Furu Wei"], "summary": "We propose EdgeFormer -- a parameter-efficient Transformer of the encoder-decoder architecture for on-device seq2seq generation, which is customized under the strict computation and memory constraints. EdgeFormer proposes two novel principles for cost-effective parameterization and further enhance the model with efficient layer adaptation. We conduct extensive experiments on two practical on-device seq2seq tasks: Machine Translation and Grammatical Error Correction, and show that EdgeFormer can effectively outperform previous parameter-efficient Transformer baselines and achieve very competitive results with knowledge distillation under both the computation and memory constraints.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07931", "id": "2202.07931", "pdf": "https://arxiv.org/pdf/2202.07931", "other": "https://arxiv.org/format/2202.07931"}, "title": "DBT-Net: Dual-branch federative magnitude and phase estimation with attention-in-attention transformer for monaural speech enhancement", "author_info": ["Guochen Yu", "Andong Li", "Hui Wang", "Yutian Wang", "Yuxuan Ke", "Chengshi Zheng"], "summary": "The decoupling-style concept begins to ignite in the speech enhancement area, which decouples the original complex spectrum estimation task into multiple easier sub-tasks (i.e., magnitude and phase), resulting in better performance and easier interpretability. In this paper, we propose a dual-branch federative magnitude and phase estimation framework, dubbed DBT-Net, for monaural speech enhancement, which aims at recovering the coarse- and fine-grained regions of the overall spectrum in parallel. From the complementary perspective, the magnitude estimation branch is designed to filter out dominant noise components in the magnitude domain, while the complex spectrum purification branch is elaborately designed to inpaint the missing spectral details and implicitly estimate the phase information in the complex domain. To facilitate the information flow between each branch, interaction modules are introduced to leverage features learned from one branch, so as to suppress the undesired parts and recover the missing components of the other branch. Instead of adopting the conventional RNNs and temporal convolutional networks for sequence modeling, we propose a novel attention-in-attention transformer-based network within each branch for better feature learning. More specially, it is composed of several adaptive spectro-temporal attention transformer-based modules and an adaptive hierarchical attention module, aiming to capture long-term time-frequency dependencies and further aggregate intermediate hierarchical contextual information. Comprehensive evaluations on the WSJ0-SI84 + DNS-Challenge and VoiceBank + DEMAND dataset demonstrate that the proposed approach consistently outperforms previous advanced systems and yields state-of-the-art performance in terms of speech quality and intelligibility.", "comment": " Comments: 13 pages;submitted to TASLP "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07925", "id": "2202.07925", "pdf": "https://arxiv.org/pdf/2202.07925", "other": "https://arxiv.org/format/2202.07925"}, "title": "ActionFormer: Localizing Moments of Actions with Transformers", "author_info": ["Chenlin Zhang", "Jianxin Wu", "Yin Li"], "summary": "Self-attention based Transformer models have demonstrated impressive results for image classification and object detection, and more recently for video understanding. Inspired by this success, we investigate the application of Transformer networks for temporal action localization in videos. To this end, we present ActionFormer -- a simple yet powerful model to identify actions in time and recognize their categories in a single shot, without using action proposals or relying on pre-defined anchor windows. ActionFormer combines a multiscale feature representation with local self-attention, and uses a light-weighted decoder to classify every moment in time and estimate the corresponding action boundaries. We show that this orchestrated design results in major improvements upon prior works. Without bells and whistles, ActionFormer achieves 65.6% mAP at tIoU=0.5 on THUMOS14, outperforming the best prior model by 8.7 absolute percentage points and crossing the 60% mAP for the first time. Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.0% average mAP) and the more recent EPIC-Kitchens 100 (+13.5% average mAP over prior works). Our code is available at http://github.com/happyharrycn/actionformer_release", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07856", "id": "2202.07856", "pdf": "https://arxiv.org/pdf/2202.07856", "other": "https://arxiv.org/format/2202.07856"}, "title": "The NLP Task Effectiveness of Long-Range Transformers", "author_info": ["Guanghui Qin", "Yukun Feng", "Benjamin Van Durme"], "summary": "Transformer models cannot easily scale to long sequences due to their O(N^2) time and space complexity. This has led to Transformer variants seeking to lessen computational complexity, such as Longformer and Performer. While such models have theoretically greater efficiency, their effectiveness on real NLP tasks has not been well studied. We benchmark 7 variants of Transformer models on 5 difficult NLP tasks and 7 datasets. We design experiments to isolate the effect of pretraining and hyperparameter settings, to focus on their capacity for long-range attention. Moreover, we present various methods to investigate attention behaviors, to illuminate model details beyond metric scores. We find that attention of long-range transformers has advantages on content selection and query-guided decoding, but they come with previously unrecognized drawbacks such as insufficient attention to distant tokens.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07829", "id": "2202.07829", "pdf": "https://arxiv.org/pdf/2202.07829", "other": "https://arxiv.org/format/2202.07829"}, "title": "Spatial Transformer K-Means", "author_info": ["Romain Cosentino", "Randall Balestriero", "Yanis Bahroun", "Anirvan Sengupta", "Richard Baraniuk", "Behnaam Aazhang"], "summary": "K-means defines one of the most employed centroid-based clustering algorithms with performances tied to the data's embedding. Intricate data embeddings have been designed to push K-means performances at the cost of reduced theoretical guarantees and interpretability of the results. Instead, we propose preserving the intrinsic data space and augment K-means with a similarity measure invariant to non-rigid transformations. This enables (i) the reduction of intrinsic nuisances associated with the data, reducing the complexity of the clustering task and increasing performances and producing state-of-the-art results, (ii) clustering in the input space of the data, leading to a fully interpretable clustering algorithm, and (iii) the benefit of convergence guarantees.", "comment": " Comments: arXiv admin note: substantial text overlap with arXiv:2012.09743 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07824", "id": "2202.07824", "pdf": "https://arxiv.org/pdf/2202.07824", "other": "https://arxiv.org/format/2202.07824"}, "title": "RNGDet: Road Network Graph Detection by Transformer in Aerial Images", "author_info": ["Zhenhua Xu", "Yuxuan Liu", "Lu Gan", "Yuxiang Sun", "Ming Liu", "Lujia Wang"], "summary": "Road network graphs provide critical information for autonomous vehicle applications, such as motion planning on drivable areas. However, manually annotating road network graphs is inefficient and labor-intensive. Automatically detecting road network graphs could alleviate this issue, but existing works are either segmentation-based approaches that could not ensure satisfactory topology correctness, or graph-based approaches that could not present precise enough detection results. To provide a solution to these problems, we propose a novel approach based on transformer and imitation learning named RNGDet (\\underline{R}oad \\underline{N}etwork \\underline{G}raph \\underline{Det}ection by Transformer) in this paper. In view of that high-resolution aerial images could be easily accessed all over the world nowadays, we make use of aerial images in our approach. Taken as input an aerial image, our approach iteratively generates road network graphs vertex-by-vertex. Our approach can handle complicated intersection points of various numbers of road segments. We evaluate our approach on a publicly available dataset. The superiority of our approach is demonstrated through the comparative experiments.", "comment": " Comments: Under review "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07800", "id": "2202.07800", "pdf": "https://arxiv.org/pdf/2202.07800", "other": "https://arxiv.org/format/2202.07800"}, "title": "Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations", "author_info": ["Youwei Liang", "Chongjian Ge", "Zhan Tong", "Yibing Song", "Jue Wang", "Pengtao Xie"], "summary": "Vision Transformers (ViTs) take all the image patches as tokens and construct multi-head self-attention (MHSA) among them. Complete leverage of these image tokens brings redundant computations since not all the tokens are attentive in MHSA. Examples include that tokens containing semantically meaningless or distractive image backgrounds do not positively contribute to the ViT predictions. In this work, we propose to reorganize image tokens during the feed-forward process of ViT models, which is integrated into ViT during training. For each forward inference, we identify the attentive image tokens between MHSA and FFN (i.e., feed-forward network) modules, which is guided by the corresponding class token attention. Then, we reorganize image tokens by preserving attentive image tokens and fusing inattentive ones to expedite subsequent MHSA and FFN computations. To this end, our method EViT improves ViTs from two perspectives. First, under the same amount of input image tokens, our method reduces MHSA and FFN computation for efficient inference. For instance, the inference speed of DeiT-S is increased by 50% while its recognition accuracy is decreased by only 0.3% for ImageNet classification. Second, by maintaining the same computational cost, our method empowers ViTs to take more image tokens as input for recognition accuracy improvement, where the image tokens are from higher resolution images. An example is that we improve the recognition accuracy of DeiT-S by 1% for ImageNet classification at the same computational cost of a vanilla DeiT-S. Meanwhile, our method does not introduce more parameters to ViTs. Experiments on the standard benchmarks show the effectiveness of our method. The code is available at https://github.com/youweiliang/evit", "comment": " Comments: ICLR 2022 Spotlight "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07570", "id": "2202.07570", "pdf": "https://arxiv.org/pdf/2202.07570", "other": "https://arxiv.org/format/2202.07570"}, "title": "ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification", "author_info": ["Thomas Stegm\u00fcller", "Antoine Spahr", "Behzad Bozorgtabar", "Jean-Philippe Thiran"], "summary": "Progress in digital pathology is hindered by high-resolution images and the prohibitive cost of exhaustive localized annotations. The commonly used paradigm to categorize pathology images is patch-based processing, which often incorporates multiple instance learning (MIL) to aggregate local patch-level representations yielding image-level prediction. Nonetheless, diagnostically relevant regions may only take a small fraction of the whole tissue, and MIL-based aggregation operation assumes that all patch representations are independent and thus mislays the contextual information from adjacent cell and tissue microenvironments. Consequently, the computational resources dedicated to a specific region are independent of its information contribution. This paper proposes a transformer-based architecture specifically tailored for histopathological image classification, which combines fine-grained local attention with a coarse global attention mechanism to learn meaningful representations of high-resolution images at an efficient computational cost. More importantly, based on the observation above, we propose a novel mixing-based data-augmentation strategy, namely ScoreMix, by leveraging the distribution of the semantic regions of images during the training and carefully guiding the data mixing via sampling the locations of discriminative image content. Thorough experiments and ablation studies on three challenging representative cohorts of Haematoxylin & Eosin (H&E) tumour regions-of-interest (TRoIs) datasets have validated the superiority of our approach over existing state-of-the-art methods and effectiveness of our proposed components, e.g., data augmentation in improving classification performance. We also demonstrate our method's interpretability, robustness, and cross-domain generalization capability.", "comment": " Comments: 17 pages, 7 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07552", "id": "2202.07552", "pdf": "https://arxiv.org/pdf/2202.07552", "other": "https://arxiv.org/format/2202.07552"}, "title": "A Theory of PAC Learnability under Transformation Invariances", "author_info": ["Han Shao", "Omar Montasser", "Avrim Blum"], "summary": "Transformation invariances are present in many real-world problems. For example, image classification is usually invariant to rotation and color transformation: a rotated car in a different color is still identified as a car. Data augmentation, which adds the transformed data into the training set and trains a model on the augmented data, is one commonly used technique to build these invariances into the learning process. However, it is unclear how data augmentation performs theoretically and what the optimal algorithm is in presence of transformation invariances. In this paper, we study PAC learnability under transformation invariances in three settings according to different levels of realizability: (i) A hypothesis fits the augmented data; (ii) A hypothesis fits only the original data and the transformed data lying in the support of the data distribution; (iii) Agnostic case. One interesting observation is that distinguishing between the original data and the transformed data is necessary to achieve optimal accuracy in setting (ii) and (iii), which implies that any algorithm not differentiating between the original and transformed data (including data augmentation) is not optimal. Furthermore, this type of algorithms can even \"harm\" the accuracy. In setting (i), although it is unnecessary to distinguish between the two data sets, data augmentation still does not perform optimally. Due to such a difference, we propose two combinatorial measures characterizing the optimal sample complexity in setting (i) and (ii)(iii) and provide the optimal algorithms.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07543", "id": "2202.07543", "pdf": "https://arxiv.org/pdf/2202.07543", "other": "https://arxiv.org/format/2202.07543"}, "title": "BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer", "author_info": ["Ana-Maria Bucur", "Adrian Cosma", "Ioan-Bogdan Iordache"], "summary": "Memes are prevalent on the internet and continue to grow and evolve alongside our culture. An automatic understanding of memes propagating on the internet can shed light on the general sentiment and cultural attitudes of people. In this work, we present team BLUE's solution for the second edition of the MEMOTION competition. We showcase two approaches for meme classification (i.e. sentiment, humour, offensive, sarcasm and motivation levels) using a text-only method using BERT, and a Multi-Modal-Multi-Task transformer network that operates on both the meme image and its caption to output the final scores. In both approaches, we leverage state-of-the-art pretrained models for text (BERT, Sentence Transformer) and image processing (EfficientNetV4, CLIP). Through our efforts, we obtain first place in task A, second place in task B and third place in task C. In addition, our team obtained the highest average score for all three tasks.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07455", "id": "2202.07455", "pdf": "https://arxiv.org/pdf/2202.07455"}, "title": "Transforming agrifood production systems and supply chains with digital twins", "author_info": ["Asaf Tzachor", "Catherine E. Richards", "Scott Jeen"], "summary": "Digital twins can transform agricultural production systems and supply chains, curbing greenhouse gas emissions, food waste and malnutrition. However, the potential of these advanced virtualization technologies is yet to be realized. Here, we consider the promise of digital twins across five typical agrifood supply chain steps and emphasize key implementation barriers.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07305", "id": "2202.07305", "pdf": "https://arxiv.org/pdf/2202.07305", "other": "https://arxiv.org/format/2202.07305"}, "title": "ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer", "author_info": ["Kohei Uehara", "Yusuke Mori", "Yusuke Mukuta", "Tatsuya Harada"], "summary": "Image narrative generation describes the creation of stories regarding the content of image data from a subjective viewpoint. Given the importance of the subjective feelings of writers, characters, and readers in storytelling, image narrative generation methods must consider human emotion, which is their major difference from descriptive caption generation tasks. The development of automated methods to generate story-like text associated with images may be considered to be of considerable social significance, because stories serve essential functions both as entertainment and also for many practical purposes such as education and advertising. In this study, we propose a model called ViNTER (Visual Narrative Transformer with Emotion arc Representation) to generate image narratives that focus on time series representing varying emotions as \"emotion arcs,\" to take advantage of recent advances in multimodal Transformer-based pre-trained models. We present experimental results of both manual and automatic evaluations, which demonstrate the effectiveness of the proposed emotion-aware approach to image narrative generation.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07304", "id": "2202.07304", "pdf": "https://arxiv.org/pdf/2202.07304", "other": "https://arxiv.org/format/2202.07304"}, "title": "XAI for Transformers: Better Explanations through Conservative Propagation", "author_info": ["Ameen Ali", "Thomas Schnake", "Oliver Eberle", "Gr\u00e9goire Montavon", "Klaus-Robert M\u00fcller", "Lior Wolf"], "summary": "Transformers have become an important workhorse of machine learning, with numerous applications. This necessitates the development of reliable methods for increasing their transparency. Multiple interpretability methods, often based on gradient information, have been proposed. We show that the gradient in a Transformer reflects the function only locally, and thus fails to reliably identify the contribution of input features to the prediction. We identify Attention Heads and LayerNorm as main reasons for such unreliable explanations and propose a more stable way for propagation through these layers. Our proposal, which can be seen as a proper extension of the well-established LRP method to Transformers, is shown both theoretically and empirically to overcome the deficiency of a simple gradient-based approach, and achieves state-of-the-art explanation performance on a broad range of Transformer models and datasets.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07208", "id": "2202.07208", "pdf": "https://arxiv.org/pdf/2202.07208", "other": "https://arxiv.org/format/2202.07208"}, "title": "Time Domain Simulation of DFIG-Based Wind Power System using Differential Transform Method", "author_info": ["Pradeep Singh", "Upasana Buragohain", "Nilanjan Senroy"], "summary": "This paper proposes a new non-iterative time-domain simulation approach using Differential Transform Method (DTM) to solve the set of non-linear Differential-Algebraic Equations (DAEs) involved in a DFIG-based wind power system. The DTM is an analytical as well as numerical approach applied to solve high dimensional non-linear dynamical systems and the solution can be expressed in the form of a series. In this approach, there is no need to compute higher-order derivatives as DAEs are converted into a set of linear equations after applying transformation rules so that the power series coefficients can be computed directly. The transformation rules are used to transform power system models of various devices, such as induction generator, wind turbine, rotor and grid side converter, which includes trigonometric, square root, exponential functions etc. Further, to increase the interval of convergence for the series solutions, the multi-step DTM (MsDTM) approach is used. The numerical performance of the proposed approach is compared with the traditional numerical RK-4 method to demonstrate the potential of the proposed approach in solving power system non-linear DAEs", "comment": " Comments: 10 pages, 11 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07154", "id": "2202.07154", "pdf": "https://arxiv.org/pdf/2202.07154", "other": "https://arxiv.org/format/2202.07154"}, "title": "Optimal range of Haar martingale transforms and its applications", "author_info": ["Sergey Astashkin", "Jinghao Huang", "Marat Pliev", "Fedor Sukochev", "Dmitriy Zanin"], "summary": "Let (Fn)n\u22650 be the standard dyadic filtration on [0,1]. Let EFn be the conditional expectation from L1=L1[0,1] onto Fn, n\u22650, and let EF\u22121=0. We present the sharp estimate for the distribution function of the martingale transform T defined by Tf=\u2211m=0\u221e(EF2mf\u2212EF2m\u22121f),\u00a0f\u2208L1, in terms of the classical Calder\u00f3n operator. As an application, for a given symmetric function space E on [0,1], we identify the symmetric space SE, the optimal Banach symmetric range of martingale transforms/Haar basis projections acting on E.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07125", "id": "2202.07125", "pdf": "https://arxiv.org/pdf/2202.07125", "other": "https://arxiv.org/format/2202.07125"}, "title": "Transformers in Time Series: A Survey", "author_info": ["Qingsong Wen", "Tian Zhou", "Chaoli Zhang", "Weiqi Chen", "Ziqing Ma", "Junchi Yan", "Liang Sun"], "summary": "Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also intrigues great interests in the time series community. Among multiple advantages of transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review transformer schemes for time series modeling by highlighting their strengths as well as limitations through a new taxonomy to summarize existing time series transformers in two perspectives. From the perspective of network modifications, we summarize the adaptations of module level and architecture level of the time series transformers. From the perspective of applications, we categorize time series transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance. To the best of our knowledge, this paper is the first work to comprehensively and systematically summarize the recent advances of Transformers for modeling time series data. We hope this survey will ignite further research interests in time series Transformers.", "comment": " Comments: 8 pages, 1 figure, 4 tables, 65 referred papers "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07055", "id": "2202.07055", "pdf": "https://arxiv.org/pdf/2202.07055", "other": "https://arxiv.org/format/2202.07055"}, "title": "Fourier-Stieltjes transform defined by induced representation on locally compact groups", "author_info": ["Y. I. Akakpo", "K. Assiamoua", "K. Enakoutsa", "M. N. Hounkonnou"], "summary": "In this work we extend the Fourier-Stieltjes transform of a vector measure and a continuous function defined on compact groups to locally compact groups. To do so, we consider a representation L of a normal compact subgroup K of a locally compact group G, and we use a representation of G induced by that of L. Then, we define the Fourier-Stieltjes transform of a vector measure and that of a continuous function with compact support defined on G from the representation of G. Then, we extend the Shur orthogonality relation established for compact groups to locally compact groups by using the representations of G induced by the unitary representations of one of its normal compact subgroups. This extension enables us to develop a Fourier-Stieltjes transform in series form that is linear, continuous, and invertible.", "comment": " Comments: 15 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07001", "id": "2202.07001", "pdf": "https://arxiv.org/pdf/2202.07001", "other": "https://arxiv.org/format/2202.07001"}, "title": "Handcrafted Histological Transformer (H2T): Unsupervised Representation of Whole Slide Images", "author_info": ["Quoc Dang Vu", "Kashif Rajpoot", "Shan E Ahmed Raza", "Nasir Rajpoot"], "summary": "Diagnostic, prognostic and therapeutic decision-making of cancer in pathology clinics can now be carried out based on analysis of multi-gigapixel tissue images, also known as whole-slide images (WSIs). Recently, deep convolutional neural networks (CNNs) have been proposed to derive unsupervised WSI representations; these are attractive as they rely less on expert annotation which is cumbersome. However, a major trade-off is that higher predictive power generally comes at the cost of interpretability, posing a challenge to their clinical use where transparency in decision-making is generally expected. To address this challenge, we present a handcrafted framework based on deep CNN for constructing holistic WSI-level representations. Building on recent findings about the internal working of the Transformer in the domain of natural language processing, we break down its processes and handcraft them into a more transparent framework that we term as the Handcrafted Histological Transformer or H2T. Based on our experiments involving various datasets consisting of a total of 5,306 WSIs, the results demonstrate that H2T based holistic WSI-level representations offer competitive performance compared to recent state-of-the-art methods and can be readily utilized for various downstream analysis tasks. Finally, our results demonstrate that the H2T framework can be up to 14 times faster than the Transformer models.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06991", "id": "2202.06991", "pdf": "https://arxiv.org/pdf/2202.06991", "other": "https://arxiv.org/format/2202.06991"}, "title": "Transformer Memory as a Differentiable Search Index", "author_info": ["Yi Tay", "Vinh Q. Tran", "Mostafa Dehghani", "Jianmo Ni", "Dara Bahri", "Harsh Mehta", "Zhen Qin", "Kai Hui", "Zhe Zhao", "Jai Gupta", "Tal Schuster", "William W. Cohen", "Donald Metzler"], "summary": "In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06817", "id": "2202.06817", "pdf": "https://arxiv.org/pdf/2202.06817", "other": "https://arxiv.org/format/2202.06817"}, "title": "CATs++: Boosting Cost Aggregation with Convolutions and Transformers", "author_info": ["Seokju Cho", "Sunghwan Hong", "Seungryong Kim"], "summary": "Cost aggregation is a highly important process in image matching tasks, which aims to disambiguate the noisy matching scores. Existing methods generally tackle this by hand-crafted or CNN-based methods, which either lack robustness to severe deformations or inherit the limitation of CNNs that fail to discriminate incorrect matches due to limited receptive fields and inadaptability. In this paper, we introduce Cost Aggregation with Transformers (CATs) to tackle this by exploring global consensus among initial correlation map with the help of some architectural designs that allow us to fully enjoy global receptive fields of self-attention mechanism. Also, to alleviate some of the limitations that CATs may face, i.e., high computational costs induced by the use of a standard transformer that its complexity grows with the size of spatial and feature dimensions, which restrict its applicability only at limited resolution and result in rather limited performance, we propose CATs++, an extension of CATs. Our proposed methods outperform the previous state-of-the-art methods by large margins, setting a new state-of-the-art for all the benchmarks, including PF-WILLOW, PF-PASCAL, and SPair-71k. We further provide extensive ablation studies and analyses.", "comment": " Comments: https://sunghwanhong.github.io/CATs/. arXiv admin note: text overlap with arXiv:2106.02520 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06709", "id": "2202.06709", "pdf": "https://arxiv.org/pdf/2202.06709", "other": "https://arxiv.org/format/2202.06709"}, "title": "How Do Vision Transformers Work?", "author_info": ["Namuk Park", "Songkuk Kim"], "summary": "The success of multi-head self-attentions (MSAs) for computer vision is now indisputable. However, little is known about how MSAs work. We present fundamental explanations to help better understand the nature of MSAs. In particular, we demonstrate the following properties of MSAs and Vision Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization by flattening the loss landscapes. Such improvement is primarily attributable to their data specificity, not long-range dependency. On the other hand, ViTs suffer from non-convex losses. Large datasets and loss landscape smoothing methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors. For example, MSAs are low-pass filters, but Convs are high-pass filters. Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks behave like a series connection of small individual models. In addition, MSAs at the end of a stage play a key role in prediction. Based on these insights, we propose AlterNet, a model in which Conv blocks at the end of a stage are replaced with MSA blocks. AlterNet outperforms CNNs not only in large data regimes but also in small data regimes. The code is available at https://github.com/xxxnell/how-do-vits-work.", "comment": " Comments: ICLR 2022 (Spotlight) "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06688", "id": "2202.06688", "pdf": "https://arxiv.org/pdf/2202.06688", "other": "https://arxiv.org/format/2202.06688"}, "title": "Geometric Transformer for Fast and Robust Point Cloud Registration", "author_info": ["Zheng Qin", "Hao Yu", "Changjian Wang", "Yulan Guo", "Yuxing Peng", "Kai Xu"], "summary": "We study the problem of extracting accurate correspondences for point cloud registration. Recent keypoint-free methods bypass the detection of repeatable keypoints which is difficult in low-overlap scenarios, showing great potential in registration. They seek correspondences over downsampled superpoints, which are then propagated to dense points. Superpoints are matched based on whether their neighboring patches overlap. Such sparse and loose matching requires contextual features capturing the geometric structure of the point clouds. We propose Geometric Transformer to learn geometric feature for robust superpoint matching. It encodes pair-wise distances and triplet-wise angles, making it robust in low-overlap cases and invariant to rigid transformation. The simplistic design attains surprisingly high matching accuracy such that no RANSAC is required in the estimation of alignment transformation, leading to 100 times acceleration. Our method improves the inlier ratio by 17\\%\u223c30\\% and the registration recall by over 7\\% on the challenging 3DLoMatch benchmark. The code and models will be released at \\url{https://github.com/qinzheng93/GeoTransformer}.", "comment": " Comments: 19 pages, 11 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06521", "id": "2202.06521", "pdf": "https://arxiv.org/pdf/2202.06521", "other": "https://arxiv.org/format/2202.06521"}, "title": "Source Code Summarization with Structural Relative Position Guided Transformer", "author_info": ["Zi Gong", "Cuiyun Gao", "Yasheng Wang", "Wenchao Gu", "Yun Peng", "Zenglin Xu"], "summary": "Source code summarization aims at generating concise and clear natural language descriptions for programming languages. Well-written code summaries are beneficial for programmers to participate in the software development and maintenance process. To learn the semantic representations of source code, recent efforts focus on incorporating the syntax structure of code into neural networks such as Transformer. Such Transformer-based approaches can better capture the long-range dependencies than other neural networks including Recurrent Neural Networks (RNNs), however, most of them do not consider the structural relative correlations between tokens, e.g., relative positions in Abstract Syntax Trees (ASTs), which is beneficial for code semantics learning. To model the structural dependency, we propose a Structural Relative Position guided Transformer, named SCRIPT. SCRIPT first obtains the structural relative positions between tokens via parsing the ASTs of source code, and then passes them into two types of Transformer encoders. One Transformer directly adjusts the input according to the structural relative distance; and the other Transformer encodes the structural relative positions during computing the self-attention scores. Finally, we stack these two types of Transformer encoders to learn representations of source code. Experimental results show that the proposed SCRIPT outperforms the state-of-the-art methods by at least 1.6%, 1.4% and 2.8% with respect to BLEU, ROUGE-L and METEOR on benchmark datasets, respectively. We further show that how the proposed SCRIPT captures the structural relative dependencies.", "comment": " Comments: 12 pages, SANER 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06498", "id": "2202.06498", "pdf": "https://arxiv.org/pdf/2202.06498", "other": "https://arxiv.org/format/2202.06498"}, "title": "Task-Adaptive Feature Transformer with Semantic Enrichment for Few-Shot Segmentation", "author_info": ["Jun Seo", "Young-Hyun Park", "Sung Whan Yoon", "Jaekyun Moon"], "summary": "Few-shot learning allows machines to classify novel classes using only a few labeled samples. Recently, few-shot segmentation aiming at semantic segmentation on low sample data has also seen great interest. In this paper, we propose a learnable module that can be placed on top of existing segmentation networks for performing few-shot segmentation. This module, called the task-adaptive feature transformer (TAFT), linearly transforms task-specific high-level features to a set of task agnostic features well-suited to conducting few-shot segmentation. The task-conditioned feature transformation allows an effective utilization of the semantic information in novel classes to generate tight segmentation masks. We also propose a semantic enrichment (SE) module that utilizes a pixel-wise attention module for high-level feature and an auxiliary loss from an auxiliary segmentation network conducting the semantic segmentation for all training classes. Experiments on PASCAL-5i and COCO-20i datasets confirm that the added modules successfully extend the capability of existing segmentators to yield highly competitive few-shot segmentation performances.", "comment": " Comments: 8 pages, 7 figures. arXiv admin note: text overlap with arXiv:2010.11437 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06444", "id": "2202.06444", "pdf": "https://arxiv.org/pdf/2202.06444", "other": "https://arxiv.org/format/2202.06444"}, "title": "Nonlinear Optical Joint Transform Correlator for Low Latency Convolution Operations", "author_info": ["Jonathan K. George", "Maria Solyanik-Gorgone", "Hangbo Yang", "Chee Wei Wong", "Volker J. Sorger"], "summary": "Convolutions are one of the most relevant operations in artificial intelligence (AI) systems. High computational complexity scaling poses significant challenges, especially in fast-responding network-edge AI applications. Fortunately, the convolution theorem can be executed on-the-fly in the optical domain via a joint transform correlator (JTC) offering to fundamentally reduce the computational complexity. Nonetheless, the iterative two-step process of a classical JTC renders them unpractical. Here we introduce a novel implementation of an optical convolution-processor capable of near-zero latency by utilizing all-optical nonlinearity inside a JTC, thus minimizing electronic signal or conversion delay. Fundamentally we show how this nonlinear auto-correlator enables reducing the high O(n4) scaling complexity of processing two-dimensional data to only O(n2). Moreover, this optical JTC processes millions of channels in time-parallel, ideal for large-matrix machine learning tasks. Exemplary utilizing the nonlinear process of four-wave mixing, we show light processing performing a full convolution that is temporally limited only by geometric features of the lens and the nonlinear material's response time. We further discuss that the all-optical nonlinearity exhibits gain in excess of >103 when enhanced by slow-light effects such as epsilon-near-zero. Such novel implementation for a machine learning accelerator featuring low-latency and non-iterative massive data parallelism enabled by fundamental reduced complexity scaling bears significant promise for network-edge, and cloud AI systems.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06431", "id": "2202.06431", "pdf": "https://arxiv.org/pdf/2202.06431", "other": "https://arxiv.org/format/2202.06431"}, "title": "AI can evolve without labels: self-evolving vision transformer for chest X-ray diagnosis through knowledge distillation", "author_info": ["Sangjoon Park", "Gwanghyun Kim", "Yujin Oh", "Joon Beom Seo", "Sang Min Lee", "Jin Hwan Kim", "Sungjun Moon", "Jae-Kwang Lim", "Chang Min Park", "Jong Chul Ye"], "summary": "Although deep learning-based computer-aided diagnosis systems have recently achieved expert-level performance, developing a robust deep learning model requires large, high-quality data with manual annotation, which is expensive to obtain. This situation poses the problem that the chest x-rays collected annually in hospitals cannot be used due to the lack of manual labeling by experts, especially in deprived areas. To address this, here we present a novel deep learning framework that uses knowledge distillation through self-supervised learning and self-training, which shows that the performance of the original model trained with a small number of labels can be gradually improved with more unlabeled data. Experimental results show that the proposed framework maintains impressive robustness against a real-world environment and has general applicability to several diagnostic tasks such as tuberculosis, pneumothorax, and COVID-19. Notably, we demonstrated that our model performs even better than those trained with the same amount of labeled data. The proposed framework has a great potential for medical imaging, where plenty of data is accumulated every year, but ground truth annotations are expensive to obtain.", "comment": " Comments: 24 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06397", "id": "2202.06397", "pdf": "https://arxiv.org/pdf/2202.06397", "other": "https://arxiv.org/format/2202.06397"}, "title": "Transformer-based Approaches for Legal Text Processing", "author_info": ["Ha-Thanh Nguyen", "Minh-Phuong Nguyen", "Thi-Hai-Yen Vuong", "Minh-Quan Bui", "Minh-Chau Nguyen", "Tran-Binh Dang", "Vu Tran", "Le-Minh Nguyen", "Ken Satoh"], "summary": "In this paper, we introduce our approaches using Transformer-based models for different problems of the COLIEE 2021 automatic legal text processing competition. Automated processing of legal documents is a challenging task because of the characteristics of legal documents as well as the limitation of the amount of data. With our detailed experiments, we found that Transformer-based pretrained language models can perform well with automated legal text processing problems with appropriate approaches. We describe in detail the processing steps for each task such as problem formulation, data processing and augmentation, pretraining, finetuning. In addition, we introduce to the community two pretrained models that take advantage of parallel translations in legal domain, NFSP and NMSP. In which, NFSP achieves the state-of-the-art result in Task 5 of the competition. Although the paper focuses on technical reporting, the novelty of its approaches can also be an useful reference in automated legal document processing using Transformer-based models.", "comment": " Comments: arXiv admin note: substantial text overlap with arXiv:2106.13405 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06387", "id": "2202.06387", "pdf": "https://arxiv.org/pdf/2202.06387", "other": "https://arxiv.org/format/2202.06387"}, "title": "Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments", "author_info": ["Maor Ivgi", "Yair Carmon", "Jonathan Berant"], "summary": "Neural scaling laws define a predictable relationship between a model's parameter count and its performance after training in the form of a power law. However, most research to date has not explicitly investigated whether scaling laws can be used to accelerate model development. In this work, we perform such an empirical investigation across a wide range of language understanding tasks, starting from models with as few as 10K parameters, and evaluate downstream performance across 9 language understanding tasks. We find that scaling laws emerge at finetuning time in some NLP tasks, and that they can also be exploited for debugging convergence when training large models. Moreover, for tasks where scaling laws exist, they can be used to predict the performance of larger models, which enables effective model selection. However, revealing scaling laws requires careful hyperparameter tuning and multiple runs for the purpose of uncertainty estimation, which incurs additional overhead, partially offsetting the computational benefits.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06335", "id": "2202.06335", "pdf": "https://arxiv.org/pdf/2202.06335", "other": "https://arxiv.org/format/2202.06335"}, "title": "ET-BERT: A Contextualized Datagram Representation with Pre-training Transformers for Encrypted Traffic Classification", "author_info": ["Xinjie Lin", "Gang Xiong", "Gaopeng Gou", "Zhen Li", "Junzheng Shi", "Jing Yu"], "summary": "Encrypted traffic classification requires discriminative and robust traffic representation captured from content-invisible and imbalanced traffic data for accurate classification, which is challenging but indispensable to achieve network security and network management. The major limitation of existing solutions is that they highly rely on the deep features, which are overly dependent on data size and hard to generalize on unseen data. How to leverage the open-domain unlabeled traffic data to learn representation with strong generalization ability remains a key challenge. In this paper,we propose a new traffic representation model called Encrypted Traffic Bidirectional Encoder Representations from Transformer (ET-BERT), which pre-trains deep contextualized datagram-level representation from large-scale unlabeled data. The pre-trained model can be fine-tuned on a small number of task-specific labeled data and achieves state-of-the-art performance across five encrypted traffic classification tasks, remarkably pushing the F1 of ISCX-Tor to 99.2% (4.4% absolute improvement), ISCX-VPN-Service to 98.9% (5.2% absolute improvement), Cross-Platform (Android) to 92.5% (5.4% absolute improvement), CSTNET-TLS 1.3 to 97.4% (10.0% absolute improvement). Notably, we provide explanation of the empirically powerful pre-training model by analyzing the randomness of ciphers. It gives us insights in understanding the boundary of classification ability over encrypted traffic. The code is available at: https://github.com/linwhitehat/ET-BERT.", "comment": " Comments: This work has been accepted in Security, Privacy, and Trust track at The Web Conference 2022 (WWW'22)(see https://www2022.thewebconf.org/cfp/research/security/) "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06268", "id": "2202.06268", "pdf": "https://arxiv.org/pdf/2202.06268", "other": "https://arxiv.org/format/2202.06268"}, "title": "BViT: Broad Attention based Vision Transformer", "author_info": ["Nannan Li", "Yaran Chen", "Weifan Li", "Zixiang Ding", "Dongbin Zhao"], "summary": "Recent works have demonstrated that transformer can achieve promising performance in computer vision, by exploiting the relationship among image patches with self-attention. While they only consider the attention in a single feature layer, but ignore the complementarity of attention in different levels. In this paper, we propose the broad attention to improve the performance by incorporating the attention relationship of different layers for vision transformer, which is called BViT. The broad attention is implemented by broad connection and parameter-free attention. Broad connection of each transformer layer promotes the transmission and integration of information for BViT. Without introducing additional trainable parameters, parameter-free attention jointly focuses on the already available attention information in different layers for extracting useful information and building their relationship. Experiments on image classification tasks demonstrate that BViT delivers state-of-the-art accuracy of 74.8\\%/81.6\\% top-1 accuracy on ImageNet with 5M/22M parameters. Moreover, we transfer BViT to downstream object recognition benchmarks to achieve 98.9\\% and 89.9\\% on CIFAR10 and CIFAR100 respectively that exceed ViT with fewer parameters. For the generalization test, the broad attention in Swin Transformer and T2T-ViT also bring an improvement of more than 1\\%. To sum up, broad attention is promising to promote the performance of attention based models. Code and pre-trained models are available at https://github.com/DRL-CASIA/Broad_ViT.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06263", "id": "2202.06263", "pdf": "https://arxiv.org/pdf/2202.06263", "other": "https://arxiv.org/format/2202.06263"}, "title": "LighTN: Light-weight Transformer Network for Performance-overhead Tradeoff in Point Cloud Downsampling", "author_info": ["Xu Wang", "Yi Jin", "Yigang Cen", "Tao Wang", "Bowen Tang", "Yidong Li"], "summary": "Compared with traditional task-irrelevant downsampling methods, task-oriented neural networks have shown improved performance in point cloud downsampling range. Recently, Transformer family of networks has shown a more powerful learning capacity in visual tasks. However, Transformer-based architectures potentially consume too many resources which are usually worthless for low overhead task networks in downsampling range. This paper proposes a novel light-weight Transformer network (LighTN) for task-oriented point cloud downsampling, as an end-to-end and plug-and-play solution. In LighTN, a single-head self-correlation module is presented to extract refined global contextual features, where three projection matrices are simultaneously eliminated to save resource overhead, and the output of symmetric matrix satisfies the permutation invariant. Then, we design a novel downsampling loss function to guide LighTN focuses on critical point cloud regions with more uniform distribution and prominent points coverage. Furthermore, We introduce a feed-forward network scaling mechanism to enhance the learnable capacity of LighTN according to the expand-reduce strategy. The result of extensive experiments on classification and registration tasks demonstrates LighTN can achieve state-of-the-art performance with limited resource overhead.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06258", "id": "2202.06258", "pdf": "https://arxiv.org/pdf/2202.06258", "other": "https://arxiv.org/format/2202.06258"}, "title": "Flowformer: Linearizing Transformers with Conservation Flows", "author_info": ["Haixu Wu", "Jialong Wu", "Jiehui Xu", "Jianmin Wang", "Mingsheng Long"], "summary": "Transformers based on the attention mechanism have achieved impressive success in various areas. However, the attention mechanism has a quadratic complexity, significantly impeding Transformers from dealing with numerous tokens and scaling up to bigger models. Previous methods mainly utilize the similarity decomposition and the associativity of matrix multiplication to devise linear-time attention mechanisms. They avoid degeneration of attention to a trivial distribution by reintroducing inductive biases such as the locality, thereby at the expense of model generality and expressiveness. In this paper, we linearize Transformers free from specific inductive biases based on the flow network theory. We cast attention as the information flow aggregated from the sources (values) to the sinks (results) through the learned flow capacities (attentions). Within this framework, we apply the property of flow conservation with attention and propose the Flow-Attention mechanism of linear complexity. By respectively conserving the incoming flow of sinks for source competition and the outgoing flow of sources for sink allocation, Flow-Attention inherently generates informative attentions without using specific inductive biases. Empowered by the Flow-Attention, Flowformer yields strong performance in linear time for wide areas, including long sequence, time series, vision, natural language, and reinforcement learning.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06076", "id": "2202.06076", "pdf": "https://arxiv.org/pdf/2202.06076", "other": "https://arxiv.org/format/2202.06076"}, "title": "Indication as Prior Knowledge for Multimodal Disease Classification in Chest Radiographs with Transformers", "author_info": ["Grzegorz Jacenk\u00f3w", "Alison Q. O'Neil", "Sotirios A. Tsaftaris"], "summary": "When a clinician refers a patient for an imaging exam, they include the reason (e.g. relevant patient history, suspected disease) in the scan request; this appears as the indication field in the radiology report. The interpretation and reporting of the image are substantially influenced by this request text, steering the radiologist to focus on particular aspects of the image. We use the indication field to drive better image classification, by taking a transformer network which is unimodally pre-trained on text (BERT) and fine-tuning it for multimodal classification of a dual image-text input. We evaluate the method on the MIMIC-CXR dataset, and present ablation studies to investigate the effect of the indication field on the classification performance. The experimental results show our approach achieves 87.8 average micro AUROC, outperforming the state-of-the-art methods for unimodal (84.4) and multimodal (86.0) classification. Our code is available at https://github.com/jacenkow/mmbt.", "comment": " Comments: Accepted at the IEEE International Symposium on Biomedical Imaging (ISBI) 2022 as an oral presentation "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06043", "id": "2202.06043", "pdf": "https://arxiv.org/pdf/2202.06043", "other": "https://arxiv.org/format/2202.06043"}, "title": "RoPGen: Towards Robust Code Authorship Attribution via Automatic Coding Style Transformation", "author_info": ["Zhen Li", " Guenevere", " Chen", "Chen Chen", "Yayi Zou", "Shouhuai Xu"], "summary": "Source code authorship attribution is an important problem often encountered in applications such as software forensics, bug fixing, and software quality analysis. Recent studies show that current source code authorship attribution methods can be compromised by attackers exploiting adversarial examples and coding style manipulation. This calls for robust solutions to the problem of code authorship attribution. In this paper, we initiate the study on making Deep Learning (DL)-based code authorship attribution robust. We propose an innovative framework called Robust coding style Patterns Generation (RoPGen), which essentially learns authors' unique coding style patterns that are hard for attackers to manipulate or imitate. The key idea is to combine data augmentation and gradient augmentation at the adversarial training phase. This effectively increases the diversity of training examples, generates meaningful perturbations to gradients of deep neural networks, and learns diversified representations of coding styles. We evaluate the effectiveness of RoPGen using four datasets of programs written in C, C++, and Java. Experimental results show that RoPGen can significantly improve the robustness of DL-based code authorship attribution, by respectively reducing 22.8% and 41.0% of the success rate of targeted and untargeted attacks on average.", "comment": " Comments: ICSE 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06014", "id": "2202.06014", "pdf": "https://arxiv.org/pdf/2202.06014", "other": "https://arxiv.org/format/2202.06014"}, "title": "Multi-direction and Multi-scale Pyramid in Transformer for Video-based Pedestrian Retrieval", "author_info": ["Xianghao Zang", "Ge Li", "Wei Gao"], "summary": "In video surveillance, pedestrian retrieval (also called person re-identification) is a critical task. This task aims to retrieve the pedestrian of interest from non-overlapping cameras. Recently, transformer-based models have achieved significant progress for this task. However, these models still suffer from ignoring fine-grained, part-informed information. This paper proposes a multi-direction and multi-scale Pyramid in Transformer (PiT) to solve this problem. In transformer-based architecture, each pedestrian image is split into many patches. Then, these patches are fed to transformer layers to obtain the feature representation of this image. To explore the fine-grained information, this paper proposes to apply vertical division and horizontal division on these patches to generate different-direction human parts. These parts provide more fine-grained information. To fuse multi-scale feature representation, this paper presents a pyramid structure containing global-level information and many pieces of local-level information from different scales. The feature pyramids of all the pedestrian images from the same video are fused to form the final multi-direction and multi-scale feature representation. Experimental results on two challenging video-based benchmarks, MARS and iLIDS-VID, show the proposed PiT achieves state-of-the-art performance. Extensive ablation studies demonstrate the superiority of the proposed pyramid structure. The code is available at https://git.openi.org.cn/zangxh/PiT.git.", "comment": " Comments: 10 pages, 6 figures, Accepted for publication in IEEE Transactions on Industrial Informatics "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05927", "id": "2202.05927", "pdf": "https://arxiv.org/pdf/2202.05927", "other": "https://arxiv.org/format/2202.05927"}, "title": "Energy landscape transformation of Ising problem with invariant eigenvalues for quantum annealing", "author_info": ["Toru Fujii", "Koshi Komuro", "Yosuke Okudaira", "Ryo Narita", "Masayasu Sawada"], "summary": "Quantum annealing tends to be more difficult as the energy landscape of the problem becomes complicated with many local minima. We have found a transformation for changing the energy landscape that swaps the eigenvalues and paired states without changing the eigenvalues of the instance at all. The transformation is basically a partial recombination of the two-spin interaction coefficient Jij and the longitudinal magnetic field interaction coefficient hi. The Hamming distance corresponding to a barrier between the states changes by the transformation, which in turn affects the ground state convergence. In the quantum annealing simulation results of a small number of spin instances, the annealing time was shortened by several orders of magnitude by applying the transformation. In addition, we also obtained a result using a D-Wave quantum annealer, which also showed a big improvement in the ground state convergence.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05728", "id": "2202.05728", "pdf": "https://arxiv.org/pdf/2202.05728", "other": "https://arxiv.org/format/2202.05728"}, "title": "Deep soccer captioning with transformer: dataset, semantics-related losses, and multi-level evaluation", "author_info": ["Ahmad Hammoudeh", "Bastein Vanderplaetse", "St\u00e9phane Dupont"], "summary": "This work aims at generating captions for soccer videos using deep learning. In this context, this paper introduces a dataset, model, and triple-level evaluation. The dataset consists of 22k caption-clip pairs and three visual features (images, optical flow, inpainting) for ~500 hours of \\emph{SoccerNet} videos. The model is divided into three parts: a transformer learns language, ConvNets learn vision, and a fusion of linguistic and visual features generates captions. The paper suggests evaluating generated captions at three levels: syntax (the commonly used evaluation metrics such as BLEU-score and CIDEr), meaning (the quality of descriptions for a domain expert), and corpus (the diversity of generated captions). The paper shows that the diversity of generated captions has improved (from 0.07 reaching 0.18) with semantics-related losses that prioritize selected words. Semantics-related losses and the utilization of more visual features (optical flow, inpainting) improved the normalized captioning score by 28\\%. The web page of this work: https://sites.google.com/view/soccercaptioning}{https://sites.google.com/view/soccercaptioning", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05690", "id": "2202.05690", "pdf": "https://arxiv.org/pdf/2202.05690", "other": "https://arxiv.org/format/2202.05690"}, "title": "HaT5: Hate Language Identification using Text-to-Text Transfer Transformer", "author_info": ["Sana Sabah Sabry", "Tosin Adewumi", "Nosheen Abid", "Gy\u00f6rgy Kovacs", "Foteini Liwicki", "Marcus Liwicki"], "summary": "We investigate the performance of a state-of-the art (SoTA) architecture T5 (available on the SuperGLUE) and compare with it 3 other previous SoTA architectures across 5 different tasks from 2 relatively diverse datasets. The datasets are diverse in terms of the number and types of tasks they have. To improve performance, we augment the training data by using an autoregressive model. We achieve near-SoTA results on a couple of the tasks - macro F1 scores of 81.66% for task A of the OLID 2019 dataset and 82.54% for task A of the hate speech and offensive content (HASOC) 2021 dataset, where SoTA are 82.9% and 83.05%, respectively. We perform error analysis and explain why one of the models (Bi-LSTM) makes the predictions it does by using a publicly available algorithm: Integrated Gradient (IG). This is because explainable artificial intelligence (XAI) is essential for earning the trust of users. The main contributions of this work are the implementation method of T5, which is discussed; the data augmentation using a new conversational AI model checkpoint, which brought performance improvements; and the revelation on the shortcomings of HASOC 2021 dataset. It reveals the difficulties of poor data annotation by using a small set of examples where the T5 model made the correct predictions, even when the ground truth of the test set were incorrect (in our opinion). We also provide our model checkpoints on the HuggingFace hub1 to foster transparency.", "comment": " MSC Class:           68                          "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05607", "id": "2202.05607", "pdf": "https://arxiv.org/pdf/2202.05607", "other": "https://arxiv.org/format/2202.05607"}, "title": "Online Decision Transformer", "author_info": ["Qinqing Zheng", "Amy Zhang", "Aditya Grover"], "summary": "Recent work has shown that offline reinforcement learning (RL) can be formulated as a sequence modeling problem (Chen et al., 2021; Janner et al., 2021) and solved via approaches similar to large-scale language modeling. However, any practical instantiation of RL also involves an online component, where policies pretrained on passive offline datasets are finetuned via taskspecific interactions with the environment. We propose Online Decision Transformers (ODT), an RL algorithm based on sequence modeling that blends offline pretraining with online finetuning in a unified framework. Our framework uses sequence-level entropy regularizers in conjunction with autoregressive modeling objectives for sample-efficient exploration and finetuning. Empirically, we show that ODT is competitive with the state-of-the-art in absolute performance on the D4RL benchmark but shows much more significant gains during the finetuning procedure.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05508", "id": "2202.05508", "pdf": "https://arxiv.org/pdf/2202.05508", "other": "https://arxiv.org/format/2202.05508"}, "title": "Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer", "author_info": ["Yair Kittenplon", "Inbal Lavi", "Sharon Fogel", "Yarin Bar", "R. Manmatha", "Pietro Perona"], "summary": "Text spotting end-to-end methods have recently gained attention in the literature due to the benefits of jointly optimizing the text detection and recognition components. Existing methods usually have a distinct separation between the detection and recognition branches, requiring exact annotations for the two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach for text spotting and the first text spotting framework which may be trained with both fully- and weakly-supervised settings. By learning a single latent representation per word detection, and using a novel loss function based on the Hungarian loss, our method alleviates the need for expensive localization annotations. Trained with only text transcription annotations on real data, our weakly-supervised method achieves competitive performance with previous state-of-the-art fully-supervised methods. When trained in a fully-supervised manner, TextTranSpotter shows state-of-the-art results on multiple benchmarks.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05492", "id": "2202.05492", "pdf": "https://arxiv.org/pdf/2202.05492", "other": "https://arxiv.org/format/2202.05492"}, "title": "Entroformer: A Transformer-based Entropy Model for Learned Image Compression", "author_info": ["Yichen Qian", "Ming Lin", "Xiuyu Sun", "Zhiyu Tan", "Rong Jin"], "summary": "One critical component in lossy deep image compression is the entropy model, which predicts the probability distribution of the quantized latent representation in the encoding and decoding modules. Previous works build entropy models upon convolutional neural networks which are inefficient in capturing global dependencies. In this work, we propose a novel transformer-based entropy model, termed Entroformer, to capture long-range dependencies in probability distribution estimation effectively and efficiently. Different from vision transformers in image classification, the Entroformer is highly optimized for image compression, including a top-k self-attention and a diamond relative position encoding. Meanwhile, we further expand this architecture with a parallel bidirectional context model to speed up the decoding process. The experiments show that the Entroformer achieves state-of-the-art performance on image compression while being time-efficient.", "comment": " Comments: Accepted at ICLR 2022 for poster. arXiv admin note: text overlap with arXiv:1809.02736 by other authors "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05451", "id": "2202.05451", "pdf": "https://arxiv.org/pdf/2202.05451", "other": "https://arxiv.org/format/2202.05451"}, "title": "ACORT: A Compact Object Relation Transformer for Parameter Efficient Image Captioning", "author_info": ["Jia Huei Tan", "Ying Hua Tan", "Chee Seng Chan", "Joon Huang Chuah"], "summary": "Recent research that applies Transformer-based architectures to image captioning has resulted in state-of-the-art image captioning performance, capitalising on the success of Transformers on natural language tasks. Unfortunately, though these models work well, one major flaw is their large model sizes. To this end, we present three parameter reduction methods for image captioning Transformers: Radix Encoding, cross-layer parameter sharing, and attention parameter sharing. By combining these methods, our proposed ACORT models have 3.7x to 21.6x fewer parameters than the baseline model without compromising test performance. Results on the MS-COCO dataset demonstrate that our ACORT models are competitive against baselines and SOTA approaches, with CIDEr score >=126. Finally, we present qualitative results and ablation studies to demonstrate the efficacy of the proposed changes further. Code and pre-trained models are publicly available at https://github.com/jiahuei/sparse-image-captioning.", "comment": " Comments: Neurocomputing; In Press "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05230", "id": "2202.05230", "pdf": "https://arxiv.org/pdf/2202.05230", "other": "https://arxiv.org/format/2202.05230"}, "title": "Integral Fourier transforms and the integral Hodge conjecture for one-cycles on abelian varieties", "author_info": ["Thorsten Beckmann", "Olivier de Gaay Fortman"], "summary": "We prove the integral Hodge conjecture for one-cycles on a principally polarized complex abelian variety whose minimal class is algebraic. In particular, the Jacobian of a smooth proper complex curve satisfies the integral Hodge conjecture for one-cycles. The main ingredient is a lift of the Fourier transform to integral Chow groups. Similarly, we prove the integral Tate conjecture for one-cycles on Jacobians of smooth proper curves over the separable closure of a finitely generated field. Furthermore, abelian varieties over the complex numbers (respectively an algebraically closed field of positive characteristic) satisfying the integral Hodge (respectively integral Tate) conjecture for one-cycles are dense in their moduli space.", "comment": " Comments: 21 pages, comments are welcome "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05170", "id": "2202.05170", "pdf": "https://arxiv.org/pdf/2202.05170", "other": "https://arxiv.org/format/2202.05170"}, "title": "Efficacy of Transformer Networks for Classification of Raw EEG Data", "author_info": ["Gourav Siddhad", "Anmol Gupta", "Debi Prosad Dogra", "Partha Pratim Roy"], "summary": "With the unprecedented success of transformer networks in natural language processing (NLP), recently, they have been successfully adapted to areas like computer vision, generative adversarial networks (GAN), and reinforcement learning. Classifying electroencephalogram (EEG) data has been challenging and researchers have been overly dependent on pre-processing and hand-crafted feature extraction. Despite having achieved automated feature extraction in several other domains, deep learning has not yet been accomplished for EEG. In this paper, the efficacy of the transformer network for the classification of raw EEG data (cleaned and pre-processed) is explored. The performance of transformer networks was evaluated on a local (age and gender data) and a public dataset (STEW). First, a classifier using a transformer network is built to classify the age and gender of a person with raw resting-state EEG data. Second, the classifier is tuned for mental workload classification with open access raw multi-tasking mental workload EEG data (STEW). The network achieves an accuracy comparable to state-of-the-art accuracy on both the local (Age and Gender dataset; 94.53% (gender) and 87.79% (age)) and the public (STEW dataset; 95.28% (two workload levels) and 88.72% (three workload levels)) dataset. The accuracy values have been achieved using raw EEG data without feature extraction. Results indicate that the transformer-based deep learning models can successfully abate the need for heavy feature-extraction of EEG data for successful classification.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05169", "id": "2202.05169", "pdf": "https://arxiv.org/pdf/2202.05169", "other": "https://arxiv.org/format/2202.05169"}, "title": "Radar-based Materials Classification Using Deep Wavelet Scattering Transform: A Comparison of Centimeter vs. Millimeter Wave Units", "author_info": ["Rami N. Khushaba", "Andrew J. Hill"], "summary": "Radar-based materials detection received significant attention in recent years for its potential inclusion in consumer and industrial applications like object recognition for grasping and manufacturing quality assurance and control. Several radar publications were developed for material classification under controlled settings with specific materials' properties and shapes. Recent literature has challenged the earlier findings on radars-based materials classification claiming that earlier solutions are not easily scaled to industrial applications due to a variety of real-world issues. Published experiments on the impact of these factors on the robustness of the extracted radar-based traditional features have already demonstrated that the application of deep neural networks can mitigate, to some extent, the impact to produce a viable solution. However, previous studies lacked an investigation of the usefulness of lower frequency radar units, specifically <10GHz, against the higher range units around and above 60GHz. This research considers two radar units with different frequency ranges: Walabot-3D (6.3-8 GHz) cm-wave and IMAGEVK-74 (62-69 GHz) mm-wave imaging units by Vayyar Imaging. A comparison is presented on the applicability of each unit for material classification. This work extends upon previous efforts, by applying deep wavelet scattering transform for the identification of different materials based on the reflected signals. In the wavelet scattering feature extractor, data is propagated through a series of wavelet transforms, nonlinearities, and averaging to produce low-variance representations of the reflected radar signals. This work is unique in comparison of the radar units and algorithms in material classification and includes real-time demonstrations that show strong performance by both units, with increased robustness offered by the cm-wave radar unit.", "comment": " Journal ref:         IEEE Robotics and Automation Letters, Volume 7, Issue 2, Pages 2016-2022, April 2022       "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05152", "id": "2202.05152", "pdf": "https://arxiv.org/pdf/2202.05152", "other": "https://arxiv.org/format/2202.05152"}, "title": "Feature-level augmentation to improve robustness of deep neural networks to affine transformations", "author_info": ["Adrian Sandru", "Mariana-Iuliana Georgescu", "Radu Tudor Ionescu"], "summary": "Recent studies revealed that convolutional neural networks do not generalize well to small image transformations, e.g. rotations by a few degrees or translations of a few pixels. To improve the robustness to such transformations, we propose to introduce data augmentation at intermediate layers of the neural architecture, in addition to the common data augmentation applied on the input images. By introducing small perturbations to activation maps (features) at various levels, we develop the capacity of the neural network to cope with such transformations. We conduct experiments on three image classification benchmarks (Tiny ImageNet, Caltech-256 and Food-101), considering two different convolutional architectures (ResNet-18 and DenseNet-121). When compared with two state-of-the-art stabilization methods, the empirical results show that our approach consistently attains the best trade-off between accuracy and mean flip rate.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05054", "id": "2202.05054", "pdf": "https://arxiv.org/pdf/2202.05054", "other": "https://arxiv.org/format/2202.05054"}, "title": "Exploiting Spatial Sparsity for Event Cameras with Visual Transformers", "author_info": ["Zuowen Wang", "Yuhuang Hu", "Shih-Chii Liu"], "summary": "Event cameras report local changes of brightness through an asynchronous stream of output events. Events are spatially sparse at pixel locations with little brightness variation. We propose using a visual transformer (ViT) architecture to leverage its ability to process a variable-length input. The input to the ViT consists of events that are accumulated into time bins and spatially separated into non-overlapping sub-regions called patches. Patches are selected when the number of nonzero pixel locations within a sub-region is above a threshold. We show that by fine-tuning a ViT model on the selected active patches, we can reduce the average number of patches fed into the backbone during the inference by at least 50% with only a minor drop (0.34%) of the classification accuracy on the N-Caltech101 dataset. This reduction translates into a decrease of 51% in Multiply-Accumulate (MAC) operations and an increase of 46% in the inference speed using a server CPU.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.04942", "id": "2202.04942", "pdf": "https://arxiv.org/pdf/2202.04942", "other": "https://arxiv.org/format/2202.04942"}, "title": "Spherical Transformer", "author_info": ["Sungmin Cho", "Raehyuk Jung", "Junseok Kwon"], "summary": "Using convolutional neural networks for 360images can induce sub-optimal performance due to distortions entailed by a planar projection. The distortion gets deteriorated when a rotation is applied to the 360image. Thus, many researches based on convolutions attempt to reduce the distortions to learn accurate representation. In contrast, we leverage the transformer architecture to solve image classification problems for 360images. Using the proposed transformer for 360images has two advantages. First, our method does not require the erroneous planar projection process by sampling pixels from the sphere surface. Second, our sampling method based on regular polyhedrons makes low rotation equivariance errors, because specific rotations can be reduced to permutations of faces. In experiments, we validate our network on two aspects, as follows. First, we show that using a transformer with highly uniform sampling methods can help reduce the distortion. Second, we demonstrate that the transformer architecture can achieve rotation equivariance on specific rotations. We compare our method to other state-of-the-art algorithms using the SPH-MNIST, SPH-CIFAR, and SUN360 datasets and show that our method is competitive with other methods.", "comment": " ACM Class:           I.4; I.2                "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.04512", "id": "2202.04512", "pdf": "https://arxiv.org/pdf/2202.04512", "other": "https://arxiv.org/format/2202.04512"}, "title": "Binary Darboux transformation of the first member of the negative part of the AKNS hierarchy and solitons", "author_info": ["Folkert M\u00fcller-Hoissen"], "summary": "Using bidifferential calculus, we derive a vectorial binary Darboux transformation for the first member of the \"negative\" part of the AKNS hierarchy. A reduction leads to a rather simple nonlinear PDE in two dimensions with a leading mixed third order derivative. This PDE may be regarded as describing dynamics of a complex scalar field in one dimension, since it is invariant under coordinate transformations of one of the two independent variables. We exploit the correspondingly reduced binary Darboux transformation to generate multi-soliton solutions of the PDE, also with additional rational dependence on the independent variables, and on a plane wave background.", "comment": " MSC Class:           35C05; 35C07; 35C08; 35C09; 35Q51                          "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.04366", "id": "2202.04366", "pdf": "https://arxiv.org/pdf/2202.04366", "other": "https://arxiv.org/format/2202.04366"}, "title": "Increasing the Minimum Distance of Polar-Like Codes with Pre-Transformation", "author_info": ["Samet Gelincik", "Philippe Mary", "Anne Savard", "Jean-Yves Baudais"], "summary": "Reed Muller (RM) codes are known for their good minimum distance. One can use their structure to construct polar-like codes with good distance properties by choosing the information set as the rows of the polarization matrix with the highest Hamming weight, instead of the most reliable synthetic channels. However, the information length options of RM codes are quite limited due to their specific structure. In this work, we present sufficient conditions to increase the information length by at least one bit for some underlying RM codes and in order to obtain pre-transformed polar-like codes with the same minimum distance than lower rate codes. Moreover, our findings are combined with the method presented in [1] to further reduce the number of minimum weight codewords. Numerical results show that the designed codes perform close to the meta-converse bound at short blocklengths and better than the polarized adjusted convolutional polar codes with the same parameters.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.04287", "id": "2202.04287", "pdf": "https://arxiv.org/pdf/2202.04287", "other": "https://arxiv.org/format/2202.04287"}, "title": "Amplitude Spectrum Transformation for Open Compound Domain Adaptive Semantic Segmentation", "author_info": ["Jogendra Nath Kundu", "Akshay Kulkarni", "Suvaansh Bhambri", "Varun Jampani", "R. Venkatesh Babu"], "summary": "Open compound domain adaptation (OCDA) has emerged as a practical adaptation setting which considers a single labeled source domain against a compound of multi-modal unlabeled target data in order to generalize better on novel unseen domains. We hypothesize that an improved disentanglement of domain-related and task-related factors of dense intermediate layer features can greatly aid OCDA. Prior-arts attempt this indirectly by employing adversarial domain discriminators on the spatial CNN output. However, we find that latent features derived from the Fourier-based amplitude spectrum of deep CNN features hold a more tractable mapping with domain discrimination. Motivated by this, we propose a novel feature space Amplitude Spectrum Transformation (AST). During adaptation, we employ the AST auto-encoder for two purposes. First, carefully mined source-target instance pairs undergo a simulation of cross-domain feature stylization (AST-Sim) at a particular layer by altering the AST-latent. Second, AST operating at a later layer is tasked to normalize (AST-Norm) the domain content by fixing its latent to a mean prototype. Our simplified adaptation technique is not only clustering-free but also free from complex adversarial alignment. We achieve leading performance against the prior arts on the OCDA scene segmentation benchmarks.", "comment": " Comments: AAAI 2022. Project page: http://sites.google.com/view/ast-ocdaseg "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.04243", "id": "2202.04243", "pdf": "https://arxiv.org/pdf/2202.04243", "other": "https://arxiv.org/format/2202.04243"}, "title": "Motion-Aware Transformer For Occluded Person Re-identification", "author_info": ["Mi Zhou", "Hongye Liu", "Zhekun Lv", "Wei Hong", "Xiai Chen"], "summary": "Recently, occluded person re-identification(Re-ID) remains a challenging task that people are frequently obscured by other people or obstacles, especially in a crowd massing situation. In this paper, we propose a self-supervised deep learning method to improve the location performance for human parts through occluded person Re-ID. Unlike previous works, we find that motion information derived from the photos of various human postures can help identify major human body components. Firstly, a motion-aware transformer encoder-decoder architecture is designed to obtain keypoints heatmaps and part-segmentation maps. Secondly, an affine transformation module is utilized to acquire motion information from the keypoint detection branch. Then the motion information will support the segmentation branch to achieve refined human part segmentation maps, and effectively divide the human body into reasonable groups. Finally, several cases demonstrate the efficiency of the proposed model in distinguishing different representative parts of the human body, which can avoid the background and occlusion disturbs. Our method consistently achieves state-of-the-art results on several popular datasets, including occluded, partial, and holistic.", "comment": " Comments: 10 pages, 3 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.04200", "id": "2202.04200", "pdf": "https://arxiv.org/pdf/2202.04200", "other": "https://arxiv.org/format/2202.04200"}, "title": "MaskGIT: Masked Generative Image Transformer", "author_info": ["Huiwen Chang", "Han Zhang", "Lu Jiang", "Ce Liu", "William T. Freeman"], "summary": "Generative transformers have experienced rapid popularity growth in the computer vision community in synthesizing high-fidelity and high-resolution images. The best generative transformer models so far, however, still treat an image naively as a sequence of tokens, and decode an image sequentially following the raster scan ordering (i.e. line-by-line). We find this strategy neither optimal nor efficient. This paper proposes a novel image synthesis paradigm using a bidirectional transformer decoder, which we term MaskGIT. During training, MaskGIT learns to predict randomly masked tokens by attending to tokens in all directions. At inference time, the model begins with generating all tokens of an image simultaneously, and then refines the image iteratively conditioned on the previous generation. Our experiments demonstrate that MaskGIT significantly outperforms the state-of-the-art transformer model on the ImageNet dataset, and accelerates autoregressive decoding by up to 64x. Besides, we illustrate that MaskGIT can be easily extended to various image editing tasks, such as inpainting, extrapolation, and image manipulation.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.04181", "id": "2202.04181", "pdf": "https://arxiv.org/pdf/2202.04181", "other": "https://arxiv.org/format/2202.04181"}, "title": "TransformNet: Self-supervised representation learning through predicting geometric transformations", "author_info": ["Sayed Hashim", "Muhammad Ali"], "summary": "Deep neural networks need a big amount of training data, while in the real world there is a scarcity of data available for training purposes. To resolve this issue unsupervised methods are used for training with limited data. In this report, we describe the unsupervised semantic feature learning approach for recognition of the geometric transformation applied to the input data. The basic concept of our approach is that if someone is unaware of the objects in the images, he/she would not be able to quantitatively predict the geometric transformation that was applied to them. This self supervised scheme is based on pretext task and the downstream task. The pretext classification task to quantify the geometric transformations should force the CNN to learn high-level salient features of objects useful for image classification. In our baseline model, we define image rotations by multiples of 90 degrees. The CNN trained on this pretext task will be used for the classification of images in the CIFAR-10 dataset as a downstream task. we run the baseline method using various models, including ResNet, DenseNet, VGG-16, and NIN with a varied number of rotations in feature extracting and fine-tuning settings. In extension of this baseline model we experiment with transformations other than rotation in pretext task. We compare performance of selected models in various settings with different transformations applied to images,various data augmentation techniques as well as using different optimizers. This series of different type of experiments will help us demonstrate the recognition accuracy of our self-supervised model when applied to a downstream task of classification.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.04053", "id": "2202.04053", "pdf": "https://arxiv.org/pdf/2202.04053", "other": "https://arxiv.org/format/2202.04053"}, "title": "DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers", "author_info": ["Jaemin Cho", "Abhay Zala", "Mohit Bansal"], "summary": "Generating images from textual descriptions has gained a lot of attention. Recently, DALL-E, a multimodal transformer language model, and its variants have shown high-quality text-to-image generation capabilities with a simple architecture and training objective, powered by large-scale training data and computation. However, despite the interesting image generation results, there has not been a detailed analysis on how to evaluate such models. In this work, we investigate the reasoning capabilities and social biases of such text-to-image generative transformers in detail. First, we measure four visual reasoning skills: object recognition, object counting, color recognition, and spatial relation understanding. For this, we propose PaintSkills, a diagnostic dataset and evaluation toolkit that measures these four visual reasoning skills. Second, we measure the text alignment and quality of the generated images based on pretrained image captioning, image-text retrieval, and image classification models. Third, we assess social biases in the models. For this, we suggest evaluation of gender and racial biases of text-to-image generation models based on a pretrained image-text retrieval model and human evaluation. In our experiments, we show that recent text-to-image models perform better in recognizing and counting objects than recognizing colors and understanding spatial relations, while there exists a large gap between model performances and oracle accuracy on all skills. Next, we demonstrate that recent text-to-image models learn specific gender/racial biases from web image-text pairs. We also show that our automatic evaluations of visual reasoning skills and gender bias are highly correlated with human judgments. We hope our work will help guide future progress in improving text-to-image models on visual reasoning skills and social biases. Code and data at: https://github.com/j-min/DallEval", "comment": " Comments: 20 pages, 10 figures, 13 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03944", "id": "2202.03944", "pdf": "https://arxiv.org/pdf/2202.03944", "other": "https://arxiv.org/format/2202.03944"}, "title": "Detecting Anomalies within Time Series using Local Neural Transformations", "author_info": ["Tim Schneider", "Chen Qiu", "Marius Kloft", "Decky Aspandi Latif", "Steffen Staab", "Stephan Mandt", "Maja Rudolph"], "summary": "We develop a new method to detect anomalies within time series, which is essential in many application domains, reaching from self-driving cars, finance, and marketing to medical diagnosis and epidemiology. The method is based on self-supervised deep learning that has played a key role in facilitating deep anomaly detection on images, where powerful image transformations are available. However, such transformations are widely unavailable for time series. Addressing this, we develop Local Neural Transformations(LNT), a method learning local transformations of time series from data. The method produces an anomaly score for each time step and thus can be used to detect anomalies within time series. We prove in a theoretical analysis that our novel training objective is more suitable for transformation learning than previous deep Anomaly detection(AD) methods. Our experiments demonstrate that LNT can find anomalies in speech segments from the LibriSpeech data set and better detect interruptions to cyber-physical systems than previous work. Visualization of the learned transformations gives insight into the type of transformations that LNT learns.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03772", "id": "2202.03772", "pdf": "https://arxiv.org/pdf/2202.03772", "other": "https://arxiv.org/format/2202.03772"}, "title": "Particle Transformer for Jet Tagging", "author_info": ["Huilin Qu", "Congqiao Li", "Sitian Qian"], "summary": "Jet tagging is a critical yet challenging classification task in particle physics. While deep learning has transformed jet tagging and significantly improved performance, the lack of a large-scale public dataset impedes further enhancement. In this work, we present JetClass, a new comprehensive dataset for jet tagging. The JetClass dataset consists of 100 M jets, about two orders of magnitude larger than existing public datasets. A total of 10 types of jets are simulated, including several types unexplored for tagging so far. Based on the large dataset, we propose a new Transformer-based architecture for jet tagging, called Particle Transformer (ParT). By incorporating pairwise particle interactions in the attention mechanism, ParT achieves higher tagging performance than a plain Transformer and surpasses the previous state-of-the-art, ParticleNet, by a large margin. The pre-trained ParT models, once fine-tuned, also substantially enhance the performance on two widely adopted jet tagging benchmarks.", "comment": " Comments: 12 pages, 3 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03709", "id": "2202.03709", "pdf": "https://arxiv.org/pdf/2202.03709", "other": "https://arxiv.org/format/2202.03709"}, "title": "Symmetric states for C\u2217-Fermi systems II: Klein transformation and their structure", "author_info": ["Francesco Fidaleo"], "summary": "In the present note, which is the second part of a work concerning the study of the set of the symmetric states, we introduce the extension of the Klein transformation for general Fermi tensor product of two $\\bz^2$ graded C\u2217-algebras, under the condition that the grading of one of the involved algebras is inner. After extending the construction to C\u2217-inductive limits, such a Klein transformation realises a canonical \u2217-isomorphism between two $\\bz^2$-graded C\u2217-algebras made of the infinite Fermi C\u2217-tensor product and the infinite C\u2217-tensor product of a single $\\bz^2$-graded C\u2217-algebra, both built with respect to the corresponding minimal C\u2217-cross norms. It preserves the grading, and its transpose sends even product states of $\\ga_{\\rm X}$ in (necessarily even) product states on $\\ga_{\\rm F}$, and therefore induces an isomorphism of simplexes \\cs_\\bp(\\ga_{\\rm\u00a0F})=\\cs_{\\bp\\times\\bz^2}(\\ga_{\\rm\u00a0F})\\sim\\cs_{\\bp\\times\\bz^2}(\\ga_{\\rm\u00a0X})\\,, which allows to reduce the study of the structure of the symmetric states for C\u2217-Fermi systems to the corresponding even symmetric states on the usual infinite C\u2217-tensor product. Other relevant properties of symmetric states on the Fermi algebra will be proved without the use of the Klein transformation.", "comment": " Comments: 20 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03576", "id": "2202.03576", "pdf": "https://arxiv.org/pdf/2202.03576", "other": "https://arxiv.org/format/2202.03576"}, "title": "Learnability Lock: Authorized Learnability Control Through Adversarial Invertible Transformations", "author_info": ["Weiqi Peng", "Jinghui Chen"], "summary": "Owing much to the revolution of information technology, the recent progress of deep learning benefits incredibly from the vastly enhanced access to data available in various digital formats. However, in certain scenarios, people may not want their data being used for training commercial models and thus studied how to attack the learnability of deep learning models. Previous works on learnability attack only consider the goal of preventing unauthorized exploitation on the specific dataset but not the process of restoring the learnability for authorized cases. To tackle this issue, this paper introduces and investigates a new concept called \"learnability lock\" for controlling the model's learnability on a specific dataset with a special key. In particular, we propose adversarial invertible transformation, that can be viewed as a mapping from image to image, to slightly modify data samples so that they become \"unlearnable\" by machine learning models with negligible loss of visual features. Meanwhile, one can unlock the learnability of the dataset and train models normally using the corresponding key. The proposed learnability lock leverages class-wise perturbation that applies a universal transformation function on data samples of the same label. This ensures that the learnability can be easily restored with a simple inverse transformation while remaining difficult to be detected or reverse-engineered. We empirically demonstrate the success and practicability of our method on visual classification tasks.", "comment": " Comments: Accepted at ICLR 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03548", "id": "2202.03548", "pdf": "https://arxiv.org/pdf/2202.03548", "other": "https://arxiv.org/format/2202.03548"}, "title": "HeadPosr: End-to-end Trainable Head Pose Estimation using Transformer Encoders", "author_info": ["Naina Dhingra"], "summary": "In this paper, HeadPosr is proposed to predict the head poses using a single RGB image. \\textit{HeadPosr} uses a novel architecture which includes a transformer encoder. In concrete, it consists of: (1) backbone; (2) connector; (3) transformer encoder; (4) prediction head. The significance of using a transformer encoder for HPE is studied. An extensive ablation study is performed on varying the (1) number of encoders; (2) number of heads; (3) different position embeddings; (4) different activations; (5) input channel size, in a transformer used in HeadPosr. Further studies on using: (1) different backbones, (2) using different learning rates are also shown. The elaborated experiments and ablations studies are conducted using three different open-source widely used datasets for HPE, i.e., 300W-LP, AFLW2000, and BIWI datasets. Experiments illustrate that \\textit{HeadPosr} outperforms all the state-of-art methods including both the landmark-free and the others based on using landmark or depth estimation on the AFLW2000 dataset and BIWI datasets when trained with 300W-LP. It also outperforms when averaging the results from the compared datasets, hence setting a benchmark for the problem of HPE, also demonstrating the effectiveness of using transformers over the state-of-the-art.", "comment": " Journal ref:         In 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021) (pp. 1-8)       "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03528", "id": "2202.03528", "pdf": "https://arxiv.org/pdf/2202.03528", "other": "https://arxiv.org/format/2202.03528"}, "title": "TACTiS: Transformer-Attentional Copulas for Time Series", "author_info": ["Alexandre Drouin", "\u00c9tienne Marcotte", "Nicolas Chapados"], "summary": "The estimation of time-varying quantities is a fundamental component of decision making in fields such as healthcare and finance. However, the practical utility of such estimates is limited by how accurately they quantify predictive uncertainty. In this work, we address the problem of estimating the joint predictive distribution of high-dimensional multivariate time series. We propose a versatile method, based on the transformer architecture, that estimates joint distributions using an attention-based decoder that provably learns to mimic the properties of non-parametric copulas. The resulting model has several desirable properties: it can scale to hundreds of time series, supports both forecasting and interpolation, can handle unaligned and non-uniformly sampled data, and can seamlessly adapt to missing data during training. We demonstrate these properties empirically and show that our model produces state-of-the-art predictions on several real-world datasets.", "comment": " Comments: 27 pages, 15 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03341", "id": "2202.03341", "pdf": "https://arxiv.org/pdf/2202.03341", "other": "https://arxiv.org/format/2202.03341"}, "title": "Neighbor2Seq: Deep Learning on Massive Graphs by Transforming Neighbors to Sequences", "author_info": ["Meng Liu", "Shuiwang Ji"], "summary": "Modern graph neural networks (GNNs) use a message passing scheme and have achieved great success in many fields. However, this recursive design inherently leads to excessive computation and memory requirements, making it not applicable to massive real-world graphs. In this work, we propose the Neighbor2Seq to transform the hierarchical neighborhood of each node into a sequence. This novel transformation enables the subsequent mini-batch training for general deep learning operations, such as convolution and attention, that are designed for grid-like data and are shown to be powerful in various domains. Therefore, our Neighbor2Seq naturally endows GNNs with the efficiency and advantages of deep learning operations on grid-like data by precomputing the Neighbor2Seq transformations. We evaluate our method on a massive graph, with more than 111 million nodes and 1.6 billion edges, as well as several medium-scale graphs. Results show that our proposed method is scalable to massive graphs and achieves superior performance across massive and medium-scale graphs. Our code is available at https://github.com/divelab/Neighbor2Seq.", "comment": " Comments: Accepted by SDM2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03280", "id": "2202.03280", "pdf": "https://arxiv.org/pdf/2202.03280", "other": "https://arxiv.org/format/2202.03280"}, "title": "Elaborating the word problem for free idempotent-generated semigroups over the full transformation monoid", "author_info": ["Igor Dolinka"], "summary": "With each semigroup one can associate a partial algebra, called the biordered set, which captures important algebraic and geometric features of the structure of idempotents of that semigroup. For a biordered set E, one can construct the free idempotent-generated semigroup over E, IG(E), which is the free-est semigroup (in a definite categorical sense) whose biorder of idempotents is isomorphic to E. Studies of these intriguing objects have been recently focusing on their particular aspects, such as maximal subgroups, the word problem, etc. In 2012, Gray and Ru\u0161kuc pointed out that a more detailed investigation into the structure of the free idempotent-generated semigroup over the biorder of Tn, the full transformation monoid over an n-element set, might be worth pursuing. In 2019, together with Gould and Yang, the present author showed that the word problem for IG(ETn) is algorithmically soluble. In a recent work by the author, it was showed that, for a wide class of biorders E, the solution of the word problem hinges on the knowledge of the so-called vertex groups, which arise as certain subgroups of direct products of pairs of maximal subgroups of IG(E). In this paper we compute these vertex groups for the case when E is the biorder of idempotents of Tn, thus making the solution of the word problem of IG(ETn) explicit (and hence, feasible to be subject to computational implementation).", "comment": " MSC Class:           Primary 20M05; Secondary 20B25; 20F10; 20M20                          "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03183", "id": "2202.03183", "pdf": "https://arxiv.org/pdf/2202.03183", "other": "https://arxiv.org/format/2202.03183"}, "title": "TransFollower: Long-Sequence Car-Following Trajectory Prediction through Transformer", "author_info": ["Meixin Zhu", "Simon S. Du", "Xuesong Wang", " Hao", " Yang", "Ziyuan Pu", "Yinhai Wang"], "summary": "Car-following refers to a control process in which the following vehicle (FV) tries to keep a safe distance between itself and the lead vehicle (LV) by adjusting its acceleration in response to the actions of the vehicle ahead. The corresponding car-following models, which describe how one vehicle follows another vehicle in the traffic flow, form the cornerstone for microscopic traffic simulation and intelligent vehicle development. One major motivation of car-following models is to replicate human drivers' longitudinal driving trajectories. To model the long-term dependency of future actions on historical driving situations, we developed a long-sequence car-following trajectory prediction model based on the attention-based Transformer model. The model follows a general format of encoder-decoder architecture. The encoder takes historical speed and spacing data as inputs and forms a mixed representation of historical driving context using multi-head self-attention. The decoder takes the future LV speed profile as input and outputs the predicted future FV speed profile in a generative way (instead of an auto-regressive way, avoiding compounding errors). Through cross-attention between encoder and decoder, the decoder learns to build a connection between historical driving and future LV speed, based on which a prediction of future FV speed can be obtained. We train and test our model with 112,597 real-world car-following events extracted from the Shanghai Naturalistic Driving Study (SH-NDS). Results show that the model outperforms the traditional intelligent driver model (IDM), a fully connected neural network model, and a long short-term memory (LSTM) based model in terms of long-sequence trajectory prediction accuracy. We also visualized the self-attention and cross-attention heatmaps to explain how the model derives its predictions.", "comment": ""}]}