{"data": [{"arxiv": {"page": "https://arxiv.org/abs/2202.10054", "id": "2202.10054", "pdf": "https://arxiv.org/pdf/2202.10054", "other": "https://arxiv.org/format/2202.10054"}, "title": "Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution", "author_info": ["Ananya Kumar", "Aditi Raghunathan", "Robbie Jones", "Tengyu Ma", "Percy Liang"], "summary": "When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer -- the \"head\"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR \u2192 STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head -- this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning).", "comment": " Comments: ICLR (Oral) 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09955", "id": "2202.09955", "pdf": "https://arxiv.org/pdf/2202.09955", "other": "https://arxiv.org/format/2202.09955"}, "title": "StyleBERT: Chinese pretraining by font style information", "author_info": ["Chao Lv", "Han Zhang", "XinKai Du", "Yunhao Zhang", "Ying Huang", "Wenhao Li", "Jia Han", "Shanshan Gu"], "summary": "With the success of down streaming task using English pre-trained langueage model, the pre-trained Chinese language model is also necessary to get a better performance of Chinese NLP task. Unlike the English language, Chinese has its special characters such as glyph information. So in this article, we propose the Chinese pre-trained language model StyleBERT which incorporate the following embedding information to enhance the savvy of language model, such as word, pinyin, five stroke and chaizi. The experiments show that the model achieves well performances on a wide range of Chinese NLP tasks.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08975", "id": "2202.08975", "pdf": "https://arxiv.org/pdf/2202.08975", "other": "https://arxiv.org/format/2202.08975"}, "title": "Probing Pretrained Models of Source Code", "author_info": ["Sergey Troshin", "Nadezhda Chirkova"], "summary": "Deep learning models are widely used for solving challenging code processing tasks, such as code generation or code summarization. Traditionally, a specific model architecture was carefully built to solve a particular code processing task. However, recently general pretrained models such as CodeBERT or CodeT5 have been shown to outperform task-specific models in many applications. While pretrained models are known to learn complex patterns from data, they may fail to understand some properties of source code. To test diverse aspects of code understanding, we introduce a set of diagnosting probing tasks. We show that pretrained models of code indeed contain information about code syntactic structure and correctness, the notions of identifiers, data flow and namespaces, and natural language naming. We also investigate how probing results are affected by using code-specific pretraining objectives, varying the model size, or finetuning.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08937", "id": "2202.08937", "pdf": "https://arxiv.org/pdf/2202.08937", "other": "https://arxiv.org/format/2202.08937"}, "title": "When, Why, and Which Pretrained GANs Are Useful?", "author_info": ["Timofey Grigoryev", "Andrey Voynov", "Artem Babenko"], "summary": "The literature has proposed several methods to finetune pretrained GANs on new datasets, which typically results in higher performance compared to training from scratch, especially in the limited-data regime. However, despite the apparent empirical benefits of GAN pretraining, its inner mechanisms were not analyzed in-depth, and understanding of its role is not entirely clear. Moreover, the essential practical details, e.g., selecting a proper pretrained GAN checkpoint, currently do not have rigorous grounding and are typically determined by trial and error.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08360", "id": "2202.08360", "pdf": "https://arxiv.org/pdf/2202.08360", "other": "https://arxiv.org/format/2202.08360"}, "title": "Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision", "author_info": ["Priya Goyal", "Quentin Duval", "Isaac Seessel", "Mathilde Caron", "Ishan Misra", "Levent Sagun", "Armand Joulin", "Piotr Bojanowski"], "summary": "Discriminative self-supervised learning allows training models on any random group of internet images, and possibly recover salient information that helps differentiate between the images. Applied to ImageNet, this leads to object centric features that perform on par with supervised features on most object-centric downstream tasks. In this work, we question if using this ability, we can learn any salient and more representative information present in diverse unbounded set of images from across the globe. To do so, we train models on billions of random images without any data pre-processing or prior assumptions about what we want the model to learn. We scale our model size to dense 10 billion parameters to avoid underfitting on a large data size. We extensively study and validate our model performance on over 50 benchmarks including fairness, robustness to distribution shift, geographical diversity, fine grained recognition, image copy detection and many image classification datasets. The resulting model, not only captures well semantic information, it also captures information about artistic style and learns salient information such as geolocations and multilingual word embeddings based on visual content only. More importantly, we discover that such model is more robust, more fair, less harmful and less biased than supervised models or models trained on object centric datasets such as ImageNet.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07894", "id": "2202.07894", "pdf": "https://arxiv.org/pdf/2202.07894", "other": "https://arxiv.org/format/2202.07894"}, "title": "Knowledge Transfer from Large-scale Pretrained Language Models to End-to-end Speech Recognizers", "author_info": ["Yotaro Kubo", "Shigeki Karita", "Michiel Bacchiani"], "summary": "End-to-end speech recognition is a promising technology for enabling compact automatic speech recognition (ASR) systems since it can unify the acoustic and language model into a single neural network. However, as a drawback, training of end-to-end speech recognizers always requires transcribed utterances. Since end-to-end models are also known to be severely data hungry, this constraint is crucial especially because obtaining transcribed utterances is costly and can possibly be impractical or impossible. This paper proposes a method for alleviating this issue by transferring knowledge from a language model neural network that can be pretrained with text-only data. Specifically, this paper attempts to transfer semantic knowledge acquired in embedding vectors of large-scale language models. Since embedding vectors can be assumed as implicit representations of linguistic information such as part-of-speech, intent, and so on, those are also expected to be useful modeling cues for ASR decoders. This paper extends two types of ASR decoders, attention-based decoders and neural transducers, by modifying training loss functions to include embedding prediction terms. The proposed systems were shown to be effective for error rate reduction without incurring extra computational costs in the decoding phase.", "comment": " Comments: To be presented in ICASSP 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07893", "id": "2202.07893", "pdf": "https://arxiv.org/pdf/2202.07893", "other": "https://arxiv.org/format/2202.07893"}, "title": "A Survey of Pretraining on Graphs: Taxonomy, Methods, and Applications", "author_info": ["Jun Xia", "Yanqiao Zhu", "Yuanqi Du", "Stan Z. Li"], "summary": "Pretrained Language Models (PLMs) such as BERT have revolutionized the landscape of Natural Language Processing (NLP). Inspired by their proliferation, tremendous efforts have been devoted to Pretrained Graph Models (PGMs). Owing to the powerful model architectures of PGMs, abundant knowledge from massive labeled and unlabeled graph data can be captured. The knowledge implicitly encoded in model parameters can benefit various downstream tasks and help to alleviate several fundamental issues of learning on graphs. In this paper, we provide the first comprehensive survey for PGMs. We firstly present the limitations of graph representation learning and thus introduce the motivation for graph pre-training. Then, we systematically categorize existing PGMs based on a taxonomy from four different perspectives. Next, we present the applications of PGMs in social recommendation and drug discovery. Finally, we outline several promising research directions that can serve as a guideline for future research.", "comment": " Comments: 9 pages. Submitted to IJCAI 2022 (Survey Track) "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07206", "id": "2202.07206", "pdf": "https://arxiv.org/pdf/2202.07206", "other": "https://arxiv.org/format/2202.07206"}, "title": "Impact of Pretraining Term Frequencies on Few-Shot Reasoning", "author_info": ["Yasaman Razeghi", "Robert L. Logan IV", "Matt Gardner", "Sameer Singh"], "summary": "Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above 70% (absolute) more accurate on the top 10\\% frequent terms in comparison to the bottom 10\\%. Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.04582", "id": "2202.04582", "pdf": "https://arxiv.org/pdf/2202.04582", "other": "https://arxiv.org/format/2202.04582"}, "title": "Topic Discovery via Latent Space Clustering of Pretrained Language Model Representations", "author_info": ["Yu Meng", "Yunyi Zhang", "Jiaxin Huang", "Yu Zhang", "Jiawei Han"], "summary": "Topic models have been the prominent tools for automatic topic discovery from text corpora. Despite their effectiveness, topic models suffer from several limitations including the inability of modeling word ordering information in documents, the difficulty of incorporating external linguistic knowledge, and the lack of both accurate and efficient inference methods for approximating the intractable posterior. Recently, pretrained language models (PLMs) have brought astonishing performance improvements to a wide variety of tasks due to their superior representations of text. Interestingly, there have not been standard approaches to deploy PLMs for topic discovery as better alternatives to topic models. In this paper, we begin by analyzing the challenges of using PLM representations for topic discovery, and then propose a joint latent space learning and clustering framework built upon PLM embeddings. In the latent space, topic-word and document-topic distributions are jointly modeled so that the discovered topics can be interpreted by coherent and distinctive terms and meanwhile serve as meaningful summaries of the documents. Our model effectively leverages the strong representation power and superb linguistic features brought by PLMs for topic discovery, and is conceptually simpler than topic models. On two benchmark datasets in different domains, our model generates significantly more coherent and diverse topics than strong topic models, and offers better topic-wise document representations, based on both automatic and human evaluations.", "comment": " Comments: WWW 2022. (Code: https://github.com/yumeng5/TopClus) "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03514", "id": "2202.03514", "pdf": "https://arxiv.org/pdf/2202.03514", "other": "https://arxiv.org/format/2202.03514"}, "title": "Maximizing Audio Event Detection Model Performance on Small Datasets Through Knowledge Transfer, Data Augmentation, And Pretraining: An Ablation Study", "author_info": ["Daniel Tompkins", "Kshitiz Kumar", "Jian Wu"], "summary": "An Xception model reaches state-of-the-art (SOTA) accuracy on the ESC-50 dataset for audio event detection through knowledge transfer from ImageNet weights, pretraining on AudioSet, and an on-the-fly data augmentation pipeline. This paper presents an ablation study that analyzes which components contribute to the boost in performance and training time. A smaller Xception model is also presented which nears SOTA performance with almost a third of the parameters.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.00187", "id": "2202.00187", "pdf": "https://arxiv.org/pdf/2202.00187", "other": "https://arxiv.org/format/2202.00187"}, "title": "Deep Reference Priors: What is the best way to pretrain a model?", "author_info": ["Yansong Gao", "Rahul Ramesh", "Pratik Chaudhari"], "summary": "What is the best way to exploit extra data -- be it unlabeled data from the same task, or labeled data from a related task -- to learn a given task? This paper formalizes the question using the theory of reference priors. Reference priors are objective, uninformative Bayesian priors that maximize the mutual information between the task and the weights of the model. Such priors enable the task to maximally affect the Bayesian posterior, e.g., reference priors depend upon the number of samples available for learning the task and for very small sample sizes, the prior puts more probability mass on low-complexity models in the hypothesis space. This paper presents the first demonstration of reference priors for medium-scale deep networks and image-based data. We develop generalizations of reference priors and demonstrate applications to two problems. First, by using unlabeled data to compute the reference prior, we develop new Bayesian semi-supervised learning methods that remain effective even with very few samples per class. Second, by using labeled data from the source task to compute the reference prior, we develop a new pretraining method for transfer learning that allows data from the target task to maximally affect the Bayesian posterior. Empirical validation of these methods is conducted on image classification datasets.", "comment": " Comments: 23 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11147", "id": "2201.11147", "pdf": "https://arxiv.org/pdf/2201.11147", "other": "https://arxiv.org/format/2201.11147"}, "title": "OntoProtein: Protein Pretraining With Gene Ontology Embedding", "author_info": ["Ningyu Zhang", "Zhen Bi", "Xiaozhuan Liang", "Siyuan Cheng", "Haosen Hong", "Shumin Deng", "Jiazhang Lian", "Qiang Zhang", "Huajun Chen"], "summary": "Self-supervised protein language models have proved their effectiveness in learning the proteins representations. With the increasing computational power, current protein language models pre-trained with millions of diverse sequences can advance the parameter scale from million-level to billion-level and achieve remarkable improvement. However, those prevailing approaches rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better protein representations. We argue that informative biology knowledge in KGs can enhance protein representation with external knowledge. In this work, we propose OntoProtein, the first general framework that makes use of structure in GO (Gene Ontology) into protein pre-training models. We construct a novel large-scale knowledge graph that consists of GO and its related proteins, and gene annotation texts or protein sequences describe all nodes in the graph. We propose novel contrastive learning with knowledge-aware negative sampling to jointly optimize the knowledge graph and protein embedding during pre-training. Experimental results show that OntoProtein can surpass state-of-the-art methods with pre-trained protein language models in TAPE benchmark and yield better performance compared with baselines in protein-protein interaction and protein function prediction. Code and datasets are available in https://github.com/zjunlp/OntoProtein.", "comment": " Comments: Accepted by ICLR 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.09745", "id": "2201.09745", "pdf": "https://arxiv.org/pdf/2201.09745", "other": "https://arxiv.org/format/2201.09745"}, "title": "Table Pre-training: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks", "author_info": ["Haoyu Dong", "Zhoujun Cheng", "Xinyi He", "Mengyu Zhou", "Anda Zhou", "Fan Zhou", "Ao Liu", "Shi Han", "Dongmei Zhang"], "summary": "Since a vast number of tables can be easily collected from web pages, spreadsheets, PDFs, and various other document types, a flurry of table pre-training frameworks have been proposed following the success of text and images, and they have achieved new state-of-the-arts on various tasks such as table question answering, table type recognition, column relation classification, table search, formula prediction, etc. To fully use the supervision signals in unlabeled tables, a variety of pre-training objectives have been designed and evaluated, for example, denoising cell values, predicting numerical relationships, and implicitly executing SQLs. And to best leverage the characteristics of (semi-)structured tables, various tabular language models, particularly with specially-designed attention mechanisms, have been explored. Since tables usually appear and interact with free-form text, table pre-training usually takes the form of table-text joint pre-training, which attracts significant research interests from multiple domains. This survey aims to provide a comprehensive review of different model designs, pre-training objectives, and downstream tasks for table pre-training, and we further share our thoughts and vision on existing challenges and future opportunities.", "comment": " Comments: Work in progress "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.08934", "id": "2201.08934", "pdf": "https://arxiv.org/pdf/2201.08934", "other": "https://arxiv.org/format/2201.08934"}, "title": "Supervised and Self-supervised Pretraining Based COVID-19 Detection Using Acoustic Breathing/Cough/Speech Signals", "author_info": ["Xing-Yu Chen", "Qiu-Shi Zhu", "Jie Zhang", "Li-Rong Dai"], "summary": "In this work, we propose a bi-directional long short-term memory (BiLSTM) network based COVID-19 detection method using breath/speech/cough signals. By using the acoustic signals to train the network, respectively, we can build individual models for three tasks, whose parameters are averaged to obtain an average model, which is then used as the initialization for the BiLSTM model training of each task. This initialization method can significantly improve the performance on the three tasks, which surpasses the official baseline results. Besides, we also utilize a public pre-trained model wav2vec2.0 and pre-train it using the official DiCOVA datasets. This wav2vec2.0 model is utilized to extract high-level features of the sound as the model input to replace conventional mel-frequency cepstral coefficients (MFCC) features. Experimental results reveal that using high-level features together with MFCC features can improve the performance. To further improve the performance, we also deploy some preprocessing techniques like silent segment removal, amplitude normalization and time-frequency mask. The proposed detection model is evaluated on the DiCOVA dataset and results show that our method achieves an area under curve (AUC) score of 88.44% on blind test in the fusion track.", "comment": " Comments: Accepted by ICASSP 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.08264", "id": "2201.08264", "pdf": "https://arxiv.org/pdf/2201.08264", "other": "https://arxiv.org/format/2201.08264"}, "title": "End-to-end Generative Pretraining for Multimodal Video Captioning", "author_info": ["Paul Hongsuck Seo", "Arsha Nagrani", "Anurag Arnab", "Cordelia Schmid"], "summary": "Recent video and language pretraining frameworks lack the ability to generate sentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new pretraining framework for learning from unlabelled videos which can be effectively used for generative tasks such as multimodal video captioning. Unlike recent video-language pretraining frameworks, our framework trains both a multimodal video encoder and a sentence decoder jointly. To overcome the lack of captions in unlabelled videos, we leverage the future utterance as an additional text source and propose a bidirectional generation objective -- we generate future utterances given the present mulitmodal context, and also the present utterance given future observations. With this objective, we train an encoder-decoder model end-to-end to generate a caption from raw pixels and transcribed speech directly. Our model achieves state-of-the-art performance for multimodal video captioning on four standard benchmarks, as well as for other video understanding tasks such as VideoQA, video retrieval and action classification.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.07449", "id": "2201.07449", "pdf": "https://arxiv.org/pdf/2201.07449"}, "title": "TourBERT: A pretrained language model for the tourism industry", "author_info": ["Veronika Arefieva", "Roman Egger"], "summary": "The Bidirectional Encoder Representations from Transformers (BERT) is currently one of the most important and state-of-the-art models for natural language. However, it has also been shown that for domain-specific tasks it is helpful to pretrain BERT on a domain-specific corpus. In this paper, we present TourBERT, a pretrained language model for tourism. We describe how TourBERT was developed and evaluated. The evaluations show that TourBERT is outperforming BERT in all tourism-specific tasks.", "comment": " Comments: 13 pages, 7 figures, 4 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.06910", "id": "2201.06910", "pdf": "https://arxiv.org/pdf/2201.06910", "other": "https://arxiv.org/format/2201.06910"}, "title": "ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization", "author_info": ["Hanwei Xu", "Yujun Chen", "Yulun Du", "Nan Shao", "Yanggang Wang", "Haiyu Li", "Zhilin Yang"], "summary": "We propose a multitask pretraining approach ZeroPrompt for zero-shot generalization, focusing on task scaling and zero-shot prompting. While previous models are trained on only a few dozen tasks, we scale to 1,000 tasks for the first time using real-world data. This leads to a crucial discovery that task scaling can be an efficient alternative to model scaling; i.e., the model size has little impact on performance with an extremely large number of tasks. Our results show that task scaling can substantially improve training efficiency by 30 times in FLOPs. Moreover, we present a prompting method that incorporates a genetic algorithm to automatically search for the best prompt for unseen tasks, along with a few other improvements. Empirically, ZeroPrompt substantially improves both the efficiency and the performance of zero-shot learning across a variety of academic and production datasets.", "comment": " Comments: 23 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.05411", "id": "2201.05411", "pdf": "https://arxiv.org/pdf/2201.05411", "other": "https://arxiv.org/format/2201.05411"}, "title": "Eliciting Knowledge from Pretrained Language Models for Prototypical Prompt Verbalizer", "author_info": ["Yinyi Wei", "Tong Mo", "Yongtao Jiang", "Weiping Li", "Wen Zhao"], "summary": "Recent advances on prompt-tuning cast few-shot classification tasks as a masked language modeling problem. By wrapping input into a template and using a verbalizer which constructs a mapping between label space and label word space, prompt-tuning can achieve excellent results in zero-shot and few-shot scenarios. However, typical prompt-tuning needs a manually designed verbalizer which requires domain expertise and human efforts. And the insufficient label space may introduce considerable bias into the results. In this paper, we focus on eliciting knowledge from pretrained language models and propose a prototypical prompt verbalizer for prompt-tuning. Labels are represented by prototypical embeddings in the feature space rather than by discrete words. The distances between the embedding at the masked position of input and prototypical embeddings are used as classification criterion. For zero-shot settings, knowledge is elicited from pretrained language models by a manually designed template to form initial prototypical embeddings. For few-shot settings, models are tuned to learn meaningful and interpretable prototypical embeddings. Our method optimizes models by contrastive learning. Extensive experimental results on several many-class text classification datasets with low-resource settings demonstrate the effectiveness of our approach compared with other verbalizer construction methods. Our implementation is available at https://github.com/Ydongd/prototypical-prompt-verbalizer.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.05273", "id": "2201.05273", "pdf": "https://arxiv.org/pdf/2201.05273", "other": "https://arxiv.org/format/2201.05273"}, "title": "A Survey of Pretrained Language Models Based Text Generation", "author_info": ["Junyi Li", "Tianyi Tang", "Wayne Xin Zhao", "Jian-Yun Nie", "Ji-Rong Wen"], "summary": "Text Generation aims to produce plausible and readable text in human language from input data. The resurgence of deep learning has greatly advanced this field by neural generation models, especially the paradigm of pretrained language models (PLMs). Grounding text generation on PLMs is seen as a promising direction in both academia and industry. In this survey, we present the recent advances achieved in the topic of PLMs for text generation. In detail, we begin with introducing three key points of applying PLMs to text generation: 1) how to encode the input data as representations preserving input semantics which can be fused into PLMs; 2) how to design a universal and performant architecture of PLMs served as generation models; and 3) how to optimize PLMs given the reference text and ensure the generated text satisfying special text properties. Then, we figure out several challenges and future directions within each key point. Next, we present a summary of various useful resources and typical text generation applications to work with PLMs. Finally, we conclude and summarize the contribution of this survey.", "comment": " Comments: 37 pages, 2 figures, 2 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.01046", "id": "2201.01046", "pdf": "https://arxiv.org/pdf/2201.01046", "other": "https://arxiv.org/format/2201.01046"}, "title": "Sound and Visual Representation Learning with Multiple Pretraining Tasks", "author_info": ["Arun Balajee Vasudevan", "Dengxin Dai", "Luc Van Gool"], "summary": "Different self-supervised tasks (SSL) reveal different features from the data. The learned feature representations can exhibit different performance for each downstream task. In this light, this work aims to combine Multiple SSL tasks (Multi-SSL) that generalizes well for all downstream tasks. Specifically, for this study, we investigate binaural sounds and image data in isolation. For binaural sounds, we propose three SSL tasks namely, spatial alignment, temporal synchronization of foreground objects and binaural audio and temporal gap prediction. We investigate several approaches of Multi-SSL and give insights into the downstream task performance on video retrieval, spatial sound super resolution, and semantic prediction on the OmniAudio dataset. Our experiments on binaural sound representations demonstrate that Multi-SSL via incremental learning (IL) of SSL tasks outperforms single SSL task models and fully supervised models in the downstream task performance. As a check of applicability on other modality, we also formulate our Multi-SSL models for image representation learning and we use the recently proposed SSL tasks, MoCov2 and DenseCL. Here, Multi-SSL surpasses recent methods such as MoCov2, DenseCL and DetCo by 2.06%, 3.27% and 1.19% on VOC07 classification and +2.83, +1.56 and +1.61 AP on COCO detection. Code will be made publicly available.", "comment": " Comments: 11 pages, 3 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.11929", "id": "2112.11929", "pdf": "https://arxiv.org/pdf/2112.11929", "other": "https://arxiv.org/format/2112.11929"}, "title": "Meta-Learning and Self-Supervised Pretraining for Real World Image Translation", "author_info": ["Ileana Rugina", "Rumen Dangovski", "Mark Veillette", "Pooya Khorrami", "Brian Cheung", "Olga Simek", "Marin Solja\u010di\u0107"], "summary": "Recent advances in deep learning, in particular enabled by hardware advances and big data, have provided impressive results across a wide range of computational problems such as computer vision, natural language, or reinforcement learning. Many of these improvements are however constrained to problems with large-scale curated data-sets which require a lot of human labor to gather. Additionally, these models tend to generalize poorly under both slight distributional shifts and low-data regimes. In recent years, emerging fields such as meta-learning or self-supervised learning have been closing the gap between proof-of-concept results and real-life applications of machine learning by extending deep-learning to the semi-supervised and few-shot domains. We follow this line of work and explore spatio-temporal structure in a recently introduced image-to-image translation problem in order to: i) formulate a novel multi-task few-shot image generation benchmark and ii) explore data augmentations in contrastive pre-training for image translation downstream tasks. We present several baselines for the few-shot problem and discuss trade-offs between different approaches. Our code is available at https://github.com/irugina/meta-image-translation.", "comment": " Comments: 10 pages, 8 figures, 2 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.11389", "id": "2112.11389", "pdf": "https://arxiv.org/pdf/2112.11389", "other": "https://arxiv.org/format/2112.11389"}, "title": "Supervised Graph Contrastive Pretraining for Text Classification", "author_info": ["Samujjwal Ghosh", "Subhadeep Maji", "Maunendra Sankar Desarkar"], "summary": "Contrastive pretraining techniques for text classification has been largely studied in an unsupervised setting. However, oftentimes labeled data from related tasks which share label semantics with current task is available. We hypothesize that using this labeled data effectively can lead to better generalization on current task. In this paper, we propose a novel way to effectively utilize labeled data from related tasks with a graph based supervised contrastive learning approach. We formulate a token-graph by extrapolating the supervised information from examples to tokens. Our formulation results in an embedding space where tokens with high/low probability of belonging to same class are near/further-away from one another. We also develop detailed theoretical insights which serve as a motivation for our method. In our experiments with 13 datasets, we show our method outperforms pretraining schemes by 2.5% and also example-level contrastive learning based formulation by 1.8% on average. In addition, we show cross-domain effectiveness of our method in a zero-shot setting by 3.91% on average. Lastly, we also demonstrate our method can be used as a noisy teacher in a knowledge distillation setting to significantly improve performance of transformer based models in low labeled data regime by 4.57% on average.", "comment": " Comments: A condensed version of this paper has been accepted to ACM SAC'22. DOI: https://doi.org/10.1145/3477314.3507194 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.09106", "id": "2112.09106", "pdf": "https://arxiv.org/pdf/2112.09106", "other": "https://arxiv.org/format/2112.09106"}, "title": "RegionCLIP: Region-based Language-Image Pretraining", "author_info": ["Yiwu Zhong", "Jianwei Yang", "Pengchuan Zhang", "Chunyuan Li", "Noel Codella", "Liunian Harold Li", "Luowei Zhou", "Xiyang Dai", "Lu Yuan", "Yin Li", "Jianfeng Gao"], "summary": "Contrastive language-image pretraining (CLIP) using image-text pairs has achieved impressive results on image classification in both zero-shot and transfer learning settings. However, we show that directly applying such models to recognize image regions for object detection leads to poor performance due to a domain shift: CLIP was trained to match an image as a whole to a text description, without capturing the fine-grained alignment between image regions and text spans. To mitigate this issue, we propose a new method called RegionCLIP that significantly extends CLIP to learn region-level visual representations, thus enabling fine-grained alignment between image regions and textual concepts. Our method leverages a CLIP model to match image regions with template captions and then pretrains our model to align these region-text pairs in the feature space. When transferring our pretrained model to the open-vocabulary object detection tasks, our method significantly outperforms the state of the art by 3.8 AP50 and 2.2 AP for novel categories on COCO and LVIS datasets, respectively. Moreoever, the learned region representations support zero-shot inference for object detection, showing promising results on both COCO and LVIS datasets. Our code is available at https://github.com/microsoft/RegionCLIP.", "comment": " Comments: Technical report "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.08786", "id": "2112.08786", "pdf": "https://arxiv.org/pdf/2112.08786", "other": "https://arxiv.org/format/2112.08786"}, "title": "Efficient Hierarchical Domain Adaptation for Pretrained Language Models", "author_info": ["Alexandra Chronopoulou", "Matthew E. Peters", "Jesse Dodge"], "summary": "Generative language models are trained on diverse, general domain corpora. However, this limits their applicability to narrower domains, and prior work has shown that continued in-domain training can provide further gains. In this paper, we introduce a method to scale domain adaptation to many diverse domains using a computationally efficient adapter approach. Our method is based on the observation that textual domains are partially overlapping, and we represent domains as a hierarchical tree structure where each node in the tree is associated with a set of adapter weights. When combined with a frozen pretrained language model, this approach enables parameter sharing among related domains, while avoiding negative interference between unrelated ones. It is efficient and computational cost scales as O(log(D)) for D domains. Experimental results with GPT-2 and a large fraction of the 100 most represented websites in C4 show across-the-board improvements in-domain. We additionally provide an inference time algorithm for a held-out domain and show that averaging over multiple paths through the tree enables further gains in generalization, while adding only a marginal cost to inference.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.08709", "id": "2112.08709", "pdf": "https://arxiv.org/pdf/2112.08709", "other": "https://arxiv.org/format/2112.08709"}, "title": "DOCmT5: Document-Level Pretraining of Multilingual Language Models", "author_info": ["Chia-Hsuan Lee", "Aditya Siddhant", "Viresh Ratnakar", "Melvin Johnson"], "summary": "In this paper, we introduce DOCmT5, a multilingual sequence-to-sequence language model pre-trained with large scale parallel documents. While previous approaches have focused on leveraging sentence-level parallel data, we try to build a general-purpose pre-trained model that can understand and generate long documents. We propose a simple and effective pre-training objective - Document Reordering Machine Translation (DrMT), in which the input documents that are shuffled and masked need to be translated. DrMT brings consistent improvements over strong baselines on a variety of document-level generation tasks, including over 12 BLEU points for seen-language-pair document-level MT, over 7 BLEU points for unseen-language-pair document-level MT and over 3 ROUGE-1 points for seen-language-pair cross-lingual summarization. We achieve state-of-the-art (SOTA) on WMT20 De-En and IWSLT15 Zh-En document translation tasks. We also conduct extensive analysis on various factors for document pre-training, including (1) the effects of pre-training data quality and (2) The effects of combining mono-lingual and cross-lingual pre-training. We plan to make our model checkpoints publicly available.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.08615", "id": "2112.08615", "pdf": "https://arxiv.org/pdf/2112.08615", "other": "https://arxiv.org/format/2112.08615"}, "title": "Commonsense Knowledge-Augmented Pretrained Language Models for Causal Reasoning Classification", "author_info": ["Pedram Hosseini", "David A. Broniatowski", "Mona Diab"], "summary": "Commonsense knowledge can be leveraged for identifying causal relations in text. In this work, we verbalize triples in ATOMIC2020, a wide coverage commonsense reasoning knowledge graph, to natural language text and continually pretrain a BERT pretrained language model. We evaluate the resulting model on answering commonsense reasoning questions. Our results show that a continually pretrained language model augmented with commonsense reasoning knowledge outperforms our baseline on two commonsense causal reasoning benchmarks, COPA and BCOPA-CE, without additional improvement on the base model or using quality-enhanced data for fine-tuning.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.08327", "id": "2112.08327", "pdf": "https://arxiv.org/pdf/2112.08327", "other": "https://arxiv.org/format/2112.08327"}, "title": "Evaluating Pretrained Transformer Models for Entity Linking in Task-Oriented Dialog", "author_info": ["Sai Muralidhar Jayanthi", "Varsha Embar", "Karthik Raghunathan"], "summary": "The wide applicability of pretrained transformer models (PTMs) for natural language tasks is well demonstrated, but their ability to comprehend short phrases of text is less explored. To this end, we evaluate different PTMs from the lens of unsupervised Entity Linking in task-oriented dialog across 5 characteristics -- syntactic, semantic, short-forms, numeric and phonetic. Our results demonstrate that several of the PTMs produce sub-par results when compared to traditional techniques, albeit competitive to other neural baselines. We find that some of their shortcomings can be addressed by using PTMs fine-tuned for text-similarity tasks, which illustrate an improved ability in comprehending semantic and syntactic correspondences, as well as some improvements for short-forms, numeric and phonetic variations in entity mentions. We perform qualitative analysis to understand nuances in their predictions and discuss scope for further improvements. Code can be found at https://github.com/murali1996/el_tod", "comment": " Comments: Accepted as short paper at ICON 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07874", "id": "2112.07874", "pdf": "https://arxiv.org/pdf/2112.07874", "other": "https://arxiv.org/format/2112.07874"}, "title": "Oracle Linguistic Graphs Complement a Pretrained Transformer Language Model: A Cross-formalism Comparison", "author_info": ["Jakob Prange", "Nathan Schneider", "Lingpeng Kong"], "summary": "We examine the extent to which, in principle, linguistic graph representations can complement and improve neural language modeling. With an ensemble setup consisting of a pretrained Transformer and ground-truth graphs from one of 7 different formalisms, we find that, overall, semantic constituency structures are most useful to language modeling performance -- outpacing syntactic constituency structures as well as syntactic and semantic dependency structures. Further, effects vary greatly depending on part-of-speech class. In sum, our findings point to promising tendencies in neuro-symbolic language modeling and invite future research quantifying the design choices made by different formalisms.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07868", "id": "2112.07868", "pdf": "https://arxiv.org/pdf/2112.07868", "other": "https://arxiv.org/format/2112.07868"}, "title": "Few-shot Instruction Prompts for Pretrained Language Models to Detect Social Biases", "author_info": ["Shrimai Prabhumoye", "Rafal Kocielnik", "Mohammad Shoeybi", "Anima Anandkumar", "Bryan Catanzaro"], "summary": "Detecting social bias in text is challenging due to nuance, subjectivity, and difficulty in obtaining good quality labeled datasets at scale, especially given the evolving nature of social biases and society. To address these challenges, we propose a few-shot instruction-based method for prompting pre-trained language models (LMs). We select a few label-balanced exemplars from a small support repository that are closest to the query to be labeled in the embedding space. We then provide the LM with instruction that consists of this subset of labeled exemplars, the query text to be classified, a definition of bias, and prompt it to make a decision. We demonstrate that large LMs used in a few-shot context can detect different types of fine-grained biases with similar and sometimes superior accuracy to fine-tuned models. We observe that the largest 530B parameter model is significantly more effective in detecting social bias compared to smaller models (achieving at least 20% improvement in AUC metric compared to other models). It also maintains a high AUC (dropping less than 5%) in a few-shot setting with a labeled repository reduced to as few as 100 samples. Large pretrained language models thus make it easier and quicker to build new bias detectors.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07447", "id": "2112.07447", "pdf": "https://arxiv.org/pdf/2112.07447", "other": "https://arxiv.org/format/2112.07447"}, "title": "Measuring Fairness with Biased Rulers: A Survey on Quantifying Biases in Pretrained Language Models", "author_info": ["Pieter Delobelle", "Ewoenam Kwaku Tokpo", "Toon Calders", "Bettina Berendt"], "summary": "An increasing awareness of biased patterns in natural language processing resources, like BERT, has motivated many metrics to quantify `bias' and `fairness'. But comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the existing literature on fairness metrics for pretrained language models and experimentally evaluate compatibility, including both biases in language models as in their downstream tasks. We do this by a mixture of traditional literature survey and correlation analysis, as well as by running empirical evaluations. We find that many metrics are not compatible and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, if not at least highly subjective. To improve future comparisons and fairness evaluations, we recommend avoiding embedding-based metrics and focusing on fairness evaluations in downstream tasks.", "comment": " Comments: 15 pages, 4 figures, 3 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07254", "id": "2112.07254", "pdf": "https://arxiv.org/pdf/2112.07254", "other": "https://arxiv.org/format/2112.07254"}, "title": "Improving Hybrid CTC/Attention End-to-end Speech Recognition with Pretrained Acoustic and Language Model", "author_info": ["Keqi Deng", "Songjun Cao", "Yike Zhang", "Long Ma"], "summary": "Recently, self-supervised pretraining has achieved impressive results in end-to-end (E2E) automatic speech recognition (ASR). However, the dominant sequence-to-sequence (S2S) E2E model is still hard to fully utilize the self-supervised pre-training methods because its decoder is conditioned on acoustic representation thus cannot be pretrained separately. In this paper, we propose a pretrained Transformer (Preformer) S2S ASR architecture based on hybrid CTC/attention E2E models to fully utilize the pretrained acoustic models (AMs) and language models (LMs). In our framework, the encoder is initialized with a pretrained AM (wav2vec2.0). The Preformer leverages CTC as an auxiliary task during training and inference. Furthermore, we design a one-cross decoder (OCD), which relaxes the dependence on acoustic representations so that it can be initialized with pretrained LM (DistilGPT2). Experiments are conducted on the AISHELL-1 corpus and achieve a 4.6% character error rate (CER) on the test set. Compared with our vanilla hybrid CTC/attention Transformer baseline, our proposed CTC/attention-based Preformer yields 27% relative CER reduction. To the best of our knowledge, this is the first work to utilize both pretrained AM and LM in a S2S ASR system.", "comment": " Comments: ASRU2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.05705", "id": "2112.05705", "pdf": "https://arxiv.org/pdf/2112.05705", "other": "https://arxiv.org/format/2112.05705"}, "title": "Pruning Pretrained Encoders with a Multitask Objective", "author_info": ["Patrick Xia", "Richard Shin"], "summary": "The sizes of pretrained language models make them challenging and expensive to use when there are multiple desired downstream tasks. In this work, we adopt recent strategies for model pruning during finetuning to explore the question of whether it is possible to prune a single encoder so that it can be used for multiple tasks. We allocate a fixed parameter budget and compare pruning a single model with a multitask objective against the best ensemble of single-task models. We find that under two pruning strategies (element-wise and rank pruning), the approach with the multitask objective outperforms training models separately when averaged across all tasks, and it is competitive on each individual one. Additional analysis finds that using a multitask objective during pruning can also be an effective method for reducing model sizes for low-resource tasks.", "comment": " Comments: ENLSP NeurIPS 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.05328", "id": "2112.05328", "pdf": "https://arxiv.org/pdf/2112.05328", "other": "https://arxiv.org/format/2112.05328"}, "title": "Multimodal Interactions Using Pretrained Unimodal Models for SIMMC 2.0", "author_info": ["Joosung Lee", "Kijong Han"], "summary": "This paper presents our work on the Situated Interactive MultiModal Conversations 2.0 challenge held at Dialog State Tracking Challenge 10. SIMMC 2.0 includes 4 subtasks, and we introduce our multimodal approaches for the subtask \\#1, \\#2 and the generation of subtask \\#4. SIMMC 2.0 dataset is a multimodal dataset containing image and text information, which is more challenging than the problem of only text-based conversations because it must be solved by understanding the relationship between image and text. Therefore, since there is a limit to solving only text models such as BERT or GPT2, we propose a multimodal model combining image and text. We first pretrain the multimodal model to understand the relationship between image and text, then finetune our model for each task. We achieve the 3rd best performance in subtask \\#1, \\#2 and a runner-up in the generation of subtask \\#4. The source code is available at https://github.com/rungjoo/simmc2.0.", "comment": " Comments: Accepted to DSTC10 challenge wokrshop at AAAI 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.04966", "id": "2112.04966", "pdf": "https://arxiv.org/pdf/2112.04966", "other": "https://arxiv.org/format/2112.04966"}, "title": "CaSP: Class-agnostic Semi-Supervised Pretraining for Detection and Segmentation", "author_info": ["Lu Qi", "Jason Kuen", "Zhe Lin", "Jiuxiang Gu", "Fengyun Rao", "Dian Li", "Weidong Guo", "Zhen Wen", "Jiaya Jia"], "summary": "To improve instance-level detection/segmentation performance, existing self-supervised and semi-supervised methods extract either very task-unrelated or very task-specific training signals from unlabeled data. We argue that these two approaches, at the two extreme ends of the task-specificity spectrum, are suboptimal for the task performance. Utilizing too little task-specific training signals causes underfitting to the ground-truth labels of downstream tasks, while the opposite causes overfitting to the ground-truth labels. To this end, we propose a novel Class-agnostic Semi-supervised Pretraining (CaSP) framework to achieve a more favorable task-specificity balance in extracting training signals from unlabeled data. Compared to semi-supervised learning, CaSP reduces the task specificity in training signals by ignoring class information in the pseudo labels and having a separate pretraining stage that uses only task-unrelated unlabeled data. On the other hand, CaSP preserves the right amount of task specificity by leveraging box/mask-level pseudo labels. As a result, our pretrained model can better avoid underfitting/overfitting to ground-truth labels when finetuned on the downstream task. Using 3.6M unlabeled data, we achieve a remarkable performance gain of 4.7% over ImageNet-pretrained baseline on object detection. Our pretrained model also demonstrates excellent transferability to other detection and segmentation tasks/frameworks.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.04187", "id": "2112.04187", "pdf": "https://arxiv.org/pdf/2112.04187", "other": "https://arxiv.org/format/2112.04187"}, "title": "Pretrained Cost Model for Distributed Constraint Optimization Problems", "author_info": ["Yanchen Deng", "Shufeng Kong", "Bo An"], "summary": "Distributed Constraint Optimization Problems (DCOPs) are an important subclass of combinatorial optimization problems, where information and controls are distributed among multiple autonomous agents. Previously, Machine Learning (ML) has been largely applied to solve combinatorial optimization problems by learning effective heuristics. However, existing ML-based heuristic methods are often not generalizable to different search algorithms. Most importantly, these methods usually require full knowledge about the problems to be solved, which are not suitable for distributed settings where centralization is not realistic due to geographical limitations or privacy concerns. To address the generality issue, we propose a novel directed acyclic graph representation schema for DCOPs and leverage the Graph Attention Networks (GATs) to embed graph representations. Our model, GAT-PCM, is then pretrained with optimally labelled data in an offline manner, so as to construct effective heuristics to boost a broad range of DCOP algorithms where evaluating the quality of a partial assignment is critical, such as local search or backtracking search. Furthermore, to enable decentralized model inference, we propose a distributed embedding schema of GAT-PCM where each agent exchanges only embedded vectors, and show its soundness and complexity. Finally, we demonstrate the effectiveness of our model by combining it with a local search or a backtracking search algorithm. Extensive empirical evaluations indicate that the GAT-PCM-boosted algorithms significantly outperform the state-of-the-art methods in various benchmarks. The pretrained model is available at https://github.com/dyc941126/GAT-PCM.", "comment": " Comments: Accepted by AAAI-22 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.03625", "id": "2112.03625", "pdf": "https://arxiv.org/pdf/2112.03625", "other": "https://arxiv.org/format/2112.03625"}, "title": "Parsing with Pretrained Language Models, Multiple Datasets, and Dataset Embeddings", "author_info": ["Rob van der Goot", "Miryam de Lhoneux"], "summary": "With an increase of dataset availability, the potential for learning from a variety of data sources has increased. One particular method to improve learning from multiple data sources is to embed the data source during training. This allows the model to learn generalizable features as well as distinguishing features between datasets. However, these dataset embeddings have mostly been used before contextualized transformer-based embeddings were introduced in the field of Natural Language Processing. In this work, we compare two methods to embed datasets in a transformer-based multilingual dependency parser, and perform an extensive evaluation. We show that: 1) embedding the dataset is still beneficial with these models 2) performance increases are highest when embedding the dataset at the encoder level 3) unsurprisingly, we confirm that performance increases are highest for small datasets and datasets with a low baseline score. 4) we show that training on the combination of all datasets performs similarly to designing smaller clusters based on language-relatedness.", "comment": " Comments: Accepted to TLT at SyntaxFest 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.03014", "id": "2112.03014", "pdf": "https://arxiv.org/pdf/2112.03014", "other": "https://arxiv.org/format/2112.03014"}, "title": "Transformer-based Korean Pretrained Language Models: A Survey on Three Years of Progress", "author_info": ["Kichang Yang"], "summary": "With the advent of Transformer, which was used in translation models in 2017, attention-based architectures began to attract attention. Furthermore, after the emergence of BERT, which strengthened the NLU-specific encoder part, which is a part of the Transformer, and the GPT architecture, which strengthened the NLG-specific decoder part, various methodologies, data, and models for learning the Pretrained Language Model began to appear. Furthermore, in the past three years, various Pretrained Language Models specialized for Korean have appeared. In this paper, we intend to numerically and qualitatively compare and analyze various Korean PLMs released to the public.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.02839", "id": "2112.02839", "pdf": "https://arxiv.org/pdf/2112.02839", "other": "https://arxiv.org/format/2112.02839"}, "title": "MoCA: Incorporating Multi-stage Domain Pretraining and Cross-guided Multimodal Attention for Textbook Question Answering", "author_info": ["Fangzhi Xu", "Qika Lin", "Jun Liu", "Lingling Zhang", "Tianzhe Zhao", "Qi Chai", "Yudai Pan"], "summary": "Textbook Question Answering (TQA) is a complex multimodal task to infer answers given large context descriptions and abundant diagrams. Compared with Visual Question Answering (VQA), TQA contains a large number of uncommon terminologies and various diagram inputs. It brings new challenges to the representation capability of language model for domain-specific spans. And it also pushes the multimodal fusion to a more complex level. To tackle the above issues, we propose a novel model named MoCA, which incorporates multi-stage domain pretraining and multimodal cross attention for the TQA task. Firstly, we introduce a multi-stage domain pretraining module to conduct unsupervised post-pretraining with the span mask strategy and supervised pre-finetune. Especially for domain post-pretraining, we propose a heuristic generation algorithm to employ the terminology corpus. Secondly, to fully consider the rich inputs of context and diagrams, we propose cross-guided multimodal attention to update the features of text, question diagram and instructional diagram based on a progressive strategy. Further, a dual gating mechanism is adopted to improve the model ensemble. The experimental results show the superiority of our model, which outperforms the state-of-the-art methods by 2.21% and 2.43% for validation and test split respectively.", "comment": " Comments: 9 pages, 6 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.02597", "id": "2112.02597", "pdf": "https://arxiv.org/pdf/2112.02597", "other": "https://arxiv.org/format/2112.02597"}, "title": "Simple Adaptive Projection with Pretrained Features for Anomaly Detection", "author_info": ["Xingtai Gui"], "summary": "Deep anomaly detection aims to separate anomaly from normal samples with high-quality representations. Pretrained features bring effective representation and promising anomaly detection performance. However, with one-class training data, adapting the pretrained features is a thorny problem. Specifically, the existing optimization objectives with global target often lead to pattern collapse, i.e. all inputs are mapped to the same. In this paper, we propose a novel adaptation framework including simple linear transformation and self-attention. Such adaptation is applied on a specific input, and its k nearest representations of normal samples in pretrained feature space and the inner-relationship between similar one-class semantic features are mined. Furthermore, based on such framework, we propose an effective constraint term to avoid learning trivial solution. Our simple adaptive projection with pretrained features(SAP2) yields a novel anomaly detection criterion which is more accurate and robust to pattern collapse. Our method achieves state-of-the-art anomaly detection performance on semantic anomaly detection and sensory anomaly detection benchmarks including 96.5% AUROC on CIFAR-100 dataset, 97.0% AUROC on CIFAR-10 dataset and 88.1% AUROC on MvTec dataset.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.01529", "id": "2112.01529", "pdf": "https://arxiv.org/pdf/2112.01529", "other": "https://arxiv.org/format/2112.01529"}, "title": "BEVT: BERT Pretraining of Video Transformers", "author_info": ["Rui Wang", "Dongdong Chen", "Zuxuan Wu", "Yinpeng Chen", "Xiyang Dai", "Mengchen Liu", "Yu-Gang Jiang", "Luowei Zhou", "Lu Yuan"], "summary": "This paper studies the BERT pretraining of video transformers. It is a straightforward but worth-studying extension given the recent success from BERT pretraining of image transformers. We introduce BEVT which decouples video representation learning into spatial representation learning and temporal dynamics learning. In particular, BEVT first performs masked image modeling on image data, and then conducts masked image modeling jointly with masked video modeling on video data. This design is motivated by two observations: 1) transformers learned on image datasets provide decent spatial priors that can ease the learning of video transformers, which are often times computationally-intensive if trained from scratch; 2) discriminative clues, i.e., spatial and temporal information, needed to make correct predictions vary among different videos due to large intra-class and inter-class variations. We conduct extensive experiments on three challenging video benchmarks where BEVT achieves very promising results. On Kinetics 400, for which recognition mostly relies on discriminative spatial representations, BEVT achieves comparable results to strong supervised baselines. On Something-Something-V2 and Diving 48, which contain videos relying on temporal dynamics, BEVT outperforms by clear margins all alternative baselines and achieves state-of-the-art performance with a 71.4% and 87.2% Top-1 accuracy respectively.", "comment": " Comments: Updated SOTA results with ImageNet-1k pretrained PeCo tokenizer "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.00804", "id": "2112.00804", "pdf": "https://arxiv.org/pdf/2112.00804", "other": "https://arxiv.org/format/2112.00804"}, "title": "PreViTS: Contrastive Pretraining with Video Tracking Supervision", "author_info": ["Brian Chen", "Ramprasaath R. Selvaraju", "Shih-Fu Chang", "Juan Carlos Niebles", "Nikhil Naik"], "summary": "Videos are a rich source for self-supervised learning (SSL) of visual representations due to the presence of natural temporal transformations of objects. However, current methods typically randomly sample video clips for learning, which results in a poor supervisory signal. In this work, we propose PreViTS, an SSL framework that utilizes an unsupervised tracking signal for selecting clips containing the same object, which helps better utilize temporal transformations of objects. PreViTS further uses the tracking signal to spatially constrain the frame regions to learn from and trains the model to locate meaningful objects by providing supervision on Grad-CAM attention maps. To evaluate our approach, we train a momentum contrastive (MoCo) encoder on VGG-Sound and Kinetics-400 datasets with PreViTS. Training with PreViTS outperforms representations learnt by MoCo alone on both image recognition and video classification downstream tasks, obtaining state-of-the-art performance on action classification. PreViTS helps learn feature representations that are more robust to changes in background and context, as seen by experiments on image and video datasets with background changes. Learning from large-scale uncurated videos with PreViTS could lead to more accurate and robust visual feature representations.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.00496", "id": "2112.00496", "pdf": "https://arxiv.org/pdf/2112.00496", "other": "https://arxiv.org/format/2112.00496"}, "title": "Revisiting the Transferability of Supervised Pretraining: an MLP Perspective", "author_info": ["Yizhou Wang", "Shixiang Tang", "Feng Zhu", "Lei Bai", "Rui Zhao", "Donglian Qi", "Wanli Ouyang"], "summary": "The pretrain-finetune paradigm is a classical pipeline in visual learning. Recent progress on unsupervised pretraining methods shows superior transfer performance to their supervised counterparts. This paper revisits this phenomenon and sheds new light on understanding the transferability gap between unsupervised and supervised pretraining from a multilayer perceptron (MLP) perspective. While previous works focus on the effectiveness of MLP on unsupervised image classification where pretraining and evaluation are conducted on the same dataset, we reveal that the MLP projector is also the key factor to better transferability of unsupervised pretraining methods than supervised pretraining methods. Based on this observation, we attempt to close the transferability gap between supervised and unsupervised pretraining by adding an MLP projector before the classifier in supervised pretraining. Our analysis indicates that the MLP projector can help retain intra-class variation of visual features, decrease the feature distribution distance between pretraining and evaluation datasets, and reduce feature redundancy. Extensive experiments on public benchmarks demonstrate that the added MLP projector significantly boosts the transferability of supervised pretraining, \\eg \\textbf{+7.2\\%} top-1 accuracy on the concept generalization task, \\textbf{+5.8\\%} top-1 accuracy for linear evaluation on 12-domain classification tasks, and \\textbf{+0.8\\%} AP on COCO object detection task, making supervised pretraining comparable or even better than unsupervised pretraining. Codes will be released upon acceptance.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.14564", "id": "2111.14564", "pdf": "https://arxiv.org/pdf/2111.14564", "other": "https://arxiv.org/format/2111.14564"}, "title": "MedRDF: A Robust and Retrain-Less Diagnostic Framework for Medical Pretrained Models Against Adversarial Attack", "author_info": ["Mengting Xu", "Tao Zhang", "Daoqiang Zhang"], "summary": "Deep neural networks are discovered to be non-robust when attacked by imperceptible adversarial examples, which is dangerous for it applied into medical diagnostic system that requires high reliability. However, the defense methods that have good effect in natural images may not be suitable for medical diagnostic tasks. The preprocessing methods (e.g., random resizing, compression) may lead to the loss of the small lesions feature in the medical image. Retraining the network on the augmented data set is also not practical for medical models that have already been deployed online. Accordingly, it is necessary to design an easy-to-deploy and effective defense framework for medical diagnostic tasks. In this paper, we propose a Robust and Retrain-Less Diagnostic Framework for Medical pretrained models against adversarial attack (i.e., MedRDF). It acts on the inference time of the pertained medical model. Specifically, for each test image, MedRDF firstly creates a large number of noisy copies of it, and obtains the output labels of these copies from the pretrained medical diagnostic model. Then, based on the labels of these copies, MedRDF outputs the final robust diagnostic result by majority voting. In addition to the diagnostic result, MedRDF produces the Robust Metric (RM) as the confidence of the result. Therefore, it is convenient and reliable to utilize MedRDF to convert pre-trained non-robust diagnostic models into robust ones. The experimental results on COVID-19 and DermaMNIST datasets verify the effectiveness of our MedRDF in improving the robustness of medical diagnostic models.", "comment": " Comments: TMI under review "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.13675", "id": "2111.13675", "pdf": "https://arxiv.org/pdf/2111.13675", "other": "https://arxiv.org/format/2111.13675"}, "title": "Self-supervised Pretraining with Classification Labels for Temporal Activity Detection", "author_info": ["Kumara Kahatapitiya", "Zhou Ren", "Haoxiang Li", "Zhenyu Wu", "Michael S. Ryoo"], "summary": "Temporal Activity Detection aims to predict activity classes per frame, in contrast to video-level predictions as done in Activity Classification (i.e., Activity Recognition). Due to the expensive frame-level annotations required for detection, the scale of detection datasets is limited. Thus, commonly, previous work on temporal activity detection resorts to fine-tuning a classification model pretrained on large-scale classification datasets (e.g., Kinetics-400). However, such pretrained models are not ideal for downstream detection performance due to the disparity between the pretraining and the downstream fine-tuning tasks. This work proposes a novel self-supervised pretraining method for detection leveraging classification labels to mitigate such disparity by introducing frame-level pseudo labels, multi-action frames, and action segments. We show that the models pretrained with the proposed self-supervised detection task outperform prior work on multiple challenging activity detection benchmarks, including Charades and MultiTHUMOS. Our extensive ablations further provide insights on when and how to use the proposed models for activity detection. Code and models will be released online.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.13138", "id": "2111.13138", "pdf": "https://arxiv.org/pdf/2111.13138", "other": "https://arxiv.org/format/2111.13138"}, "title": "TunBERT: Pretrained Contextualized Text Representation for Tunisian Dialect", "author_info": ["Abir Messaoudi", "Ahmed Cheikhrouhou", "Hatem Haddad", "Nourchene Ferchichi", "Moez BenHajhmida", "Abir Korched", "Malek Naski", "Faten Ghriss", "Amine Kerkeni"], "summary": "Pretrained contextualized text representation models learn an effective representation of a natural language to make it machine understandable. After the breakthrough of the attention mechanism, a new generation of pretrained models have been proposed achieving good performances since the introduction of the Transformer. Bidirectional Encoder Representations from Transformers (BERT) has become the state-of-the-art model for language understanding. Despite their success, most of the available models have been trained on Indo-European languages however similar research for under-represented languages and dialects remains sparse.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.11646", "id": "2111.11646", "pdf": "https://arxiv.org/pdf/2111.11646", "other": "https://arxiv.org/format/2111.11646"}, "title": "CytoImageNet: A large-scale pretraining dataset for bioimage transfer learning", "author_info": ["Stanley Bryan Z. Hua", "Alex X. Lu", "Alan M. Moses"], "summary": "Motivation: In recent years, image-based biological assays have steadily become high-throughput, sparking a need for fast automated methods to extract biologically-meaningful information from hundreds of thousands of images. Taking inspiration from the success of ImageNet, we curate CytoImageNet, a large-scale dataset of openly-sourced and weakly-labeled microscopy images (890K images, 894 classes). Pretraining on CytoImageNet yields features that are competitive to ImageNet features on downstream microscopy classification tasks. We show evidence that CytoImageNet features capture information not available in ImageNet-trained features. The dataset is made available at https://www.kaggle.com/stanleyhua/cytoimagenet.", "comment": " Comments: Accepted paper at NeurIPS 2021 Learning Meaningful Representations for Life (LMRL) Workshop "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.04673", "id": "2111.04673", "pdf": "https://arxiv.org/pdf/2111.04673", "other": "https://arxiv.org/format/2111.04673"}, "title": "Information-Theoretic Bias Assessment Of Learned Representations Of Pretrained Face Recognition", "author_info": ["Jiazhi Li", "Wael Abd-Almageed"], "summary": "As equality issues in the use of face recognition have garnered a lot of attention lately, greater efforts have been made to debiased deep learning models to improve fairness to minorities. However, there is still no clear definition nor sufficient analysis for bias assessment metrics. We propose an information-theoretic, independent bias assessment metric to identify degree of bias against protected demographic attributes from learned representations of pretrained facial recognition systems. Our metric differs from other methods that rely on classification accuracy or examine the differences between ground truth and predicted labels of protected attributes predicted using a shallow network. Also, we argue, theoretically and experimentally, that logits-level loss is not adequate to explain bias since predictors based on neural networks will always find correlations. Further, we present a synthetic dataset that mitigates the issue of insufficient samples in certain cohorts. Lastly, we establish a benchmark metric by presenting advantages in clear discrimination and small variation comparing with other metrics, and evaluate the performance of different debiased models with the proposed metric.", "comment": " Comments: IEEE International Conference on Automatic Face and Gesture Recognition 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.04130", "id": "2111.04130", "pdf": "https://arxiv.org/pdf/2111.04130", "other": "https://arxiv.org/format/2111.04130"}, "title": "NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework", "author_info": ["Xingcheng Yao", "Yanan Zheng", "Xiaocong Yang", "Zhilin Yang"], "summary": "Pretrained language models have become the standard approach for many NLP tasks due to strong performance, but they are very expensive to train. We propose a simple and efficient learning framework, TLM, that does not rely on large-scale pretraining. Given some labeled task data and a large general corpus, TLM uses task data as queries to retrieve a tiny subset of the general corpus and jointly optimizes the task objective and the language modeling objective from scratch. On eight classification datasets in four domains, TLM achieves results better than or similar to pretrained language models (e.g., RoBERTa-Large) while reducing the training FLOPs by two orders of magnitude. With high accuracy and efficiency, we hope TLM will contribute to democratizing NLP and expediting its development.", "comment": " Comments: 13 pages, 5 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.01619", "id": "2111.01619", "pdf": "https://arxiv.org/pdf/2111.01619", "other": "https://arxiv.org/format/2111.01619"}, "title": "StyleGAN of All Trades: Image Manipulation with Only Pretrained StyleGAN", "author_info": ["Min Jin Chong", "Hsin-Ying Lee", "David Forsyth"], "summary": "Recently, StyleGAN has enabled various image manipulation and editing tasks thanks to the high-quality generation and the disentangled latent space. However, additional architectures or task-specific training paradigms are usually required for different tasks. In this work, we take a deeper look at the spatial properties of StyleGAN. We show that with a pretrained StyleGAN along with some operations, without any additional architecture, we can perform comparably to the state-of-the-art methods on various tasks, including image blending, panorama generation, generation from a single image, controllable and local multimodal image to image translation, and attributes transfer. The proposed method is simple, effective, efficient, and applicable to any existing pretrained StyleGAN model.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.01398", "id": "2111.01398", "pdf": "https://arxiv.org/pdf/2111.01398", "other": "https://arxiv.org/format/2111.01398"}, "title": "Integrating Pretrained Language Model for Dialogue Policy Learning", "author_info": ["Hongru Wang", "Huimin Wang", "Zezhong Wang", "Kam-Fai Wong"], "summary": "Reinforcement Learning (RL) has been witnessed its potential for training a dialogue policy agent towards maximizing the accumulated rewards given from users. However, the reward can be very sparse for it is usually only provided at the end of a dialog session, which causes unaffordable interaction requirements for an acceptable dialog agent. Distinguished from many efforts dedicated to optimizing the policy and recovering the reward alternatively which suffers from easily getting stuck in local optima and model collapse, we decompose the adversarial training into two steps: 1) we integrate a pre-trained language model as a discriminator to judge whether the current system action is good enough for the last user action (i.e., \\textit{next action prediction}); 2) the discriminator gives and extra local dense reward to guide the agent's exploration. The experimental result demonstrates that our method significantly improves the complete rate (~4.4\\%) and success rate (~8.0\\%) of the dialogue system.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.01231", "id": "2111.01231", "pdf": "https://arxiv.org/pdf/2111.01231", "other": "https://arxiv.org/format/2111.01231"}, "title": "Switch Point biased Self-Training: Re-purposing Pretrained Models for Code-Switching", "author_info": ["Parul Chopra", "Sai Krishna Rallabandi", "Alan W Black", "Khyathi Raghavi Chandu"], "summary": "Code-switching (CS), a ubiquitous phenomenon due to the ease of communication it offers in multilingual communities still remains an understudied problem in language processing. The primary reasons behind this are: (1) minimal efforts in leveraging large pretrained multilingual models, and (2) the lack of annotated data. The distinguishing case of low performance of multilingual models in CS is the intra-sentence mixing of languages leading to switch points. We first benchmark two sequence labeling tasks -- POS and NER on 4 different language pairs with a suite of pretrained models to identify the problems and select the best performing model, char-BERT, among them (addressing (1)). We then propose a self training method to repurpose the existing pretrained models using a switch-point bias by leveraging unannotated data (addressing (2)). We finally demonstrate that our approach performs well on both tasks by reducing the gap between the switch point performance while retaining the overall performance on two distinct language pairs in both the tasks. Our code is available here: https://github.com/PC09/EMNLP2021-Switch-Point-biased-Self-Training.", "comment": " Comments: Accepted at EMNLP Findings 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.01124", "id": "2111.01124", "pdf": "https://arxiv.org/pdf/2111.01124", "other": "https://arxiv.org/format/2111.01124"}, "title": "When Does Contrastive Learning Preserve Adversarial Robustness from Pretraining to Finetuning?", "author_info": ["Lijie Fan", "Sijia Liu", "Pin-Yu Chen", "Gaoyuan Zhang", "Chuang Gan"], "summary": "Contrastive learning (CL) can learn generalizable feature representations and achieve the state-of-the-art performance of downstream tasks by finetuning a linear classifier on top of it. However, as adversarial robustness becomes vital in image classification, it remains unclear whether or not CL is able to preserve robustness to downstream tasks. The main challenge is that in the self-supervised pretraining + supervised finetuning paradigm, adversarial robustness is easily forgotten due to a learning task mismatch from pretraining to finetuning. We call such a challenge 'cross-task robustness transferability'. To address the above problem, in this paper we revisit and advance CL principles through the lens of robustness enhancement. We show that (1) the design of contrastive views matters: High-frequency components of images are beneficial to improving model robustness; (2) Augmenting CL with pseudo-supervision stimulus (e.g., resorting to feature clustering) helps preserve robustness without forgetting. Equipped with our new designs, we propose AdvCL, a novel adversarial contrastive pretraining framework. We show that AdvCL is able to enhance cross-task robustness transferability without loss of model accuracy and finetuning efficiency. With a thorough experimental study, we demonstrate that AdvCL outperforms the state-of-the-art self-supervised robust learning methods across multiple datasets (CIFAR-10, CIFAR-100, and STL-10) and finetuning schemes (linear evaluation and full model finetuning).", "comment": " Comments: NeurIPS 2021. Code is available at https://github.com/LijieFan/AdvCL "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.00943", "id": "2111.00943", "pdf": "https://arxiv.org/pdf/2111.00943", "other": "https://arxiv.org/format/2111.00943"}, "title": "SVBRDF Recovery From a Single Image With Highlights using a Pretrained Generative Adversarial Network", "author_info": ["Tao Wen", "Beibei Wang", "Lei Zhang", "Jie Guo", "Nicolas Holzschuch"], "summary": "Spatially-varying bi-directional reflectance distribution functions (SVBRDFs) are crucial for designers to incorporate new materials in virtual scenes, making them look more realistic. Reconstruction of SVBRDFs is a long-standing problem. Existing methods either rely on extensive acquisition system or require huge datasets which are nontrivial to acquire. We aim to recover SVBRDFs from a single image, without any datasets. A single image contains incomplete information about the SVBRDF, making the reconstruction task highly ill-posed. It is also difficult to separate between the changes in color that are caused by the material and those caused by the illumination, without the prior knowledge learned from the dataset. In this paper, we use an unsupervised generative adversarial neural network (GAN) to recover SVBRDFs maps with a single image as input. To better separate the effects due to illumination from the effects due to the material, we add the hypothesis that the material is stationary and introduce a new loss function based on Fourier coefficients to enforce this stationarity. For efficiency, we train the network in two stages: reusing a trained model to initialize the SVBRDFs and fine-tune it based on the input image. Our method generates high-quality SVBRDFs maps from a single input photograph, and provides more vivid rendering results compared to previous work. The two-stage training boosts runtime performance, making it 8 times faster than previous work.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.15621", "id": "2110.15621", "pdf": "https://arxiv.org/pdf/2110.15621", "other": "https://arxiv.org/format/2110.15621"}, "title": "MentalBERT: Publicly Available Pretrained Language Models for Mental Healthcare", "author_info": ["Shaoxiong Ji", "Tianlin Zhang", "Luna Ansari", "Jie Fu", "Prayag Tiwari", "Erik Cambria"], "summary": "Mental health is a critical issue in modern society, and mental disorders could sometimes turn to suicidal ideation without adequate treatment. Early detection of mental disorders and suicidal ideation from social content provides a potential way for effective social intervention. Recent advances in pretrained contextualized language representations have promoted the development of several domain-specific pretrained models and facilitated several downstream applications. However, there are no existing pretrained language models for mental healthcare. This paper trains and release two pretrained masked language models, i.e., MentalBERT and MentalRoBERTa, to benefit machine learning for the mental healthcare research community. Besides, we evaluate our trained domain-specific models and several variants of pretrained language models on several mental disorder detection benchmarks and demonstrate that language representations pretrained in the target domain improve the performance of mental health detection tasks.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.13640", "id": "2110.13640", "pdf": "https://arxiv.org/pdf/2110.13640", "other": "https://arxiv.org/format/2110.13640"}, "title": "s2s-ft: Fine-Tuning Pretrained Transformer Encoders for Sequence-to-Sequence Learning", "author_info": ["Hangbo Bao", "Li Dong", "Wenhui Wang", "Nan Yang", "Furu Wei"], "summary": "Pretrained bidirectional Transformers, such as BERT, have achieved significant improvements in a wide variety of language understanding tasks, while it is not straightforward to directly apply them for natural language generation. In this paper, we present a sequence-to-sequence fine-tuning toolkit s2s-ft, which adopts pretrained Transformers for conditional generation tasks. Inspired by UniLM, we implement three sequence-to-sequence fine-tuning algorithms, namely, causal fine-tuning, masked fine-tuning, and pseudo-masked fine-tuning. By leveraging the existing pretrained bidirectional Transformers, experimental results show that s2s-ft achieves strong performance on several benchmarks of abstractive summarization, and question generation. Moreover, we demonstrate that the package s2s-ft supports both monolingual and multilingual NLG tasks. The s2s-ft toolkit is available at https://github.com/microsoft/unilm/tree/master/s2s-ft.", "comment": " Comments: Demo paper for the s2s-ft toolkit: https://github.com/microsoft/unilm/tree/master/s2s-ft "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.13492", "id": "2110.13492", "pdf": "https://arxiv.org/pdf/2110.13492", "other": "https://arxiv.org/format/2110.13492"}, "title": "TUNet: A Block-online Bandwidth Extension Model based on Transformers and Self-supervised Pretraining", "author_info": ["Viet-Anh Nguyen", "Anh H. T. Nguyen", "Andy W. H. Khong"], "summary": "We introduce a block-online variant of the temporal feature-wise linear modulation (TFiLM) model to achieve bandwidth extension. The proposed architecture simplifies the UNet backbone of the TFiLM to reduce inference time and employs an efficient transformer at the bottleneck to alleviate performance degradation. We also utilize self-supervised pretraining and data augmentation to enhance the quality of bandwidth extended signals and reduce the sensitivity with respect to downsampling methods. Experiment results on the VCTK dataset show that the proposed method outperforms several recent baselines in both intrusive and non-intrusive metrics. Pretraining and filter augmentation also help stabilize and enhance the overall performance.", "comment": " Comments: ICASSP 2022 submitted, 5 pages, 4 figures, 3 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.13398", "id": "2110.13398", "pdf": "https://arxiv.org/pdf/2110.13398", "other": "https://arxiv.org/format/2110.13398"}, "title": "Unified Instance and Knowledge Alignment Pretraining for Aspect-based Sentiment Analysis", "author_info": ["Juhua Liu", "Qihuang Zhong", "Liang Ding", "Hua Jin", "Bo Du", "Dacheng Tao"], "summary": "Aspect-based Sentiment Analysis (ABSA) aims to determine the sentiment polarity towards an aspect. Because of the expensive and limited labelled data, the pretraining strategy has become the de-facto standard for ABSA. However, there always exists severe domain shift between the pretraining and downstream ABSA datasets, hindering the effective knowledge transfer when directly finetuning and making the downstream task performs sub-optimal. To mitigate such domain shift, we introduce a unified alignment pretraining framework into the vanilla pretrain-finetune pipeline with both instance- and knowledge-level alignments. Specifically, we first devise a novel coarse-to-fine retrieval sampling approach to select target domain-related instances from the large-scale pretraining dataset, thus aligning the instances between pretraining and target domains (\\textit{First Stage}). Then, we introduce a knowledge guidance-based strategy to further bridge the domain gap at the knowledge level. In practice, we formulate the model pretrained on the sampled instances into a knowledge guidance model and a learner model, respectively. On the target dataset, we design an on-the-fly teacher-student joint fine-tuning approach to progressively transfer the knowledge from the knowledge guidance model to the learner model (\\textit{Second Stage}). Thereby, the learner model can maintain more domain-invariant knowledge when learning new knowledge from the target dataset. In the \\textit{Third Stage,} the learner model is finetuned to better adapt its learned knowledge to the target dataset. Extensive experiments and analyses on several ABSA benchmarks demonstrate the effectiveness and universality of our proposed pretraining framework. Notably, our pretraining framework pushes several strong baseline models up to the new state-of-the-art records. We release our code and models.", "comment": " Comments: Under review "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.13071", "id": "2110.13071", "pdf": "https://arxiv.org/pdf/2110.13071", "other": "https://arxiv.org/format/2110.13071"}, "title": "Unsupervised Source Separation By Steering Pretrained Music Models", "author_info": ["Ethan Manilow", "Patrick O'Reilly", "Prem Seetharaman", "Bryan Pardo"], "summary": "We showcase an unsupervised method that repurposes deep models trained for music generation and music tagging for audio source separation, without any retraining. An audio generation model is conditioned on an input mixture, producing a latent encoding of the audio used to generate audio. This generated audio is fed to a pretrained music tagger that creates source labels. The cross-entropy loss between the tag distribution for the generated audio and a predefined distribution for an isolated source is used to guide gradient ascent in the (unchanging) latent space of the generative model. This system does not update the weights of the generative model or the tagger, and only relies on moving through the generative model's latent space to produce separated sources. We use OpenAI's Jukebox as the pretrained generative model, and we couple it with four kinds of pretrained music taggers (two architectures and two tagging datasets). Experimental results on two source separation datasets, show this approach can produce separation estimates for a wider variety of sources than any tested supervised or unsupervised system. This work points to the vast and heretofore untapped potential of large pretrained music models for audio-to-audio tasks like source separation.", "comment": " Comments: Submitted to ICASSP 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.12010", "id": "2110.12010", "pdf": "https://arxiv.org/pdf/2110.12010", "other": "https://arxiv.org/format/2110.12010"}, "title": "ClimateBert: A Pretrained Language Model for Climate-Related Text", "author_info": ["Nicolas Webersinke", "Mathias Kraus", "Julia Anna Bingler", "Markus Leippold"], "summary": "Over the recent years, large pretrained language models (LM) have revolutionized the field of natural language processing (NLP). However, while pretraining on general language has been shown to work very well for common language, it has been observed that niche language poses problems. In particular, climate-related texts include specific language that common LMs can not represent accurately. We argue that this shortcoming of today's LMs limits the applicability of modern NLP to the broad field of text processing of climate-related texts. As a remedy, we propose ClimateBert, a transformer-based language model that is further pretrained on over 1.6 million paragraphs of climate-related texts, crawled from various sources such as common news, research articles, and climate reporting of companies. We find that ClimateBertleads to a 46% improvement on a masked language model objective which, in turn, leads to lowering error rates by 3.57% to 35.71% for various climate-related downstream tasks like text classification, sentiment analysis, and fact-checking.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.10319", "id": "2110.10319", "pdf": "https://arxiv.org/pdf/2110.10319", "other": "https://arxiv.org/format/2110.10319"}, "title": "LMSOC: An Approach for Socially Sensitive Pretraining", "author_info": ["Vivek Kulkarni", "Shubhanshu Mishra", "Aria Haghighi"], "summary": "While large-scale pretrained language models have been shown to learn effective linguistic representations for many NLP tasks, there remain many real-world contextual aspects of language that current approaches do not capture. For instance, consider a cloze-test \"I enjoyed the ____ game this weekend\": the correct answer depends heavily on where the speaker is from, when the utterance occurred, and the speaker's broader social milieu and preferences. Although language depends heavily on the geographical, temporal, and other social contexts of the speaker, these elements have not been incorporated into modern transformer-based language models. We propose a simple but effective approach to incorporate speaker social context into the learned representations of large-scale language models. Our method first learns dense representations of social contexts using graph representation learning algorithms and then primes language model pretraining with these social context representations. We evaluate our approach on geographically-sensitive language-modeling tasks and show a substantial improvement (more than 100% relative lift on MRR) compared to baselines.", "comment": " MSC Class:           68T50; 68T07                              ACM Class:           I.2.7                "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.10318", "id": "2110.10318", "pdf": "https://arxiv.org/pdf/2110.10318", "other": "https://arxiv.org/format/2110.10318"}, "title": "Improved Multilingual Language Model Pretraining for Social Media Text via Translation Pair Prediction", "author_info": ["Shubhanshu Mishra", "Aria Haghighi"], "summary": "We evaluate a simple approach to improving zero-shot multilingual transfer of mBERT on social media corpus by adding a pretraining task called translation pair prediction (TPP), which predicts whether a pair of cross-lingual texts are a valid translation. Our approach assumes access to translations (exact or approximate) between source-target language pairs, where we fine-tune a model on source language task data and evaluate the model in the target language. In particular, we focus on language pairs where transfer learning is difficult for mBERT: those where source and target languages are different in script, vocabulary, and linguistic typology. We show improvements from TPP pretraining over mBERT alone in zero-shot transfer from English to Hindi, Arabic, and Japanese on two social media tasks: NER (a 37% average relative improvement in F1 across target languages) and sentiment classification (12% relative improvement in F1) on social media text, while also benchmarking on a non-social media task of Universal Dependency POS tagging (6.7% relative improvement in accuracy). Our results are promising given the lack of social media bitext corpus. Our code can be found at: https://github.com/twitter-research/multilingual-alignment-tpp.", "comment": " MSC Class:           68T50; 68T07                              ACM Class:           I.2.7                "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.09930", "id": "2110.09930", "pdf": "https://arxiv.org/pdf/2110.09930", "other": "https://arxiv.org/format/2110.09930"}, "title": "Speech Representation Learning Through Self-supervised Pretraining And Multi-task Finetuning", "author_info": ["Yi-Chen Chen", "Shu-wen Yang", "Cheng-Kuang Lee", "Simon See", "Hung-yi Lee"], "summary": "Speech representation learning plays a vital role in speech processing. Among them, self-supervised learning (SSL) has become an important research direction. It has been shown that an SSL pretraining model can achieve excellent performance in various downstream tasks of speech processing. On the other hand, supervised multi-task learning (MTL) is another representation learning paradigm, which has been proven effective in computer vision (CV) and natural language processing (NLP). However, there is no systematic research on the general representation learning model trained by supervised MTL in speech processing. In this paper, we show that MTL finetuning can further improve SSL pretraining. We analyze the generalizability of supervised MTL finetuning to examine if the speech representation learned by MTL finetuning can generalize to unseen new tasks.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.09456", "id": "2110.09456", "pdf": "https://arxiv.org/pdf/2110.09456", "other": "https://arxiv.org/format/2110.09456"}, "title": "NormFormer: Improved Transformer Pretraining with Extra Normalization", "author_info": ["Sam Shleifer", "Jason Weston", "Myle Ott"], "summary": "During pretraining, the Pre-LayerNorm transformer suffers from a gradient magnitude mismatch: gradients at early layers are much larger than at later layers. These issues can be alleviated by our proposed NormFormer architecture, which adds three normalization operations to each layer: a Layer Norm after self attention, head-wise scaling of self-attention outputs, and a Layer Norm after the first fully connected layer. The extra operations incur negligible compute cost (+0.4% parameter increase), but improve pretraining perplexity and downstream task performance for both causal and masked language models ranging from 125 Million to 2.7 Billion parameters. For example, adding NormFormer on top of our strongest 1.3B parameter baseline can reach equal perplexity 24% faster, or converge 0.27 perplexity better in the same compute budget. This model reaches GPT3-Large (1.3B) zero shot performance 60% faster. For masked language modeling, NormFormer improves fine-tuned GLUE performance by 1.9% on average. Code to train NormFormer models is available in fairseq https://github.com/pytorch/fairseq/tree/main/examples/normformer .", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.08547", "id": "2110.08547", "pdf": "https://arxiv.org/pdf/2110.08547", "other": "https://arxiv.org/format/2110.08547"}, "title": "Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation", "author_info": ["Guanhua Chen", "Shuming Ma", "Yun Chen", "Dongdong Zhang", "Jia Pan", "Wenping Wang", "Furu Wei"], "summary": "This paper demonstrates that multilingual pretraining, a proper fine-tuning method and a large-scale parallel dataset from multiple auxiliary languages are all critical for zero-shot translation, where the NMT model is tested on source languages unseen during supervised training. Following this idea, we present SixT++, a strong many-to-English NMT model that supports 100 source languages but is trained once with a parallel dataset from only six source languages. SixT++ initializes the decoder embedding and the full encoder with XLM-R large, and then trains the encoder and decoder layers with a simple two-stage training strategy. SixT++ achieves impressive performance on many-to-English translation. It significantly outperforms CRISS and m2m-100, two strong multilingual NMT systems, with an average gain of 7.2 and 5.0 BLEU respectively. Additionally, SixT++ offers a set of model parameters that can be further fine-tuned to develop unsupervised NMT models for low-resource languages. With back-translation on monolingual data of low-resource language, it outperforms all current state-of-the-art unsupervised methods on Nepali and Sinhal for both translating into and from English.", "comment": " Comments: Preprint "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.08534", "id": "2110.08534", "pdf": "https://arxiv.org/pdf/2110.08534", "other": "https://arxiv.org/format/2110.08534"}, "title": "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora", "author_info": ["Xisen Jin", "Dejiao Zhang", "Henghui Zhu", "Wei Xiao", "Shang-Wen Li", "Xiaokai Wei", "Andrew Arnold", "Xiang Ren"], "summary": "Pretrained language models (PTLMs) are typically learned over a large, static corpus and further fine-tuned for various downstream tasks. However, when deployed in the real world, a PTLM-based model must deal with data from a new domain that deviates from what the PTLM was initially trained on, or newly emerged data that contains out-of-distribution information. In this paper, we study a lifelong language model pretraining challenge where a PTLM is continually updated so as to adapt to emerging data. Over a domain-incremental research paper stream and a chronologically ordered tweet stream, we incrementally pretrain a PTLM with different continual learning algorithms, and keep track of the downstream task performance (after fine-tuning) to analyze its ability of acquiring new knowledge and preserving learned knowledge. Our experiments show continual learning algorithms improve knowledge preservation, with logit distillation being the most effective approach. We further show that continual pretraining improves generalization when training and testing data of downstream tasks are drawn from different time steps, but do not improve when they are from the same time steps. We believe our problem formulation, methods, and analysis will inspire future studies towards continual pretraining of language models.", "comment": " Comments: 8 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.08455", "id": "2110.08455", "pdf": "https://arxiv.org/pdf/2110.08455", "other": "https://arxiv.org/format/2110.08455"}, "title": "Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey", "author_info": ["Xiaokai Wei", "Shen Wang", "Dejiao Zhang", "Parminder Bhatia", "Andrew Arnold"], "summary": "Pretrained Language Models (PLM) have established a new paradigm through learning informative contextualized representations on large-scale text corpus. This new paradigm has revolutionized the entire field of natural language processing, and set the new state-of-the-art performance for a wide variety of NLP tasks. However, though PLMs could store certain knowledge/facts from training corpus, their knowledge awareness is still far from satisfactory. To address this issue, integrating knowledge into PLMs have recently become a very active research area and a variety of approaches have been developed. In this paper, we provide a comprehensive survey of the literature on this emerging and fast-growing field - Knowledge Enhanced Pretrained Language Models (KE-PLMs). We introduce three taxonomies to categorize existing work. Besides, we also survey the various NLU and NLG applications on which KE-PLM has demonstrated superior performance over vanilla PLMs. Finally, we discuss challenges that face KE-PLMs and also promising directions for future research.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.08443", "id": "2110.08443", "pdf": "https://arxiv.org/pdf/2110.08443", "other": "https://arxiv.org/format/2110.08443"}, "title": "Prix-LM: Pretraining for Multilingual Knowledge Base Construction", "author_info": ["Wenxuan Zhou", "Fangyu Liu", "Ivan Vuli\u0107", "Nigel Collier", "Muhao Chen"], "summary": "Knowledge bases (KBs) contain plenty of structured world and commonsense knowledge. As such, they often complement distributional text-based information and facilitate various downstream tasks. Since their manual construction is resource- and time-intensive, recent efforts have tried leveraging large pretrained language models (PLMs) to generate additional monolingual knowledge facts for KBs. However, such methods have not been attempted for building and enriching multilingual KBs. Besides wider application, such multilingual KBs can provide richer combined knowledge than monolingual (e.g., English) KBs. Knowledge expressed in different languages may be complementary and unequally distributed: this implies that the knowledge available in high-resource languages can be transferred to low-resource ones. To achieve this, it is crucial to represent multilingual knowledge in a shared/unified space. To this end, we propose a unified framework, Prix-LM, for multilingual KB construction and completion. We leverage two types of knowledge, monolingual triples and cross-lingual links, extracted from existing multilingual KBs, and tune a multilingual language encoder XLM-R via a causal language modeling objective. Prix-LM integrates useful multilingual and KB-based factual knowledge into a single model. Experiments on standard entity-related tasks, such as link prediction in multiple languages, cross-lingual entity linking and bilingual lexicon induction, demonstrate its effectiveness, with gains reported over strong task-specialised baselines.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.08241", "id": "2110.08241", "pdf": "https://arxiv.org/pdf/2110.08241", "other": "https://arxiv.org/format/2110.08241"}, "title": "Intent-based Product Collections for E-commerce using Pretrained Language Models", "author_info": ["Hiun Kim", "Jisu Jeong", "Kyung-Min Kim", "Dongjun Lee", "Hyun Dong Lee", "Dongpil Seo", "Jeeseung Han", "Dong Wook Park", "Ji Ae Heo", "Rak Yeong Kim"], "summary": "Building a shopping product collection has been primarily a human job. With the manual efforts of craftsmanship, experts collect related but diverse products with common shopping intent that are effective when displayed together, e.g., backpacks, laptop bags, and messenger bags for freshman bag gifts. Automatically constructing a collection requires an ML system to learn a complex relationship between the customer's intent and the product's attributes. However, there have been challenging points, such as 1) long and complicated intent sentences, 2) rich and diverse product attributes, and 3) a huge semantic gap between them, making the problem difficult. In this paper, we use a pretrained language model (PLM) that leverages textual attributes of web-scale products to make intent-based product collections. Specifically, we train a BERT with triplet loss by setting an intent sentence to an anchor and corresponding products to positive examples. Also, we improve the performance of the model by search-based negative sampling and category-wise positive pair augmentation. Our model significantly outperforms the search-based baseline model for intent-based product matching in offline evaluations. Furthermore, online experimental results on our e-commerce platform show that the PLM-based method can construct collections of products with increased CTR, CVR, and order-diversity compared to expert-crafted collections.", "comment": " Comments: Accepted to IEEE International Workshop on Data Mining for Service (DMS2021) "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.08190", "id": "2110.08190", "pdf": "https://arxiv.org/pdf/2110.08190", "other": "https://arxiv.org/format/2110.08190"}, "title": "Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm", "author_info": ["Shaoyi Huang", "Dongkuan Xu", "Ian E. H. Yen", "Sung-en Chang", "Bingbing Li", "Shiyang Chen", "Mimi Xie", "Hang Liu", "Caiwen Ding"], "summary": "Various pruning approaches have been proposed to reduce the footprint requirements of Transformer-based language models. Conventional wisdom is that pruning reduces the model expressiveness and thus is more likely to underfit than overfit compared to the original model. However, under the trending pretrain-and-finetune paradigm, we argue that pruning increases the risk of overfitting if pruning was performed at the fine-tuning phase, as it increases the amount of information a model needs to learn from the downstream task, resulting in relative data deficiency. In this paper, we aim to address the overfitting issue under the pretrain-and-finetune paradigm to improve pruning performance via progressive knowledge distillation (KD) and sparse pruning. Furthermore, to mitigate the interference between different strategies of learning rate, pruning and distillation, we propose a three-stage learning framework. We show for the first time that reducing the risk of overfitting can help the effectiveness of pruning under the pretrain-and-finetune paradigm. Experiments on multiple datasets of GLUE benchmark show that our method achieves highly competitive pruning performance over the state-of-the-art competitors across different pruning ratio constraints.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.08151", "id": "2110.08151", "pdf": "https://arxiv.org/pdf/2110.08151", "other": "https://arxiv.org/format/2110.08151"}, "title": "mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models", "author_info": ["Ryokan Ri", "Ikuya Yamada", "Yoshimasa Tsuruoka"], "summary": "Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from Wikipedia entities. However, existing methods only exploit entity information in pretraining and do not explicitly use entities in downstream tasks. In this study, we explore the effectiveness of leveraging entity representations for downstream cross-lingual tasks. We train a multilingual language model with 24 languages with entity representations and show the model consistently outperforms word-based pretrained models in various cross-lingual transfer tasks. We also analyze the model and the key insight is that incorporating entity representations into the input allows us to extract more language-agnostic features. We also evaluate the model with a multilingual cloze prompt task with the mLAMA dataset. We show that entity-based prompt elicits correct factual knowledge more likely than using only word representations.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.07919", "id": "2110.07919", "pdf": "https://arxiv.org/pdf/2110.07919", "other": "https://arxiv.org/format/2110.07919"}, "title": "Combining CNNs With Transformer for Multimodal 3D MRI Brain Tumor Segmentation With Self-Supervised Pretraining", "author_info": ["Mariia Dobko", "Danylo-Ivan Kolinko", "Ostap Viniavskyi", "Yurii Yelisieiev"], "summary": "We apply an ensemble of modified TransBTS, nnU-Net, and a combination of both for the segmentation task of the BraTS 2021 challenge. In fact, we change the original architecture of the TransBTS model by adding Squeeze-and-Excitation blocks, an increasing number of CNN layers, replacing positional encoding in Transformer block with a learnable Multilayer Perceptron (MLP) embeddings, which makes Transformer adjustable to any input size during inference. With these modifications, we are able to largely improve TransBTS performance. Inspired by a nnU-Net framework we decided to combine it with our modified TransBTS by changing the architecture inside nnU-Net to our custom model. On the Validation set of BraTS 2021, the ensemble of these approaches achieves 0.8496, 0.8698, 0.9256 Dice score and 15.72, 11.057, 3.374 HD95 for enhancing tumor, tumor core, and whole tumor, correspondingly. Our code is publicly available.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.07143", "id": "2110.07143", "pdf": "https://arxiv.org/pdf/2110.07143", "other": "https://arxiv.org/format/2110.07143"}, "title": "bert2BERT: Towards Reusable Pretrained Language Models", "author_info": ["Cheng Chen", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Yujia Qin", "Fengyu Wang", "Zhi Wang", "Xiao Chen", "Zhiyuan Liu", "Qun Liu"], "summary": "In recent years, researchers tend to pre-train ever-larger language models to explore the upper limit of deep models. However, large language model pre-training costs intensive computational resources and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful. In this paper, we propose bert2BERT, which can effectively transfer the knowledge of an existing smaller pre-trained model (e.g., BERT_BASE) to a large model (e.g., BERT_LARGE) through parameter initialization and significantly improve the pre-training efficiency of the large model. Specifically, we extend the previous function-preserving on Transformer-based language model, and further improve it by proposing advanced knowledge for large model's initialization. In addition, a two-stage pre-training method is proposed to further accelerate the training process. We did extensive experiments on representative PLMs (e.g., BERT and GPT) and demonstrate that (1) our method can save a significant amount of training cost compared with baselines including learning from scratch, StackBERT and MSLT; (2) our method is generic and applicable to different types of pre-trained models. In particular, bert2BERT saves about 45% and 47% computational cost of pre-training BERT_BASE and GPT_BASE by reusing the models of almost their half sizes. The source code will be publicly available upon publication.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.05877", "id": "2110.05877", "pdf": "https://arxiv.org/pdf/2110.05877", "other": "https://arxiv.org/format/2110.05877"}, "title": "OpenHands: Making Sign Language Recognition Accessible with Pose-based Pretrained Models across Languages", "author_info": ["Prem Selvaraj", "Gokul NC", "Pratyush Kumar", "Mitesh Khapra"], "summary": "AI technologies for Natural Languages have made tremendous progress recently. However, commensurate progress has not been made on Sign Languages, in particular, in recognizing signs as individual words or as complete sentences. We introduce OpenHands, a library where we take four key ideas from the NLP community for low-resource languages and apply them to sign languages for word-level recognition. First, we propose using pose extracted through pretrained models as the standard modality of data to reduce training time and enable efficient inference, and we release standardized pose datasets for 6 different sign languages - American, Argentinian, Chinese, Greek, Indian, and Turkish. Second, we train and release checkpoints of 4 pose-based isolated sign language recognition models across all 6 languages, providing baselines and ready checkpoints for deployment. Third, to address the lack of labelled data, we propose self-supervised pretraining on unlabelled data. We curate and release the largest pose-based pretraining dataset on Indian Sign Language (Indian-SL). Fourth, we compare different pretraining strategies and for the first time establish that pretraining is effective for sign language recognition by demonstrating (a) improved fine-tuning performance especially in low-resource settings, and (b) high crosslingual transfer from Indian-SL to few other sign languages. We open-source all models and datasets in OpenHands with a hope that it makes research in sign languages more accessible, available here at https://github.com/AI4Bharat/OpenHands .", "comment": " ACM Class:           I.2.7                "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.05074", "id": "2110.05074", "pdf": "https://arxiv.org/pdf/2110.05074", "other": "https://arxiv.org/format/2110.05074"}, "title": "VTBR: Semantic-based Pretraining for Person Re-Identification", "author_info": ["Suncheng Xiang", "Zirui Zhang", "Mengyuan Guan", "Hao Chen", "Binjie Yan", "Ting Liu", "Yuzhuo Fu"], "summary": "Pretraining is a dominant paradigm in computer vision. Generally, supervised ImageNet pretraining is commonly used to initialize the backbones of person re-identification (Re-ID) models. However, recent works show a surprising result that ImageNet pretraining has limited impacts on Re-ID system due to the large domain gap between ImageNet and person Re-ID data. To seek an alternative to traditional pretraining, we manually construct a diversified FineGPR-C caption dataset for the first time on person Re-ID events. Based on it, we propose a pure semantic-based pretraining approach named VTBR, which uses dense captions to learn visual representations with fewer images. Specifically, we train convolutional networks from scratch on the captions of FineGPR-C dataset, and transfer them to downstream Re-ID tasks. Comprehensive experiments conducted on benchmarks show that our VTBR can achieve competitive performance compared with ImageNet pretraining -- despite using up to 1.4x fewer images, revealing its potential in Re-ID pretraining.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.04590", "id": "2110.04590", "pdf": "https://arxiv.org/pdf/2110.04590", "other": "https://arxiv.org/format/2110.04590"}, "title": "An Exploration of Self-Supervised Pretrained Representations for End-to-End Speech Recognition", "author_info": ["Xuankai Chang", "Takashi Maekaku", "Pengcheng Guo", "Jing Shi", "Yen-Ju Lu", "Aswin Shanmugam Subramanian", "Tianzi Wang", "Shu-wen Yang", "Yu Tsao", "Hung-yi Lee", "Shinji Watanabe"], "summary": "Self-supervised pretraining on speech data has achieved a lot of progress. High-fidelity representation of the speech signal is learned from a lot of untranscribed data and shows promising performance. Recently, there are several works focusing on evaluating the quality of self-supervised pretrained representations on various tasks without domain restriction, e.g. SUPERB. However, such evaluations do not provide a comprehensive comparison among many ASR benchmark corpora. In this paper, we focus on the general applications of pretrained speech representations, on advanced end-to-end automatic speech recognition (E2E-ASR) models. We select several pretrained speech representations and present the experimental results on various open-source and publicly available corpora for E2E-ASR. Without any modification of the back-end model architectures or training strategy, some of the experiments with pretrained representations, e.g., WSJ, WSJ0-2mix with HuBERT, reach or outperform current state-of-the-art (SOTA) recognition performance. Moreover, we further explore more scenarios for whether the pretraining representations are effective, such as the cross-language or overlapped speech. The scripts, configuratons and the trained models have been released in ESPnet to let the community reproduce our experiments and improve them.", "comment": " Comments: To appear in ASRU2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.04541", "id": "2110.04541", "pdf": "https://arxiv.org/pdf/2110.04541", "other": "https://arxiv.org/format/2110.04541"}, "title": "The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design", "author_info": ["Yoav Levine", "Noam Wies", "Daniel Jannai", "Dan Navon", "Yedid Hoshen", "Amnon Shashua"], "summary": "Pretraining Neural Language Models (NLMs) over a large corpus involves chunking the text into training examples, which are contiguous text segments of sizes processable by the neural architecture. We highlight a bias introduced by this common practice: we prove that the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples. This intuitive result has a twofold role. First, it formalizes the motivation behind a broad line of recent successful NLM training heuristics, proposed for the pretraining and fine-tuning stages, which do not necessarily appear related at first glance. Second, our result clearly indicates further improvements to be made in NLM pretraining for the benefit of Natural Language Understanding tasks. As an example, we propose \"kNN-Pretraining\": we show that including semantically related non-neighboring sentences in the same pretraining example yields improved sentence representations and open domain question answering abilities. This theoretically motivated degree of freedom for \"pretraining example design\" indicates new training schemes for self-improving representations.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.03888", "id": "2110.03888", "pdf": "https://arxiv.org/pdf/2110.03888", "other": "https://arxiv.org/format/2110.03888"}, "title": "M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining", "author_info": ["Junyang Lin", "An Yang", "Jinze Bai", "Chang Zhou", "Le Jiang", "Xianyan Jia", "Ang Wang", "Jie Zhang", "Yong Li", "Wei Lin", "Jingren Zhou", "Hongxia Yang"], "summary": "Recent expeditious developments in deep learning algorithms, distributed training, and even hardware design for large models have enabled training extreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of billions or even trillions of parameters. However, under limited resources, extreme-scale model training that requires enormous amounts of computes and memory footprint suffers from frustratingly low efficiency in model convergence. In this paper, we propose a simple training strategy called \"Pseudo-to-Real\" for high-memory-footprint-required large models. Pseudo-to-Real is compatible with large models with architecture of sequential layers. We demonstrate a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the application of Pseudo-to-Real, we also provide a technique, Granular CPU offloading, to manage CPU memory for training large model and maintain high GPU utilities. Fast training of extreme-scale models on a decent amount of resources can bring much smaller carbon footprint and contribute to greener AI.", "comment": " Comments: 14 pages, 4 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.03501", "id": "2110.03501", "pdf": "https://arxiv.org/pdf/2110.03501", "other": "https://arxiv.org/format/2110.03501"}, "title": "Pretrained Language Models are Symbolic Mathematics Solvers too!", "author_info": ["Kimia Noorbakhsh", "Modar Sulaiman", "Mahdi Sharifi", "Kallol Roy", "Pooyan Jamshidi"], "summary": "Solving symbolic mathematics has always been of in the arena of human ingenuity that needs compositional reasoning and recurrence. However, recent studies have shown that large-scale language models such as transformers are universal and surprisingly can be trained as a sequence-to-sequence task to solve complex mathematical equations. These large transformer models need humongous amounts of training data to generalize to unseen symbolic mathematics problems. In this paper, we present a sample efficient way of solving the symbolic tasks by first pretraining the transformer model with language translation and then fine-tuning the pretrained transformer model to solve the downstream task of symbolic mathematics. We achieve comparable accuracy on the integration task with our pretrained model while using around 1.5 orders of magnitude less number of training samples with respect to the state-of-the-art deep learning for symbolic mathematics. The test accuracy on differential equation tasks is considerably lower comparing with integration as they need higher order recursions that are not present in language translations. We pretrain our model with different pairs of language translations. Our results show language bias in solving symbolic mathematics tasks. Finally, we study the robustness of the fine-tuned model on symbolic math tasks against distribution shift, and our approach generalizes better in distribution shift scenarios for the function integration.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.02852", "id": "2110.02852", "pdf": "https://arxiv.org/pdf/2110.02852", "other": "https://arxiv.org/format/2110.02852"}, "title": "Pretrained Transformers for Offensive Language Identification in Tanglish", "author_info": ["Sean Benhur", "Kanchana Sivanraju"], "summary": "This paper describes the system submitted to Dravidian-Codemix-HASOC2021: Hate Speech and Offensive Language Identification in Dravidian Languages (Tamil-English and Malayalam-English). This task aims to identify offensive content in code-mixed comments/posts in Dravidian Languages collected from social media. Our approach utilizes pooling the last layers of pretrained transformer multilingual BERT for this task which helped us achieve rank nine on the leaderboard with a weighted average score of 0.61 for the Tamil-English dataset in subtask B. After the task deadline, we sampled the dataset uniformly and used the MuRIL pretrained model, which helped us achieve a weighted average score of 0.67, the top score in the leaderboard. Furthermore, our approach to utilizing the pretrained models helps reuse our models for the same task with a different dataset. Our code and models are available in https://github.com/seanbenhur/tanglish-offensive-language-identification", "comment": " Comments: Accepted at FIRE 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.02497", "id": "2110.02497", "pdf": "https://arxiv.org/pdf/2110.02497", "other": "https://arxiv.org/format/2110.02497"}, "title": "Pretraining & Reinforcement Learning: Sharpening the Axe Before Cutting the Tree", "author_info": ["Saurav Kadavath", "Samuel Paradis", "Brian Yao"], "summary": "Pretraining is a common technique in deep learning for increasing performance and reducing training time, with promising experimental results in deep reinforcement learning (RL). However, pretraining requires a relevant dataset for training. In this work, we evaluate the effectiveness of pretraining for RL tasks, with and without distracting backgrounds, using both large, publicly available datasets with minimal relevance, as well as case-by-case generated datasets labeled via self-supervision. Results suggest filters learned during training on less relevant datasets render pretraining ineffective, while filters learned during training on the in-distribution datasets reliably reduce RL training time and improve performance after 80k RL training steps. We further investigate, given a limited number of environment steps, how to optimally divide the available steps into pretraining and RL training to maximize RL performance. Our code is available on GitHub", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.02067", "id": "2110.02067", "pdf": "https://arxiv.org/pdf/2110.02067", "other": "https://arxiv.org/format/2110.02067"}, "title": "Teach Me What to Say and I Will Learn What to Pick: Unsupervised Knowledge Selection Through Response Generation with Pretrained Generative Models", "author_info": ["Ehsan Lotfi", "Maxime De Bruyn", "Jeska Buhmann", "Walter Daelemans"], "summary": "Knowledge Grounded Conversation Models (KGCM) are usually based on a selection/retrieval module and a generation module, trained separately or simultaneously, with or without having access to a gold knowledge option. With the introduction of large pre-trained generative models, the selection and generation part have become more and more entangled, shifting the focus towards enhancing knowledge incorporation (from multiple sources) instead of trying to pick the best knowledge option. These approaches however depend on knowledge labels and/or a separate dense retriever for their best performance. In this work we study the unsupervised selection abilities of pre-trained generative models (e.g. BART) and show that by adding a score-and-aggregate module between encoder and decoder, they are capable of learning to pick the proper knowledge through minimising the language modelling loss (i.e. without having access to knowledge labels). Trained as such, our model - K-Mine - shows competitive selection and generation performance against models that benefit from knowledge labels and/or separate dense retriever.", "comment": " Comments: Accepted at ConvAI workshop (EMNLP 2021) "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.01509", "id": "2110.01509", "pdf": "https://arxiv.org/pdf/2110.01509", "other": "https://arxiv.org/format/2110.01509"}, "title": "DeepA2: A Modular Framework for Deep Argument Analysis with Pretrained Neural Text2Text Language Models", "author_info": ["Gregor Betz", "Kyle Richardson"], "summary": "In this paper, we present and implement a multi-dimensional, modular framework for performing deep argument analysis (DeepA2) using current pre-trained language models (PTLMs). ArgumentAnalyst -- a T5 model (Raffel et al. 2020) set up and trained within DeepA2 -- reconstructs argumentative texts, which advance an informal argumentation, as valid arguments: It inserts, e.g., missing premises and conclusions, formalizes inferences, and coherently links the logical reconstruction to the source text. We create a synthetic corpus for deep argument analysis, and evaluate ArgumentAnalyst on this new dataset as well as on existing data, specifically EntailmentBank (Dalvi et al. 2021). Our empirical findings vindicate the overall framework and highlight the advantages of a modular design, in particular its ability to emulate established heuristics (such as hermeneutic cycles), to explore the model's uncertainty, to cope with the plurality of correct solutions (underdetermination), and to exploit higher-order evidence.", "comment": " Comments: A Demo is available at https://huggingface.co/spaces/debatelab/deepa2-demo , the model can be downloaded from https://huggingface.co/debatelab/argument-analyst , and the datasets can be accessed at https://huggingface.co/datasets/debatelab/aaac "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.15300", "id": "2109.15300", "pdf": "https://arxiv.org/pdf/2109.15300", "other": "https://arxiv.org/format/2109.15300"}, "title": "Semi-Supervised Text Classification via Self-Pretraining", "author_info": ["Payam Karisani", "Negin Karisani"], "summary": "We present a neural semi-supervised learning model termed Self-Pretraining. Our model is inspired by the classic self-training algorithm. However, as opposed to self-training, Self-Pretraining is threshold-free, it can potentially update its belief about previously labeled documents, and can cope with the semantic drift problem. Self-Pretraining is iterative and consists of two classifiers. In each iteration, one classifier draws a random set of unlabeled documents and labels them. This set is used to initialize the second classifier, to be further trained by the set of labeled documents. The algorithm proceeds to the next iteration and the classifiers' roles are reversed. To improve the flow of information across the iterations and also to cope with the semantic drift problem, Self-Pretraining employs an iterative distillation process, transfers hypotheses across the iterations, utilizes a two-stage training model, uses an efficient learning rate schedule, and employs a pseudo-label transformation heuristic. We have evaluated our model in three publicly available social media datasets. Our experiments show that Self-Pretraining outperforms the existing state-of-the-art semi-supervised classifiers across multiple settings. Our code is available at https://github.com/p-karisani/self_pretraining.", "comment": " Comments: WSDM 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.15101", "id": "2109.15101", "pdf": "https://arxiv.org/pdf/2109.15101", "other": "https://arxiv.org/format/2109.15101"}, "title": "Compositional generalization in semantic parsing with pretrained transformers", "author_info": ["A. Emin Orhan"], "summary": "Large-scale pretraining instills large amounts of knowledge in deep neural networks. This, in turn, improves the generalization behavior of these models in downstream tasks. What exactly are the limits to the generalization benefits of large-scale pretraining? Here, we report observations from some simple experiments aimed at addressing this question in the context of two semantic parsing tasks involving natural language, SCAN and COGS. We show that language models pretrained exclusively with non-English corpora, or even with programming language corpora, significantly improve out-of-distribution generalization in these benchmarks, compared with models trained from scratch, even though both benchmarks are English-based. This demonstrates the surprisingly broad transferability of pretrained representations and knowledge. Pretraining with a large-scale protein sequence prediction task, on the other hand, mostly deteriorates the generalization performance in SCAN and COGS, suggesting that pretrained representations do not transfer universally and that there are constraints on the similarity between the pretraining and downstream domains for successful transfer. Finally, we show that larger models are harder to train from scratch and their generalization accuracy is lower when trained up to convergence on the relatively small SCAN and COGS datasets, but the benefits of large-scale pretraining become much clearer with larger models.", "comment": " Comments: v2 adds more references, adds more discussion, updates previous work "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.13228", "id": "2109.13228", "pdf": "https://arxiv.org/pdf/2109.13228", "other": "https://arxiv.org/format/2109.13228"}, "title": "PASS: An ImageNet replacement for self-supervised pretraining without humans", "author_info": ["Yuki M. Asano", "Christian Rupprecht", "Andrew Zisserman", "Andrea Vedaldi"], "summary": "Computer vision has long relied on ImageNet and other large datasets of images sampled from the Internet for pretraining models. However, these datasets have ethical and technical shortcomings, such as containing personal information taken without consent, unclear license usage, biases, and, in some cases, even problematic image content. On the other hand, state-of-the-art pretraining is nowadays obtained with unsupervised methods, meaning that labelled datasets such as ImageNet may not be necessary, or perhaps not even optimal, for model pretraining. We thus propose an unlabelled dataset PASS: Pictures without humAns for Self-Supervision. PASS only contains images with CC-BY license and complete attribution metadata, addressing the copyright issue. Most importantly, it contains no images of people at all, and also avoids other types of images that are problematic for data protection or ethics. We show that PASS can be used for pretraining with methods such as MoCo-v2, SwAV and DINO. In the transfer learning setting, it yields similar downstream performances to ImageNet pretraining even on tasks that involve humans, such as human pose estimation. PASS does not make existing datasets obsolete, as for instance it is insufficient for benchmarking. However, it shows that model pretraining is often possible while using safer data, and it also provides the basis for a more robust evaluation of pretraining methods.", "comment": " Comments: Accepted to NeurIPS Track on Datasets and Benchmarks 2021. Webpage: https://www.robots.ox.ac.uk/~vgg/research/pass/ "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.12174", "id": "2109.12174", "pdf": "https://arxiv.org/pdf/2109.12174", "other": "https://arxiv.org/format/2109.12174"}, "title": "Leveraging Pretrained Models for Automatic Summarization of Doctor-Patient Conversations", "author_info": ["Longxiang Zhang", "Renato Negrinho", "Arindam Ghosh", "Vasudevan Jagannathan", "Hamid Reza Hassanzadeh", "Thomas Schaaf", "Matthew R. Gormley"], "summary": "Fine-tuning pretrained models for automatically summarizing doctor-patient conversation transcripts presents many challenges: limited training data, significant domain shift, long and noisy transcripts, and high target summary variability. In this paper, we explore the feasibility of using pretrained transformer models for automatically summarizing doctor-patient conversations directly from transcripts. We show that fluent and adequate summaries can be generated with limited training data by fine-tuning BART on a specially constructed dataset. The resulting models greatly surpass the performance of an average human annotator and the quality of previous published work for the task. We evaluate multiple methods for handling long conversations, comparing them to the obvious baseline of truncating the conversation to fit the pretrained model length limit. We introduce a multistage approach that tackles the task by learning two fine-tuned models: one for summarizing conversation chunks into partial summaries, followed by one for rewriting the collection of partial summaries into a complete summary. Using a carefully chosen fine-tuning dataset, this method is shown to be effective at handling longer conversations, improving the quality of generated summaries. We conduct both an automatic evaluation (through ROUGE and two concept-based metrics focusing on medical findings) and a human evaluation (through qualitative examples from literature, assessing hallucination, generalization, fluency, and general quality of the generated summaries).", "comment": " Comments: Accepted in Findings of the EMNLP 2021. Code is available at https://github.com/negrinho/medical_conversation_summarization "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.12028", "id": "2109.12028", "pdf": "https://arxiv.org/pdf/2109.12028"}, "title": "Investigating Post-pretraining Representation Alignment for Cross-Lingual Question Answering", "author_info": ["Fahim Faisal", "Antonios Anastasopoulos"], "summary": "Human knowledge is collectively encoded in the roughly 6500 languages spoken around the world, but it is not distributed equally across languages. Hence, for information-seeking question answering (QA) systems to adequately serve speakers of all languages, they need to operate cross-lingually. In this work we investigate the capabilities of multilingually pre-trained language models on cross-lingual QA. We find that explicitly aligning the representations across languages with a post-hoc fine-tuning step generally leads to improved performance. We additionally investigate the effect of data size as well as the language choice in this fine-tuning step, also releasing a dataset for evaluating cross-lingual QA systems. Code and dataset are publicly available here: https://github.com/ffaisal93/aligned_qa", "comment": " Comments: Accepted at MRQA Workshop 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.11778", "id": "2109.11778", "pdf": "https://arxiv.org/pdf/2109.11778", "other": "https://arxiv.org/format/2109.11778"}, "title": "Dense Contrastive Visual-Linguistic Pretraining", "author_info": ["Lei Shi", "Kai Shuang", "Shijie Geng", "Peng Gao", "Zuohui Fu", "Gerard de Melo", "Yunpeng Chen", "Sen Su"], "summary": "Inspired by the success of BERT, several multimodal representation learning approaches have been proposed that jointly represent image and text. These approaches achieve superior performance by capturing high-level semantic information from large-scale multimodal pretraining. In particular, LXMERT and UNITER adopt visual region feature regression and label classification as pretext tasks. However, they tend to suffer from the problems of noisy labels and sparse semantic annotations, based on the visual features having been pretrained on a crowdsourced dataset with limited and inconsistent semantic labeling. To overcome these issues, we propose unbiased Dense Contrastive Visual-Linguistic Pretraining (DCVLP), which replaces the region regression and classification with cross-modality region contrastive learning that requires no annotations. Two data augmentation strategies (Mask Perturbation and Intra-/Inter-Adversarial Perturbation) are developed to improve the quality of negative samples used in contrastive learning. Overall, DCVLP allows cross-modality dense region contrastive learning in a self-supervised setting independent of any object annotations. We compare our method against prior visual-linguistic pretraining frameworks to validate the superiority of dense contrastive learning on multimodal representation learning.", "comment": " Comments: Accepted by ACM Multimedia 2021. arXiv admin note: text overlap with arXiv:2007.13135 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.11129", "id": "2109.11129", "pdf": "https://arxiv.org/pdf/2109.11129", "other": "https://arxiv.org/format/2109.11129"}, "title": "Cross-Lingual Language Model Meta-Pretraining", "author_info": ["Zewen Chi", "Heyan Huang", "Luyang Liu", "Yu Bai", "Xian-Ling Mao"], "summary": "The success of pretrained cross-lingual language models relies on two essential abilities, i.e., generalization ability for learning downstream tasks in a source language, and cross-lingual transferability for transferring the task knowledge to other languages. However, current methods jointly learn the two abilities in a single-phase cross-lingual pretraining process, resulting in a trade-off between generalization and cross-lingual transfer. In this paper, we propose cross-lingual language model meta-pretraining, which learns the two abilities in different training phases. Our method introduces an additional meta-pretraining phase before cross-lingual pretraining, where the model learns generalization ability on a large-scale monolingual corpus. Then, the model focuses on learning cross-lingual transfer on a multilingual corpus. Experimental results show that our method improves both generalization and cross-lingual transfer, and produces better-aligned representations across different languages.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2109.10540", "id": "2109.10540", "pdf": "https://arxiv.org/pdf/2109.10540", "other": "https://arxiv.org/format/2109.10540"}, "title": "Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing", "author_info": ["Qian Liu", "Dejian Yang", "Jiahui Zhang", "Jiaqi Guo", "Bin Zhou", "Jian-Guang Lou"], "summary": "Recent years pretrained language models (PLMs) hit a success on several downstream tasks, showing their power on modeling language. To better understand and leverage what PLMs have learned, several techniques have emerged to explore syntactic structures entailed by PLMs. However, few efforts have been made to explore grounding capabilities of PLMs, which are also essential. In this paper, we highlight the ability of PLMs to discover which token should be grounded to which concept, if combined with our proposed erasing-then-awakening approach. Empirical studies on four datasets demonstrate that our approach can awaken latent grounding which is understandable to human experts, even if it is not exposed to such labels during training. More importantly, our approach shows great potential to benefit downstream semantic parsing models. Taking text-to-SQL as a case study, we successfully couple our approach with two off-the-shelf parsers, obtaining an absolute improvement of up to 9.8%.", "comment": " Comments: Accepted by ACL 2021 Findings. The first three authors contributed equally "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.10504", "id": "2109.10504", "pdf": "https://arxiv.org/pdf/2109.10504", "other": "https://arxiv.org/format/2109.10504"}, "title": "KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation", "author_info": ["Yongfei Liu", "Chenfei Wu", "Shao-yen Tseng", "Vasudev Lal", "Xuming He", "Nan Duan"], "summary": "Self-supervised vision-and-language pretraining (VLP) aims to learn transferable multi-modal representations from large-scale image-text data and to achieve strong performances on a broad scope of vision-language tasks after finetuning. Previous mainstream VLP approaches typically adopt a two-step strategy relying on external object detectors to encode images in a multi-modal Transformer framework, which suffer from restrictive object concept space, limited image context and inefficient computation. In this paper, we propose an object-aware end-to-end VLP framework, which directly feeds image grid features from CNNs into the Transformer and learns the multi-modal representations jointly. More importantly, we propose to perform object knowledge distillation to facilitate learning cross-modal alignment at different semantic levels. To achieve that, we design two novel pretext tasks by taking object features and their semantic labels from external detectors as supervision: 1.) Object-guided masked vision modeling task focuses on enforcing object-aware representation learning in the multi-modal Transformer; 2.) Phrase-region alignment task aims to improve cross-modal alignment by utilizing the similarities between noun phrases and object labels in the linguistic space. Extensive experiments on a wide range of vision-language tasks demonstrate the efficacy of our proposed framework, and we achieve competitive or superior performances over the existing pretraining strategies. The code is available in supplementary materials.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2109.10246", "id": "2109.10246", "pdf": "https://arxiv.org/pdf/2109.10246", "other": "https://arxiv.org/format/2109.10246"}, "title": "Does Vision-and-Language Pretraining Improve Lexical Grounding?", "author_info": ["Tian Yun", "Chen Sun", "Ellie Pavlick"], "summary": "Linguistic representations derived from text alone have been criticized for their lack of grounding, i.e., connecting words to their meanings in the physical world. Vision-and-Language (VL) models, trained jointly on text and image or video data, have been offered as a response to such criticisms. However, while VL pretraining has shown success on multimodal tasks such as visual question answering, it is not yet known how the internal linguistic representations themselves compare to their text-only counterparts. This paper compares the semantic representations learned via VL vs. text-only pretraining for two recent VL models using a suite of analyses (clustering, probing, and performance on a commonsense question answering task) in a language-only setting. We find that the multimodal models fail to significantly outperform the text-only variants, suggesting that future work is required if multimodal pretraining is to be pursued as a means of improving NLP in general.", "comment": " Comments: Camera ready for Findings of EMNLP 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.10126", "id": "2109.10126", "pdf": "https://arxiv.org/pdf/2109.10126", "other": "https://arxiv.org/format/2109.10126"}, "title": "ConvFiT: Conversational Fine-Tuning of Pretrained Language Models", "author_info": ["Ivan Vuli\u0107", "Pei-Hao Su", "Sam Coope", "Daniela Gerz", "Pawe\u0142 Budzianowski", "I\u00f1igo Casanueva", "Nikola Mrk\u0161i\u0107", "Tsung-Hsien Wen"], "summary": "Transformer-based language models (LMs) pretrained on large text collections are proven to store a wealth of semantic knowledge. However, 1) they are not effective as sentence encoders when used off-the-shelf, and 2) thus typically lag behind conversationally pretrained (e.g., via response selection) encoders on conversational tasks such as intent detection (ID). In this work, we propose ConvFiT, a simple and efficient two-stage procedure which turns any pretrained LM into a universal conversational encoder (after Stage 1 ConvFiT-ing) and task-specialised sentence encoder (after Stage 2). We demonstrate that 1) full-blown conversational pretraining is not required, and that LMs can be quickly transformed into effective conversational encoders with much smaller amounts of unannotated data; 2) pretrained LMs can be fine-tuned into task-specialised sentence encoders, optimised for the fine-grained semantics of a particular task. Consequently, such specialised sentence encoders allow for treating ID as a simple semantic similarity task based on interpretable nearest neighbours retrieval. We validate the robustness and versatility of the ConvFiT framework with such similarity-based inference on the standard ID evaluation sets: ConvFiT-ed LMs achieve state-of-the-art ID performance across the board, with particular gains in the most challenging, few-shot setups.", "comment": " Comments: EMNLP 2021 (long paper) "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.10052", "id": "2109.10052", "pdf": "https://arxiv.org/pdf/2109.10052", "other": "https://arxiv.org/format/2109.10052"}, "title": "Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?", "author_info": ["Rochelle Choenni", "Ekaterina Shutova", "Robert van Rooij"], "summary": "In this paper, we investigate what types of stereotypical information are captured by pretrained language models. We present the first dataset comprising stereotypical attributes of a range of social groups and propose a method to elicit stereotypes encoded by pretrained language models in an unsupervised fashion. Moreover, we link the emergent stereotypes to their manifestation as basic emotions as a means to study their emotional effects in a more generalized manner. To demonstrate how our methods can be used to analyze emotion and stereotype shifts due to linguistic experience, we use fine-tuning on news sources as a case study. Our experiments expose how attitudes towards different social groups vary across models and how quickly emotions and stereotypes can shift at the fine-tuning stage.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2109.09237", "id": "2109.09237", "pdf": "https://arxiv.org/pdf/2109.09237", "other": "https://arxiv.org/format/2109.09237"}, "title": "MirrorWiC: On Eliciting Word-in-Context Representations from Pretrained Language Models", "author_info": ["Qianchu Liu", "Fangyu Liu", "Nigel Collier", "Anna Korhonen", "Ivan Vuli\u0107"], "summary": "Recent work indicated that pretrained language models (PLMs) such as BERT and RoBERTa can be transformed into effective sentence and word encoders even via simple self-supervised techniques. Inspired by this line of work, in this paper we propose a fully unsupervised approach to improving word-in-context (WiC) representations in PLMs, achieved via a simple and efficient WiC-targeted fine-tuning procedure: MirrorWiC. The proposed method leverages only raw texts sampled from Wikipedia, assuming no sense-annotated data, and learns context-aware word representations within a standard contrastive learning setup. We experiment with a series of standard and comprehensive WiC benchmarks across multiple languages. Our proposed fully unsupervised MirrorWiC models obtain substantial gains over off-the-shelf PLMs across all monolingual, multilingual and cross-lingual setups. Moreover, on some standard WiC benchmarks, MirrorWiC is even on-par with supervised models fine-tuned with in-task data and sense labels.", "comment": " Comments: CoNLL 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.07953", "id": "2109.07953", "pdf": "https://arxiv.org/pdf/2109.07953", "other": "https://arxiv.org/format/2109.07953"}, "title": "Efficient Attribute Injection for Pretrained Language Models", "author_info": ["Reinald Kim Amplayo", "Kang Min Yoo", "Sang-Woo Lee"], "summary": "Metadata attributes (e.g., user and product IDs from reviews) can be incorporated as additional inputs to neural-based NLP models, by modifying the architecture of the models, in order to improve their performance. Recent models however rely on pretrained language models (PLMs), where previously used techniques for attribute injection are either nontrivial or ineffective. In this paper, we propose a lightweight and memory-efficient method to inject attributes to PLMs. We extend adapters, i.e. tiny plug-in feed-forward modules, to include attributes both independently of or jointly with the text. To limit the increase of parameters especially when the attribute vocabulary is large, we use low-rank approximations and hypercomplex multiplications, significantly decreasing the total parameters. We also introduce training mechanisms to handle domains in which attributes can be multi-labeled or sparse. Extensive experiments and analyses on eight datasets from different domains show that our method outperforms previous attribute injection methods and achieves state-of-the-art performance on various datasets.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2109.07323", "id": "2109.07323", "pdf": "https://arxiv.org/pdf/2109.07323", "other": "https://arxiv.org/format/2109.07323"}, "title": "FORTAP: Using Formulae for Numerical-Reasoning-Aware Table Pretraining", "author_info": ["Zhoujun Cheng", "Haoyu Dong", "Fan Cheng", "Ran Jia", "Pengfei Wu", "Shi Han", "Dongmei Zhang"], "summary": "Tables store rich numerical data, but numerical reasoning over tables is still a challenge. In this paper, we find that the spreadsheet formula, which performs calculations on numerical values in tables, is naturally a strong supervision of numerical reasoning. More importantly, large amounts of spreadsheets with expert-made formulae are available on the web and can be obtained easily. FORTAP is the first method for numerical-reasoning-aware table pretraining by leveraging large corpus of spreadsheet formulae. We design two formula pretraining tasks to explicitly guide FORTAP to learn numerical reference and calculation in semi-structured tables. FORTAP achieves state-of-the-art results on two representative downstream tasks, cell type classification and formula prediction, showing great potential of numerical-reasoning-aware pretraining.", "comment": " Comments: Work in progress "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.06605", "id": "2109.06605", "pdf": "https://arxiv.org/pdf/2109.06605", "other": "https://arxiv.org/format/2109.06605"}, "title": "MDAPT: Multilingual Domain Adaptive Pretraining in a Single Model", "author_info": ["Rasmus K\u00e6r J\u00f8rgensen", "Mareike Hartmann", "Xiang Dai", "Desmond Elliott"], "summary": "Domain adaptive pretraining, i.e. the continued unsupervised pretraining of a language model on domain-specific text, improves the modelling of text for downstream tasks within the domain. Numerous real-world applications are based on domain-specific text, e.g. working with financial or biomedical documents, and these applications often need to support multiple languages. However, large-scale domain-specific multilingual pretraining data for such scenarios can be difficult to obtain, due to regulations, legislation, or simply a lack of language- and domain-specific text. One solution is to train a single multilingual model, taking advantage of the data available in as many languages as possible. In this work, we explore the benefits of domain adaptive pretraining with a focus on adapting to multiple languages within a specific domain. We propose different techniques to compose pretraining corpora that enable a language model to both become domain-specific and multilingual. Evaluation on nine domain-specific datasets-for biomedical named entity recognition and financial sentence classification-covering seven different languages show that a single multilingual domain-specific model can outperform the general multilingual model, and performs close to its monolingual counterpart. This finding holds across two different pretraining methods, adapter-based pretraining and full model pretraining.", "comment": " Comments: Findings of EMNLP 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.04953", "id": "2109.04953", "pdf": "https://arxiv.org/pdf/2109.04953", "other": "https://arxiv.org/format/2109.04953"}, "title": "Does Pretraining for Summarization Require Knowledge Transfer?", "author_info": ["Kundan Krishna", "Jeffrey Bigham", "Zachary C. Lipton"], "summary": "Pretraining techniques leveraging enormous datasets have driven recent advances in text summarization. While folk explanations suggest that knowledge transfer accounts for pretraining's benefits, little is known about why it works or what makes a pretraining task or dataset suitable. In this paper, we challenge the knowledge transfer story, showing that pretraining on documents consisting of character n-grams selected at random, we can nearly match the performance of models pretrained on real corpora. This work holds the promise of eliminating upstream corpora, which may alleviate some concerns over offensive language, bias, and copyright issues. To see whether the small residual benefit of using real data could be accounted for by the structure of the pretraining task, we design several tasks motivated by a qualitative study of summarization corpora. However, these tasks confer no appreciable benefit, leaving open the possibility of a small role for knowledge transfer.", "comment": " Comments: Camera-ready for Findings of EMNLP 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.04715", "id": "2109.04715", "pdf": "https://arxiv.org/pdf/2109.04715", "other": "https://arxiv.org/format/2109.04715"}, "title": "AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages", "author_info": ["Machel Reid", "Junjie Hu", "Graham Neubig", "Yutaka Matsuo"], "summary": "Reproducible benchmarks are crucial in driving progress of machine translation research. However, existing machine translation benchmarks have been mostly limited to high-resource or well-represented languages. Despite an increasing interest in low-resource machine translation, there are no standardized reproducible benchmarks for many African languages, many of which are used by millions of speakers but have less digitized textual data. To tackle these challenges, we propose AfroMT, a standardized, clean, and reproducible machine translation benchmark for eight widely spoken African languages. We also develop a suite of analysis tools for system diagnosis taking into account the unique properties of these languages. Furthermore, we explore the newly considered case of low-resource focused pretraining and develop two novel data augmentation-based strategies, leveraging word-level alignment information and pseudo-monolingual data for pretraining multilingual sequence-to-sequence models. We demonstrate significant improvements when pretraining on 11 languages, with gains of up to 2 BLEU points over strong baselines. We also show gains of up to 12 BLEU points over cross-lingual transfer baselines in data-constrained scenarios. All code and pretrained models will be released as further steps towards larger reproducible benchmarks for African languages.", "comment": " Comments: EMNLP 2021 "}]}