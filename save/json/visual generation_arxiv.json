{"data": [{"arxiv": {"page": "https://arxiv.org/abs/2202.09979", "id": "2202.09979", "pdf": "https://arxiv.org/pdf/2202.09979", "other": "https://arxiv.org/format/2202.09979"}, "title": "Audio Visual Scene-Aware Dialog Generation with Transformer-based Video Representations", "author_info": ["Yoshihiro Yamazaki", "Shota Orihashi", "Ryo Masumura", "Mihiro Uchida", "Akihiko Takashima"], "summary": "There have been many attempts to build multimodal dialog systems that can respond to a question about given audio-visual information, and the representative task for such systems is the Audio Visual Scene-Aware Dialog (AVSD). Most conventional AVSD models adopt the Convolutional Neural Network (CNN)-based video feature extractor to understand visual information. While a CNN tends to obtain both temporally and spatially local information, global information is also crucial for boosting video understanding because AVSD requires long-term temporal visual dependency and whole visual information. In this study, we apply the Transformer-based video feature that can capture both temporally and spatially global representations more efficiently than the CNN-based feature. Our AVSD model with its Transformer-based feature attains higher objective performance scores for answer generation. In addition, our model achieves a subjective score close to that of human answers in DSTC10. We observed that the Transformer-based visual feature is beneficial for the AVSD task because our model tends to correctly answer the questions that need a temporally and spatially broad range of visual information.", "comment": " Comments: Accepted at DSTC10 Workshop at AAAI 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06464", "id": "2202.06464", "pdf": "https://arxiv.org/pdf/2202.06464", "other": "https://arxiv.org/format/2202.06464"}, "title": "Learn by Challenging Yourself: Contrastive Visual Representation Learning with Hard Sample Generation", "author_info": ["Yawen Wu", "Zhepeng Wang", "Dewen Zeng", "Yiyu Shi", "Jingtong Hu"], "summary": "Contrastive learning (CL), a self-supervised learning approach, can effectively learn visual representations from unlabeled data. However, CL requires learning on vast quantities of diverse data to achieve good performance, without which the performance of CL will greatly degrade. To tackle this problem, we propose a framework with two approaches to improve the data efficiency of CL training by generating beneficial samples and joint learning. The first approach generates hard samples for the main model. The generator is jointly learned with the main model to dynamically customize hard samples based on the training state of the main model. With the progressively growing knowledge of the main model, the generated samples also become harder to constantly encourage the main model to learn better representations. Besides, a pair of data generators are proposed to generate similar but distinct samples as positive pairs. In joint learning, the hardness of a positive pair is progressively increased by decreasing their similarity. In this way, the main model learns to cluster hard positives by pulling the representations of similar yet distinct samples together, by which the representations of similar samples are well-clustered and better representations can be learned. Comprehensive experiments show superior accuracy and data efficiency of the proposed methods over the state-of-the-art on multiple datasets. For example, about 5% accuracy improvement on ImageNet-100 and CIFAR-10, and more than 6% accuracy improvement on CIFAR-100 are achieved for linear classification. Besides, up to 2x data efficiency for linear classification and up to 5x data efficiency for transfer learning are achieved.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05331", "id": "2202.05331", "pdf": "https://arxiv.org/pdf/2202.05331", "other": "https://arxiv.org/format/2202.05331"}, "title": "Describing image focused in cognitive and visual details for visually impaired people: An approach to generating inclusive paragraphs", "author_info": ["Daniel Louzada Fernandes", "Marcos Henrique Fonseca Ribeiro", "Fabio Ribeiro Cerqueira", "Michel Melo Silva"], "summary": "Several services for people with visual disabilities have emerged recently due to achievements in Assistive Technologies and Artificial Intelligence areas. Despite the growth in assistive systems availability, there is a lack of services that support specific tasks, such as understanding the image context presented in online content, e.g., webinars. Image captioning techniques and their variants are limited as Assistive Technologies as they do not match the needs of visually impaired people when generating specific descriptions. We propose an approach for generating context of webinar images combining a dense captioning technique with a set of filters, to fit the captions in our domain, and a language model for the abstractive summary task. The results demonstrated that we can produce descriptions with higher interpretability and focused on the relevant information for that group of people by combining image analysis methods and neural language models.", "comment": " Comments: Accepted in the 17th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISAPP) 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03697", "id": "2202.03697", "pdf": "https://arxiv.org/pdf/2202.03697", "other": "https://arxiv.org/format/2202.03697"}, "title": "DURableVS: Data-efficient Unsupervised Recalibrating Visual Servoing via online learning in a structured generative model", "author_info": ["Nishad Gothoskar", "Miguel L\u00e1zaro-Gredilla", "Yasemin Bekiroglu", "Abhishek Agarwal", "Joshua B. Tenenbaum", "Vikash K. Mansinghka", "Dileep George"], "summary": "Visual servoing enables robotic systems to perform accurate closed-loop control, which is required in many applications. However, existing methods either require precise calibration of the robot kinematic model and cameras or use neural architectures that require large amounts of data to train. In this work, we present a method for unsupervised learning of visual servoing that does not require any prior calibration and is extremely data-efficient. Our key insight is that visual servoing does not depend on identifying the veridical kinematic and camera parameters, but instead only on an accurate generative model of image feature observations from the joint positions of the robot. We demonstrate that with our model architecture and learning algorithm, we can consistently learn accurate models from less than 50 training samples (which amounts to less than 1 min of unsupervised data collection), and that such data-efficient learning is not possible with standard neural architectures. Further, we show that by using the generative model in the loop and learning online, we can enable a robotic system to recover from calibration errors and to detect and quickly adapt to possibly unexpected changes in the robot-camera system (e.g. bumped camera, new objects).", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.12723", "id": "2201.12723", "pdf": "https://arxiv.org/pdf/2201.12723", "other": "https://arxiv.org/format/2201.12723"}, "title": "VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training", "author_info": ["Ziyang Luo", "Yadong Xi", "Rongsheng Zhang", "Jing Ma"], "summary": "Vision-and-language pre-trained models (VLMs) have achieved tremendous success in the cross-modal area, but most of them require a large amount of parallel image-caption data for pre-training. Collating such data is expensive and labor-intensive. In this work, we focus on reducing such need for generative vision-and-language pre-training (G-VLP) by taking advantage of the visual pre-trained model (CLIP-ViT) as encoder and language pre-trained model (GPT2) as decoder. Unfortunately, GPT2 lacks a necessary cross-attention module, which hinders the direct connection of CLIP-ViT and GPT2. To remedy such defects, we conduct extensive experiments to empirically investigate how to design and pre-train our model. Based on our experimental results, we propose a novel G-VLP framework, Visual Conditioned GPT (VC-GPT), and pre-train it with a small-scale image-caption corpus (Visual Genome, only 110k distinct images). Evaluating on the image captioning downstream tasks (MSCOCO and Flickr30k Captioning), VC-GPT achieves either the best or the second-best performance across all evaluation metrics over the previous works which consume around 30 times more distinct images during cross-modal pre-training.", "comment": " Comments: Work in progress "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11975", "id": "2201.11975", "pdf": "https://arxiv.org/pdf/2201.11975", "other": "https://arxiv.org/format/2201.11975"}, "title": "Generalized Visual Quality Assessment of GAN-Generated Face Images", "author_info": ["Yu Tian", "Zhangkai Ni", "Baoliang Chen", "Shiqi Wang", "Hanli Wang", "Sam Kwong"], "summary": "Recent years have witnessed the dramatically increased interest in face generation with generative adversarial networks (GANs). A number of successful GAN algorithms have been developed to produce vivid face images towards different application scenarios. However, little work has been dedicated to automatic quality assessment of such GAN-generated face images (GFIs), even less have been devoted to generalized and robust quality assessment of GFIs generated with unseen GAN model. Herein, we make the first attempt to study the subjective and objective quality towards generalized quality assessment of GFIs. More specifically, we establish a large-scale database consisting of GFIs from four GAN algorithms, the pseudo labels from image quality assessment (IQA) measures, as well as the human opinion scores via subjective testing. Subsequently, we develop a quality assessment model that is able to deliver accurate quality predictions for GFIs from both available and unseen GAN algorithms based on meta-learning. In particular, to learn shared knowledge from GFIs pairs that are born of limited GAN algorithms, we develop the convolutional block attention (CBA) and facial attributes-based analysis (ABA) modules, ensuring that the learned knowledge tends to be consistent with human visual perception. Extensive experiments exhibit that the proposed model achieves better performance compared with the state-of-the-art IQA models, and is capable of retaining the effectiveness when evaluating GFIs from the unseen GAN algorithms.", "comment": " Comments: 12 pages, 8 figures, journal paper "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11316", "id": "2201.11316", "pdf": "https://arxiv.org/pdf/2201.11316", "other": "https://arxiv.org/format/2201.11316"}, "title": "Transformer Module Networks for Systematic Generalization in Visual Question Answering", "author_info": ["Moyuru Yamada", "Vanessa D'Amario", "Kentaro Takemoto", "Xavier Boix", "Tomotake Sasaki"], "summary": "Transformer-based models achieve great performance on Visual Question Answering (VQA). However, when we evaluate them on systematic generalization, i.e., handling novel combinations of known concepts, their performance degrades. Neural Module Networks (NMNs) are a promising approach for systematic generalization that consists on composing modules, i.e., neural networks that tackle a sub-task. Inspired by Transformers and NMNs, we propose Transformer Module Network (TMN), a novel Transformer-based model for VQA that dynamically composes modules into a question-specific Transformer network. TMNs achieve state-of-the-art systematic generalization performance in three VQA datasets, namely, CLEVR-CoGenT, CLOSURE and GQA-SGL, in some cases improving more than 30% over standard Transformers.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.09107", "id": "2201.09107", "pdf": "https://arxiv.org/pdf/2201.09107", "other": "https://arxiv.org/format/2201.09107"}, "title": "Visual Information Guided Zero-Shot Paraphrase Generation", "author_info": ["Zhe Lin", "Xiaojun Wan"], "summary": "Zero-shot paraphrase generation has drawn much attention as the large-scale high-quality paraphrase corpus is limited. Back-translation, also known as the pivot-based method, is typical to this end. Several works leverage different information as \"pivot\" such as language, semantic representation and so on. In this paper, we explore using visual information such as image as the \"pivot\" of back-translation. Different with the pipeline back-translation method, we propose visual information guided zero-shot paraphrase generation (ViPG) based only on paired image-caption data. It jointly trains an image captioning model and a paraphrasing model and leverage the image captioning model to guide the training of the paraphrasing model. Both automatic evaluation and human evaluation show our model can generate paraphrase with good relevancy, fluency and diversity, and image is a promising kind of pivot for zero-shot paraphrase generation.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.05664", "id": "2201.05664", "pdf": "https://arxiv.org/pdf/2201.05664", "other": "https://arxiv.org/format/2201.05664"}, "title": "Demonstration of PI2: Interactive Visualization Interface Generation for SQL Analysis in Notebook", "author_info": ["Jeffrey Tao", "Yiru Chen", "Eugene Wu"], "summary": "We demonstrate PI2, the first notebook extension that can automatically generate interactive visualization interfaces during SQL-based analyses.", "comment": " ACM Class:           H.2; H.5.2                "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.13906", "id": "2112.13906", "pdf": "https://arxiv.org/pdf/2112.13906", "other": "https://arxiv.org/format/2112.13906"}, "title": "Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?", "author_info": ["Sedigheh Eslami", "Gerard de Melo", "Christoph Meinel"], "summary": "Contrastive Language--Image Pre-training (CLIP) has shown remarkable success in learning with cross-modal supervision from extensive amounts of image--text pairs collected online. Thus far, the effectiveness of CLIP has been investigated primarily in general-domain multimodal problems. This work evaluates the effectiveness of CLIP for the task of Medical Visual Question Answering (MedVQA). To this end, we present PubMedCLIP, a fine-tuned version of CLIP for the medical domain based on PubMed articles. Our experiments are conducted on two MedVQA benchmark datasets and investigate two MedVQA methods, MEVF (Mixture of Enhanced Visual Features) and QCR (Question answering via Conditional Reasoning). For each of these, we assess the merits of visual representation learning using PubMedCLIP, the original CLIP, and state-of-the-art MAML (Model-Agnostic Meta-Learning) networks pre-trained only on visual data. We open source the code for our MedVQA pipeline and pre-training PubMedCLIP. CLIP and PubMedCLIP achieve improvements in comparison to MAML's visual encoder. PubMedCLIP achieves the best results with gains in the overall accuracy of up to 3%. Individual examples illustrate the strengths of PubMedCLIP in comparison to the previously widely used MAML networks. Visual representation learning with language supervision in PubMedCLIP leads to noticeable improvements for MedVQA. Our experiments reveal distributional differences in the two MedVQA benchmark datasets that have not been imparted in previous work and cause different back-end visual encoders in PubMedCLIP to exhibit different behavior on these datasets. Moreover, we witness fundamental performance differences of VQA in general versus medical domains.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.13243", "id": "2112.13243", "pdf": "https://arxiv.org/pdf/2112.13243", "other": "https://arxiv.org/format/2112.13243"}, "title": "Evolutionary Generation of Visual Motion Illusions", "author_info": ["Lana Sinapayen", "Eiji Watanabe"], "summary": "Why do we sometimes perceive static images as if they were moving? Visual motion illusions enjoy a sustained popularity, yet there is no definitive answer to the question of why they work. We present a generative model, the Evolutionary Illusion GENerator (EIGen), that creates new visual motion illusions. The structure of EIGen supports the hypothesis that illusory motion might be the result of perceiving the brain's own predictions rather than perceiving raw visual input from the eyes. The scientific motivation of this paper is to demonstrate that the perception of illusory motion could be a side effect of the predictive abilities of the brain. The philosophical motivation of this paper is to call attention to the untapped potential of \"motivated failures\", ways for artificial systems to fail as biological systems fail, as a worthy outlet for Artificial Intelligence and Artificial Life research.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.13207", "id": "2112.13207", "pdf": "https://arxiv.org/pdf/2112.13207", "other": "https://arxiv.org/format/2112.13207"}, "title": "FMViz: Visualizing Tests Generated by AFL at the Byte-level", "author_info": ["Aftab Hussain", "Mohammad Amin Alipour"], "summary": "Software fuzzing is a strong testing technique that has become the de facto approach for automated software testing and software vulnerability detection in the industry. The random nature of fuzzing makes monitoring and understanding the behavior of fuzzers difficult. In this paper, we report the development of Fuzzer Mutation Visualizer (FMViz), a tool that focuses on visualizing byte-level mutations in fuzzers. In particular, FMViz extends American Fuzzy Lop (AFL) to visualize the generated test inputs and highlight changes between consecutively generated seeds as a fuzzing campaign progresses. The overarching goal of our tool is to help developers and students comprehend the inner-workings of the AFL fuzzer better. In this paper, we present the architecture of FMViz, discuss a sample case study of it, and outline the future work. FMViz is open-source and publicly available at https://github.com/AftabHussain/afl-test-viz.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.03245", "id": "2112.03245", "pdf": "https://arxiv.org/pdf/2112.03245", "other": "https://arxiv.org/format/2112.03245"}, "title": "GAM Changer: Editing Generalized Additive Models with Interactive Visualization", "author_info": ["Zijie J. Wang", "Alex Kale", "Harsha Nori", "Peter Stella", "Mark Nunnally", "Duen Horng Chau", "Mihaela Vorvoreanu", "Jennifer Wortman Vaughan", "Rich Caruana"], "summary": "Recent strides in interpretable machine learning (ML) research reveal that models exploit undesirable patterns in the data to make predictions, which potentially causes harms in deployment. However, it is unclear how we can fix these models. We present our ongoing work, GAM Changer, an open-source interactive system to help data scientists and domain experts easily and responsibly edit their Generalized Additive Models (GAMs). With novel visualization techniques, our tool puts interpretability into action -- empowering human users to analyze, validate, and align model behaviors with their knowledge and values. Built using modern web technologies, our tool runs locally in users' computational notebooks or web browsers without requiring extra compute resources, lowering the barrier to creating more responsible ML models. GAM Changer is available at https://interpret.ml/gam-changer.", "comment": " Comments: 7 pages, 15 figures, accepted to the Research2Clinics workshop at NeurIPS 2021. For a demo video, see https://youtu.be/2gVSoPoSeJ8. For a live demo, visit https://interpret.ml/gam-changer/ "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.03109", "id": "2112.03109", "pdf": "https://arxiv.org/pdf/2112.03109", "other": "https://arxiv.org/format/2112.03109"}, "title": "General Facial Representation Learning in a Visual-Linguistic Manner", "author_info": ["Yinglin Zheng", "Hao Yang", "Ting Zhang", "Jianmin Bao", "Dongdong Chen", "Yangyu Huang", "Lu Yuan", "Dong Chen", "Ming Zeng", "Fang Wen"], "summary": "How to learn a universal facial representation that boosts all face analysis tasks? This paper takes one step toward this goal. In this paper, we study the transfer performance of pre-trained models on face analysis tasks and introduce a framework, called FaRL, for general Facial Representation Learning in a visual-linguistic manner. On one hand, the framework involves a contrastive loss to learn high-level semantic meaning from image-text pairs. On the other hand, we propose exploring low-level information simultaneously to further enhance the face representation, by adding a masked image modeling. We perform pre-training on LAION-FACE, a dataset containing large amount of face image-text pairs, and evaluate the representation capability on multiple downstream tasks. We show that FaRL achieves better transfer performance compared with previous pre-trained models. We also verify its superiority in the low-data regime. More importantly, our model surpasses the state-of-the-art methods on face analysis tasks including face parsing and face alignment.", "comment": " Comments: 15 pages, 5 figures, 12 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.02749", "id": "2112.02749", "pdf": "https://arxiv.org/pdf/2112.02749", "other": "https://arxiv.org/format/2112.02749"}, "title": "One-shot Talking Face Generation from Single-speaker Audio-Visual Correlation Learning", "author_info": ["Suzhen Wang", "Lincheng Li", "Yu Ding", "Xin Yu"], "summary": "Audio-driven one-shot talking face generation methods are usually trained on video resources of various persons. However, their created videos often suffer unnatural mouth shapes and asynchronous lips because those methods struggle to learn a consistent speech style from different speakers. We observe that it would be much easier to learn a consistent speech style from a specific speaker, which leads to authentic mouth movements. Hence, we propose a novel one-shot talking face generation framework by exploring consistent correlations between audio and visual motions from a specific speaker and then transferring audio-driven motion fields to a reference image. Specifically, we develop an Audio-Visual Correlation Transformer (AVCT) that aims to infer talking motions represented by keypoint based dense motion fields from an input audio. In particular, considering audio may come from different identities in deployment, we incorporate phonemes to represent audio signals. In this manner, our AVCT can inherently generalize to audio spoken by other identities. Moreover, as face keypoints are used to represent speakers, AVCT is agnostic against appearances of the training speaker, and thus allows us to manipulate face images of different identities readily. Considering different face shapes lead to different motions, a motion field transfer module is exploited to reduce the audio-driven dense motion field gap between the training identity and the one-shot reference. Once we obtained the dense motion field of the reference image, we employ an image renderer to generate its talking face videos from an audio clip. Thanks to our learned consistent speaking style, our method generates authentic mouth shapes and vivid movements. Extensive experiments demonstrate that our synthesized videos outperform the state-of-the-art in terms of visual quality and lip-sync.", "comment": " Journal ref:         AAAI 2022       "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.14447", "id": "2111.14447", "pdf": "https://arxiv.org/pdf/2111.14447", "other": "https://arxiv.org/format/2111.14447"}, "title": "Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic", "author_info": ["Yoad Tewel", "Yoav Shalev", "Idan Schwartz", "Lior Wolf"], "summary": "Recent text-to-image matching models apply contrastive learning to large corpora of uncurated pairs of images and sentences. While such models can provide a powerful score for matching and subsequent zero-shot tasks, they are not capable of generating caption given an image. In this work, we repurpose such models to generate a descriptive text given an image at inference time, without any further training or tuning step. This is done by combining the visual-semantic model with a large language model, benefiting from the knowledge in both web-scale models. The resulting captions are much less restrictive than those obtained by supervised captioning methods. Moreover, as a zero-shot learning method, it is extremely flexible and we demonstrate its ability to perform image arithmetic in which the inputs can be either images or text and the output is a sentence. This enables novel high-level vision capabilities such as comparing two images or solving visual analogy tests.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.13163", "id": "2111.13163", "pdf": "https://arxiv.org/pdf/2111.13163", "other": "https://arxiv.org/format/2111.13163"}, "title": "Semantic-Aware Generation for Self-Supervised Visual Representation Learning", "author_info": ["Yunjie Tian", "Lingxi Xie", "Xiaopeng Zhang", "Jiemin Fang", "Haohang Xu", "Wei Huang", "Jianbin Jiao", "Qi Tian", "Qixiang Ye"], "summary": "In this paper, we propose a self-supervised visual representation learning approach which involves both generative and discriminative proxies, where we focus on the former part by requiring the target network to recover the original image based on the mid-level features. Different from prior work that mostly focuses on pixel-level similarity between the original and generated images, we advocate for Semantic-aware Generation (SaGe) to facilitate richer semantics rather than details to be preserved in the generated image. The core idea of implementing SaGe is to use an evaluator, a deep network that is pre-trained without labels, for extracting semantic-aware features. SaGe complements the target network with view-specific features and thus alleviates the semantic degradation brought by intensive data augmentations. We execute SaGe on ImageNet-1K and evaluate the pre-trained models on five downstream tasks including nearest neighbor test, linear classification, and fine-scaled image recognition, demonstrating its ability to learn stronger visual representations.", "comment": " Comments: 13 pages, 5 figures, 11 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.03481", "id": "2111.03481", "pdf": "https://arxiv.org/pdf/2111.03481", "other": "https://arxiv.org/format/2111.03481"}, "title": "Improving Visual Quality of Image Synthesis by A Token-based Generator with Transformers", "author_info": ["Yanhong Zeng", "Huan Yang", "Hongyang Chao", "Jianbo Wang", "Jianlong Fu"], "summary": "We present a new perspective of achieving image synthesis by viewing this task as a visual token generation problem. Different from existing paradigms that directly synthesize a full image from a single input (e.g., a latent code), the new formulation enables a flexible local manipulation for different image regions, which makes it possible to learn content-aware and fine-grained style control for image synthesis. Specifically, it takes as input a sequence of latent tokens to predict the visual tokens for synthesizing an image. Under this perspective, we propose a token-based generator (i.e.,TokenGAN). Particularly, the TokenGAN inputs two semantically different visual tokens, i.e., the learned constant content tokens and the style tokens from the latent space. Given a sequence of style tokens, the TokenGAN is able to control the image synthesis by assigning the styles to the content tokens by attention mechanism with a Transformer. We conduct extensive experiments and show that the proposed TokenGAN has achieved state-of-the-art results on several widely-used image synthesis benchmarks, including FFHQ and LSUN CHURCH with different resolutions. In particular, the generator is able to synthesize high-fidelity images with 1024x1024 size, dispensing with convolutions entirely.", "comment": " Comments: NeurIPS 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.01127", "id": "2111.01127", "pdf": "https://arxiv.org/pdf/2111.01127", "other": "https://arxiv.org/format/2111.01127"}, "title": "NSS-VAEs: Generative Scene Decomposition for Visual Navigable Space Construction", "author_info": ["Zheng Chen", "Lantao Liu"], "summary": "Detecting navigable space is the first and also a critical step for successful robot navigation. In this work, we treat the visual navigable space segmentation as a scene decomposition problem and propose a new network, NSS-VAEs (Navigable Space Segmentation Variational AutoEncoders), a representation-learning-based framework to enable robots to learn the navigable space segmentation in an unsupervised manner. Different from prevalent segmentation techniques which heavily rely on supervised learning strategies and typically demand immense pixel-level annotated images, the proposed framework leverages a generative model - Variational Auto-Encoder (VAE) - to learn a probabilistic polyline representation that compactly outlines the desired navigable space boundary. Uniquely, our method also assesses the prediction uncertainty related to the unstructuredness of the scenes, which is important for robot navigation in unstructured environments. Through extensive experiments, we have validated that our proposed method can achieve remarkably high accuracy (>90%) even without a single label. We also show that the prediction of NSS-VAEs can be further improved using few labels with results significantly outperforming the SOTA fully supervised-learning-based method.", "comment": " Comments: arXiv admin note: substantial text overlap with arXiv:2111.00063 "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.00116", "id": "2111.00116", "pdf": "https://arxiv.org/pdf/2111.00116", "other": "https://arxiv.org/format/2111.00116"}, "title": "Visual Explanations for Convolutional Neural Networks via Latent Traversal of Generative Adversarial Networks", "author_info": ["Amil Dravid", "Aggelos K. Katsaggelos"], "summary": "Lack of explainability in artificial intelligence, specifically deep neural networks, remains a bottleneck for implementing models in practice. Popular techniques such as Gradient-weighted Class Activation Mapping (Grad-CAM) provide a coarse map of salient features in an image, which rarely tells the whole story of what a convolutional neural network (CNN) learned. Using COVID-19 chest X-rays, we present a method for interpreting what a CNN has learned by utilizing Generative Adversarial Networks (GANs). Our GAN framework disentangles lung structure from COVID-19 features. Using this GAN, we can visualize the transition of a pair of COVID negative lungs in a chest radiograph to a COVID positive pair by interpolating in the latent space of the GAN, which provides fine-grained visualization of how the CNN responds to varying features within the lungs.", "comment": " ACM Class:           I.5.4; I.5.1; I.4.9; I.2.10                "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.00063", "id": "2111.00063", "pdf": "https://arxiv.org/pdf/2111.00063", "other": "https://arxiv.org/format/2111.00063"}, "title": "Polyline Based Generative Navigable Space Segmentation for Autonomous Visual Navigation", "author_info": ["Zheng Chen", "Zhengming Ding", "David Crandall", "Lantao Liu"], "summary": "Detecting navigable space is a fundamental capability for mobile robots navigating in unknown or unmapped environments. In this work, we treat the visual navigable space segmentation as a scene decomposition problem and propose Polyline Segmentation Variational AutoEncoder Networks (PSV-Nets), a representation-learning-based framework to enable robots to learn the navigable space segmentation in an unsupervised manner. Current segmentation techniques heavily rely on supervised learning strategies which demand a large amount of pixel-level annotated images. In contrast, the proposed framework leverages a generative model - Variational AutoEncoder (VAE) and an AutoEncoder (AE) to learn a polyline representation that compactly outlines the desired navigable space boundary in an unsupervised way. We also propose a visual receding horizon planning method that uses the learned navigable space and a Scaled Euclidean Distance Field (SEDF) to achieve autonomous navigation without an explicit map. Through extensive experiments, we have validated that the proposed PSV-Nets can learn the visual navigable space with high accuracy, even without any single label. We also show that the prediction of the PSV-Nets can be further improved with a small number of labels (if available) and can significantly outperform the state-of-the-art fully supervised-learning-based segmentation methods.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.14810", "id": "2110.14810", "pdf": "https://arxiv.org/pdf/2110.14810", "other": "https://arxiv.org/format/2110.14810"}, "title": "Telling Creative Stories Using Generative Visual Aids", "author_info": ["Safinah Ali", "Devi Parikh"], "summary": "Can visual artworks created using generative visual algorithms inspire human creativity in storytelling? We asked writers to write creative stories from a starting prompt, and provided them with visuals created by generative AI models from the same prompt. Compared to a control group, writers who used the visuals as story writing aid wrote significantly more creative, original, complete and visualizable stories, and found the task more fun. Of the generative algorithms used (BigGAN, VQGAN, DALL-E, CLIPDraw), VQGAN was the most preferred. The control group that did not view the visuals did significantly better in integrating the starting prompts. Findings indicate that cross modality inputs by AI can benefit divergent aspects of creativity in human-AI co-creation, but hinders convergent thinking.", "comment": " Comments: Accepted in the Machine Learning for Creativity and Design Workshop at NeurIPS 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.14063", "id": "2110.14063", "pdf": "https://arxiv.org/pdf/2110.14063", "other": "https://arxiv.org/format/2110.14063"}, "title": "Visualization of sphere and horosphere packings related to Coxeter tilings generated by simply truncated orthoschemes with parallel faces", "author_info": ["Arnasli Yahya", "Jen\u0151 Szirmai"], "summary": "In this paper, we describe and visualize the densest ball and horoball packing configurations belonging to the simply truncated 3-dimensional hyperbolic Coxeter orthoschemes with parallel faces. These beautiful packing arrangements describe and show the very interesting structure of the mentioned orthoschemes and the corresponding Coxeter groups. We use for the visualization the Beltrami-Cayley-Klein ball model of 3-dimensional hyperbolic space H3 and the pictures were made by the Python software.", "comment": " MSC Class:           52C17; 52C22; 52B15                              ACM Class:           G.0; G.m                "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.12536", "id": "2110.12536", "pdf": "https://arxiv.org/pdf/2110.12536", "other": "https://arxiv.org/format/2110.12536"}, "title": "Neo: Generalizing Confusion Matrix Visualization to Hierarchical and Multi-Output Labels", "author_info": ["Jochen G\u00f6rtler", "Fred Hohman", "Dominik Moritz", "Kanit Wongsuphasawat", "Donghao Ren", "Rahul Nair", "Marc Kirchner", "Kayur Patel"], "summary": "The confusion matrix, a ubiquitous visualization for helping people evaluate machine learning models, is a tabular layout that compares predicted class labels against actual class labels over all data instances. We conduct formative research with machine learning practitioners at Apple and find that conventional confusion matrices do not support more complex data-structures found in modern-day applications, such as hierarchical and multi-output labels. To express such variations of confusion matrices, we design an algebra that models confusion matrices as probability distributions. Based on this algebra, we develop Neo, a visual analytics system that enables practitioners to flexibly author and interact with hierarchical and multi-output confusion matrices, visualize derived metrics, renormalize confusions, and share matrix specifications. Finally, we demonstrate Neo's utility with three model evaluation scenarios that help people better understand model performance and reveal hidden confusions.", "comment": " ACM Class:           H.2.m; I.7.m                "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.10366", "id": "2110.10366", "pdf": "https://arxiv.org/pdf/2110.10366", "other": "https://arxiv.org/format/2110.10366"}, "title": "Repaint: Improving the Generalization of Down-Stream Visual Tasks by Generating Multiple Instances of Training Examples", "author_info": ["Amin Banitalebi-Dehkordi", "Yong Zhang"], "summary": "Convolutional Neural Networks (CNNs) for visual tasks are believed to learn both the low-level textures and high-level object attributes, throughout the network depth. This paper further investigates the `texture bias' in CNNs. To this end, we regenerate multiple instances of training examples from each original image, through a process we call `repainting'. The repainted examples preserve the shape and structure of the regions and objects within the scenes, but diversify their texture and color. Our method can regenerate a same image at different daylight, season, or weather conditions, can have colorization or de-colorization effects, or even bring back some texture information from blacked-out areas. The in-place repaint allows us to further use these repainted examples for improving the generalization of CNNs. Through an extensive set of experiments, we demonstrate the usefulness of the repainted examples in training, for the tasks of image classification (ImageNet) and object detection (COCO), over several state-of-the-art network architectures at different capacities, and across different data availability regimes.", "comment": " Comments: BMVC 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.10101", "id": "2110.10101", "pdf": "https://arxiv.org/pdf/2110.10101", "other": "https://arxiv.org/format/2110.10101"}, "title": "Domain Generalization through Audio-Visual Relative Norm Alignment in First Person Action Recognition", "author_info": ["Mirco Planamente", "Chiara Plizzari", "Emanuele Alberti", "Barbara Caputo"], "summary": "First person action recognition is becoming an increasingly researched area thanks to the rising popularity of wearable cameras. This is bringing to light cross-domain issues that are yet to be addressed in this context. Indeed, the information extracted from learned representations suffers from an intrinsic \"environmental bias\". This strongly affects the ability to generalize to unseen scenarios, limiting the application of current methods to real settings where labeled data are not available during training. In this work, we introduce the first domain generalization approach for egocentric activity recognition, by proposing a new audio-visual loss, called Relative Norm Alignment loss. It re-balances the contributions from the two modalities during training, over different domains, by aligning their feature norm representations. Our approach leads to strong results in domain generalization on both EPIC-Kitchens-55 and EPIC-Kitchens-100, as demonstrated by extensive experiments, and can be extended to work also on domain adaptation settings with competitive results.", "comment": " Comments: Accepted at WACV 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.09893", "id": "2110.09893", "pdf": "https://arxiv.org/pdf/2110.09893"}, "title": "Visualizing Collective Idea Generation and Innovation Processes in Social Networks", "author_info": ["Yiding Cao", "Yingjun Dong", "Minjun Kim", "Neil G. MacLaren", "Sriniwas Pandey", "Shelley D. Dionne", "Francis J. Yammarino", "Hiroki Sayama"], "summary": "Collective idea generation and innovation processes are complex and dynamic, involving a large amount of qualitative narrative information that is difficult to monitor, analyze and visualize using traditional methods. In this study, we developed three new visualization methods for collective idea generation and innovation processes and applied them to data from online collaboration experiments. The first visualization is the Idea Cloud, which helps monitor collective idea posting activity and intuitively tracks idea clustering and transition. The second visualization is the Idea Geography, which helps understand how the idea space and its utility landscape are structured and how collaboration was performed in that space. The third visualization is the Idea Network, which connects idea dynamics with the social structure of the people who generated them, displaying how social influence among neighbors may have affected collaborative activities and where innovative ideas arose and spread in the social network.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.08791", "id": "2110.08791", "pdf": "https://arxiv.org/pdf/2110.08791", "other": "https://arxiv.org/format/2110.08791"}, "title": "Taming Visually Guided Sound Generation", "author_info": ["Vladimir Iashin", "Esa Rahtu"], "summary": "Recent advances in visually-induced audio generation are based on sampling short, low-fidelity, and one-class sounds. Moreover, sampling 1 second of audio from the state-of-the-art model takes minutes on a high-end GPU. In this work, we propose a single model capable of generating visually relevant, high-fidelity sounds prompted with a set of frames from open-domain videos in less time than it takes to play it on a single GPU.", "comment": " Comments: Accepted as an oral presentation for the BMVC 2021. Code: https://github.com/v-iashin/SpecVQGAN Project page: https://v-iashin.github.io/SpecVQGAN "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.08226", "id": "2110.08226", "pdf": "https://arxiv.org/pdf/2110.08226", "other": "https://arxiv.org/format/2110.08226"}, "title": "Guiding Visual Question Generation", "author_info": ["Nihir Vedd", "Zixu Wang", "Marek Rei", "Yishu Miao", "Lucia Specia"], "summary": "In traditional Visual Question Generation (VQG), most images have multiple concepts (e.g. objects and categories) for which a question could be generated, but models are trained to mimic an arbitrary choice of concept as given in their training data. This makes training difficult and also poses issues for evaluation -- multiple valid questions exist for most images but only one or a few are captured by the human references. We present Guiding Visual Question Generation - a variant of VQG which conditions the question generator on categorical information based on expectations on the type of question and the objects it should explore. We propose two variants: (i) an explicitly guided model that enables an actor (human or automated) to select which objects and categories to generate a question for; and (ii) an implicitly guided model that learns which objects and categories to condition on, based on discrete latent variables. The proposed models are evaluated on an answer-category augmented VQA dataset and our quantitative results show a substantial improvement over the current state of the art (over 9 BLEU-4 increase). Human evaluation validates that guidance helps the generation of questions that are grammatically coherent and relevant to the given image and objects.", "comment": " Comments: 14 pages including references and Appendix. 3 figures and 4 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.03223", "id": "2110.03223", "pdf": "https://arxiv.org/pdf/2110.03223"}, "title": "Goal-Directed Design Agents: Integrating Visual Imitation with One-Step Lookahead Optimization for Generative Design", "author_info": ["Ayush Raina", "Lucas Puentes", "Jonathan Cagan", "Christopher McComb"], "summary": "Engineering design problems often involve large state and action spaces along with highly sparse rewards. Since an exhaustive search of those spaces is not feasible, humans utilize relevant domain knowledge to condense the search space. Previously, deep learning agents (DLAgents) were introduced to use visual imitation learning to model design domain knowledge. This note builds on DLAgents and integrates them with one-step lookahead search to develop goal-directed agents capable of enhancing learned strategies for sequentially generating designs. Goal-directed DLAgents can employ human strategies learned from data along with optimizing an objective function. The visual imitation network from DLAgents is composed of a convolutional encoder-decoder network, acting as a rough planning step that is agnostic to feedback. Meanwhile, the lookahead search identifies the fine-tuned design action guided by an objective. These design agents are trained on an unconstrained truss design problem that is modeled as a sequential, action-based configuration design problem. The agents are then evaluated on two versions of the problem: the original version used for training and an unseen constrained version with an obstructed construction space. The goal-directed agents outperform the human designers used to train the network as well as the previous objective-agnostic versions of the agent in both scenarios. This illustrates a design agent framework that can efficiently use feedback to not only enhance learned design strategies but also adapt to unseen design problems.", "comment": " Journal ref:         J. Mech. Des. Dec 2021, 143(12): 124501 (6 pages)       "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.12761", "id": "2109.12761", "pdf": "https://arxiv.org/pdf/2109.12761", "other": "https://arxiv.org/format/2109.12761"}, "title": "OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts", "author_info": ["Shuhe Wang", "Yuxian Meng", "Xiaoya Li", "Xiaofei Sun", "Rongbin Ouyang", "Jiwei Li"], "summary": "In order to better simulate the real human conversation process, models need to generate dialogue utterances based on not only preceding textual contexts but also visual contexts. However, with the development of multi-modal dialogue learning, the dataset scale gradually becomes a bottleneck. In this report, we release OpenViDial 2.0, a larger-scale open-domain multi-modal dialogue dataset compared to the previous version OpenViDial 1.0. OpenViDial 2.0 contains a total number of 5.6 million dialogue turns extracted from either movies or TV series from different resources, and each dialogue turn is paired with its corresponding visual context. We hope this large-scale dataset can help facilitate future researches on open-domain multi-modal dialog generation, e.g., multi-modal pretraining for dialogue generation.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2109.11439", "id": "2109.11439", "pdf": "https://arxiv.org/pdf/2109.11439", "other": "https://arxiv.org/format/2109.11439"}, "title": "DeepRare: Generic Unsupervised Visual Attention Models", "author_info": ["Phutphalla Kong", "Matei Mancas", "Bernard Gosselin", "Kimtho Po"], "summary": "Human visual system is modeled in engineering field providing feature-engineered methods which detect contrasted/surprising/unusual data into images. This data is \"interesting\" for humans and leads to numerous applications. Deep learning (DNNs) drastically improved the algorithms efficiency on the main benchmark datasets. However, DNN-based models are counter-intuitive: surprising or unusual data is by definition difficult to learn because of its low occurrence probability. In reality, DNN-based models mainly learn top-down features such as faces, text, people, or animals which usually attract human attention, but they have low efficiency in extracting surprising or unusual data in the images. In this paper, we propose a new visual attention model called DeepRare2021 (DR21) which uses the power of DNNs feature extraction and the genericity of feature-engineered algorithms. This algorithm is an evolution of a previous version called DeepRare2019 (DR19) based on a common framework. DR21 1) does not need any training and uses the default ImageNet training, 2) is fast even on CPU, 3) is tested on four very different eye-tracking datasets showing that the DR21 is generic and is always in the within the top models on all datasets and metrics while no other model exhibits such a regularity and genericity. Finally DR21 4) is tested with several network architectures such as VGG16 (V16), VGG19 (V19) and MobileNetV2 (MN2) and 5) it provides explanation and transparency on which parts of the image are the most surprising at different levels despite the use of a DNN-based feature extractor. DeepRare2021 code can be found at https://github.com/numediart/VisualAttention-RareFamil}.", "comment": " Comments: Submited to journal, 36 pages. arXiv admin note: substantial text overlap with arXiv:2005.12073 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.10613", "id": "2109.10613", "pdf": "https://arxiv.org/pdf/2109.10613", "other": "https://arxiv.org/format/2109.10613"}, "title": "COVR: A test-bed for Visually Grounded Compositional Generalization with real images", "author_info": ["Ben Bogin", "Shivanshu Gupta", "Matt Gardner", "Jonathan Berant"], "summary": "While interest in models that generalize at test time to new compositions has risen in recent years, benchmarks in the visually-grounded domain have thus far been restricted to synthetic images. In this work, we propose COVR, a new test-bed for visually-grounded compositional generalization with real images. To create COVR, we use real images annotated with scene graphs, and propose an almost fully automatic procedure for generating question-answer pairs along with a set of context images. COVR focuses on questions that require complex reasoning, including higher-order operations such as quantification and aggregation. Due to the automatic generation process, COVR facilitates the creation of compositional splits, where models at test time need to generalize to new concepts and compositions in a zero- or few-shot setting. We construct compositional splits using COVR and demonstrate a myriad of cases where state-of-the-art pre-trained language-and-vision models struggle to compositionally generalize.", "comment": " Comments: EMNLP 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.08478", "id": "2109.08478", "pdf": "https://arxiv.org/pdf/2109.08478", "other": "https://arxiv.org/format/2109.08478"}, "title": "Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation", "author_info": ["Feilong Chen", "Fandong Meng", "Xiuyi Chen", "Peng Li", "Jie Zhou"], "summary": "Visual dialogue is a challenging task since it needs to answer a series of coherent questions on the basis of understanding the visual environment. Previous studies focus on the implicit exploration of multimodal co-reference by implicitly attending to spatial image features or object-level image features but neglect the importance of locating the objects explicitly in the visual content, which is associated with entities in the textual content. Therefore, in this paper we propose a {\\bf M}ultimodal {\\bf I}ncremental {\\bf T}ransformer with {\\bf V}isual {\\bf G}rounding, named MITVG, which consists of two key parts: visual grounding and multimodal incremental transformer. Visual grounding aims to explicitly locate related objects in the image guided by textual entities, which helps the model exclude the visual content that does not need attention. On the basis of visual grounding, the multimodal incremental transformer encodes the multi-turn dialogue history combined with visual scene step by step according to the order of the dialogue and then generates a contextually and visually coherent response. Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate the superiority of the proposed model, which achieves comparable performance.", "comment": " Comments: ACL Fingdings 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.05778", "id": "2109.05778", "pdf": "https://arxiv.org/pdf/2109.05778", "other": "https://arxiv.org/format/2109.05778"}, "title": "Text is NOT Enough: Integrating Visual Impressions into Open-domain Dialogue Generation", "author_info": ["Lei Shen", "Haolan Zhan", "Xin Shen", "Yonghao Song", "Xiaofang Zhao"], "summary": "Open-domain dialogue generation in natural language processing (NLP) is by default a pure-language task, which aims to satisfy human need for daily communication on open-ended topics by producing related and informative responses. In this paper, we point out that hidden images, named as visual impressions (VIs), can be explored from the text-only data to enhance dialogue understanding and help generate better responses. Besides, the semantic dependency between an dialogue post and its response is complicated, e.g., few word alignments and some topic transitions. Therefore, the visual impressions of them are not shared, and it is more reasonable to integrate the response visual impressions (RVIs) into the decoder, rather than the post visual impressions (PVIs). However, both the response and its RVIs are not given directly in the test process. To handle the above issues, we propose a framework to explicitly construct VIs based on pure-language dialogue datasets and utilize them for better dialogue understanding and generation. Specifically, we obtain a group of images (PVIs) for each post based on a pre-trained word-image mapping model. These PVIs are used in a co-attention encoder to get a post representation with both visual and textual information. Since the RVIs are not provided directly during testing, we design a cascade decoder that consists of two sub-decoders. The first sub-decoder predicts the content words in response, and applies the word-image mapping model to get those RVIs. Then, the second sub-decoder generates the response based on the post and RVIs. Experimental results on two open-domain dialogue datasets show that our proposed approach achieves superior performance over competitive baselines.", "comment": " Comments: Accepted by ACM MultiMedia 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.04993", "id": "2109.04993", "pdf": "https://arxiv.org/pdf/2109.04993", "other": "https://arxiv.org/format/2109.04993"}, "title": "LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation", "author_info": ["Mohammad Abuzar Shaikh", "Zhanghexuan Ji", "Dana Moukheiber", "Yan Shen", "Sargur Srihari", "Mingchen Gao"], "summary": "Pre-training visual and textual representations from large-scale image-text pairs is becoming a standard approach for many downstream vision-language tasks. The transformer-based models learn inter and intra-modal attention through a list of self-supervised learning tasks. This paper proposes LAViTeR, a novel architecture for visual and textual representation learning. The main module, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks, GAN-based image synthesis and Image Captioning. We also propose a new evaluation metric measuring the similarity between the learnt visual and textual embedding. The experimental results on two public datasets, CUB and MS-COCO, demonstrate superior visual and textual representation alignment in the joint feature embedding space", "comment": " Comments: 14 pages, 10 Figures, 5 Tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.03892", "id": "2109.03892", "pdf": "https://arxiv.org/pdf/2109.03892", "other": "https://arxiv.org/format/2109.03892"}, "title": "Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models", "author_info": ["Steven Y. Feng", "Kevin Lu", "Zhuofu Tao", "Malihe Alikhani", "Teruko Mitamura", "Eduard Hovy", "Varun Gangal"], "summary": "We investigate the use of multimodal information contained in images as an effective method for enhancing the commonsense of Transformer models for text generation. We perform experiments using BART and T5 on concept-to-text generation, specifically the task of generative commonsense reasoning, or CommonGen. We call our approach VisCTG: Visually Grounded Concept-to-Text Generation. VisCTG involves captioning images representing appropriate everyday scenarios, and using these captions to enrich and steer the generation process. Comprehensive evaluation and analysis demonstrate that VisCTG noticeably improves model performance while successfully addressing several issues of the baseline generations, including poor commonsense, fluency, and specificity.", "comment": " Comments: Accepted to AAAI 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.02370", "id": "2109.02370", "pdf": "https://arxiv.org/pdf/2109.02370", "other": "https://arxiv.org/format/2109.02370"}, "title": "Improved RAMEN: Towards Domain Generalization for Visual Question Answering", "author_info": ["Bhanuka Manesha Samarasekara Vitharana Gamage", "Lim Chern Hong"], "summary": "Currently nearing human-level performance, Visual Question Answering (VQA) is an emerging area in artificial intelligence.", "comment": " Comments: 11 pages, 3 figures, 2 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.09208", "id": "2108.09208", "pdf": "https://arxiv.org/pdf/2108.09208", "other": "https://arxiv.org/format/2108.09208"}, "title": "Exploring Data Aggregation and Transformations to Generalize across Visual Domains", "author_info": ["Antono D'Innocente"], "summary": "Computer vision has flourished in recent years thanks to Deep Learning advancements, fast and scalable hardware solutions and large availability of structured image data. Convolutional Neural Networks trained on supervised tasks with backpropagation learn to extract meaningful representations from raw pixels automatically, and surpass shallow methods in image understanding. Though convenient, data-driven feature learning is prone to dataset bias: a network learns its parameters from training signals alone, and will usually perform poorly if train and test distribution differ. To alleviate this problem, research on Domain Generalization (DG), Domain Adaptation (DA) and their variations is increasing. This thesis contributes to these research topics by presenting novel and effective ways to solve the dataset bias problem in its various settings. We propose new frameworks for Domain Generalization and Domain Adaptation which make use of feature aggregation strategies and visual transformations via data-augmentation and multi-task integration of self-supervision. We also design an algorithm that adapts an object detection model to any out of distribution sample at test time. With through experimentation, we show how our proposed solutions outperform competitive state-of-the-art approaches in established DG and DA benchmarks.", "comment": " Comments: PhD thesis "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.05067", "id": "2108.05067", "pdf": "https://arxiv.org/pdf/2108.05067"}, "title": "Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning", "author_info": ["Guangyi Liu", "Yinghong Liao", "Fuyu Wang", "Bin Zhang", "Lu Zhang", "Xiaodan Liang", "Xiang Wan", "Shaolin Li", "Zhen Li", "Shuixing Zhang", "Shuguang Cui"], "summary": "Medical imaging technologies, including computed tomography (CT) or chest X-Ray (CXR), are largely employed to facilitate the diagnosis of the COVID-19. Since manual report writing is usually too time-consuming, a more intelligent auxiliary medical system that could generate medical reports automatically and immediately is urgently needed. In this article, we propose to use the medical visual language BERT (Medical-VLBERT) model to identify the abnormality on the COVID-19 scans and generate the medical report automatically based on the detected lesion regions. To produce more accurate medical reports and minimize the visual-and-linguistic differences, this model adopts an alternate learning strategy with two procedures that are knowledge pretraining and transferring. To be more precise, the knowledge pretraining procedure is to memorize the knowledge from medical texts, while the transferring procedure is to utilize the acquired knowledge for professional medical sentences generations through observations of medical images. In practice, for automatic medical report generation on the COVID-19 cases, we constructed a dataset of 368 medical findings in Chinese and 1104 chest CT scans from The First Affiliated Hospital of Jinan University, Guangzhou, China, and The Fifth Affiliated Hospital of Sun Yat-sen University, Zhuhai, China. Besides, to alleviate the insufficiency of the COVID-19 training samples, our model was first trained on the large-scale Chinese CX-CHR dataset and then transferred to the COVID-19 CT dataset for further fine-tuning. The experimental results showed that Medical-VLBERT achieved state-of-the-art performances on terminology prediction and report generation with the Chinese COVID-19 CT dataset and the CX-CHR dataset. The Chinese COVID-19 CT dataset is available at https://covid19ct.github.io/.", "comment": " Comments: Accepted by IEEE Transactions on Neural Networks and Learning Systems "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.04238", "id": "2108.04238"}, "title": "TDLS: A Top-Down Layer Searching Algorithm for Generating Counterfactual Visual Explanation", "author_info": ["Cong Wang", "Haocheng Han", "Caleb Chen Cao"], "summary": "Explanation of AI, as well as fairness of algorithms' decisions and the transparency of the decision model, are becoming more and more important. And it is crucial to design effective and human-friendly techniques when opening the black-box model. Counterfactual conforms to the human way of thinking and provides a human-friendly explanation, and its corresponding explanation algorithm refers to a strategic alternation of a given data point so that its model output is \"counter-facted\", i.e. the prediction is reverted. In this paper, we adapt counterfactual explanation over fine-grained image classification problem. We demonstrated an adaptive method that could give a counterfactual explanation by showing the composed counterfactual feature map using top-down layer searching algorithm (TDLS). We have proved that our TDLS algorithm could provide more flexible counterfactual visual explanation in an efficient way using VGG-16 model on Caltech-UCSD Birds 200 dataset. At the end, we discussed several applicable scenarios of counterfactual visual explanations.", "comment": " ACM Class:           I.4.0                "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.03857", "id": "2108.03857", "pdf": "https://arxiv.org/pdf/2108.03857"}, "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network", "author_info": ["Sakib Shahriar"], "summary": "\"Art is the lie that enables us to realize the truth.\" - Pablo Picasso. For centuries, humans have dedicated themselves to producing arts to convey their imagination. The advancement in technology and deep learning in particular, has caught the attention of many researchers trying to investigate whether art generation is possible by computers and algorithms. Using generative adversarial networks (GANs), applications such as synthesizing photorealistic human faces and creating captions automatically from images were realized. This survey takes a comprehensive look at the recent works using GANs for generating visual arts, music, and literary text. A performance comparison and description of the various GAN architecture are also presented. Finally, some of the key challenges in art generation using GANs are highlighted along with recommendations for future work.", "comment": " Comments: Submitted to Pattern Recognition Letters "}, {"arxiv": {"page": "https://arxiv.org/abs/2108.03356", "id": "2108.03356", "pdf": "https://arxiv.org/pdf/2108.03356", "other": "https://arxiv.org/format/2108.03356"}, "title": "HelpViz: Automatic Generation of Contextual Visual MobileTutorials from Text-Based Instructions", "author_info": ["Mingyuan Zhong", "Gang Li", "Peggy Chi", "Yang Li"], "summary": "We present HelpViz, a tool for generating contextual visual mobile tutorials from text-based instructions that are abundant on the web. HelpViz transforms text instructions to graphical tutorials in batch, by extracting a sequence of actions from each text instruction through an instruction parsing model, and executing the extracted actions on a simulation infrastructure that manages an array of Android emulators. The automatic execution of each instruction produces a set of graphical and structural assets, including images, videos, and metadata such as clicked elements for each step. HelpViz then synthesizes a tutorial by combining parsed text instructions with the generated assets, and contextualizes the tutorial to user interaction by tracking the user's progress and highlighting the next step.", "comment": " Comments: Accepted to UIST'21 "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.11576", "id": "2107.11576", "pdf": "https://arxiv.org/pdf/2107.11576", "other": "https://arxiv.org/format/2107.11576"}, "title": "X-GGM: Graph Generative Modeling for Out-of-Distribution Generalization in Visual Question Answering", "author_info": ["Jingjing Jiang", "Ziyi Liu", "Yifan Liu", "Zhixiong Nan", "Nanning Zheng"], "summary": "Encouraging progress has been made towards Visual Question Answering (VQA) in recent years, but it is still challenging to enable VQA models to adaptively generalize to out-of-distribution (OOD) samples. Intuitively, recompositions of existing visual concepts (\\ie, attributes and objects) can generate unseen compositions in the training set, which will promote VQA models to generalize to OOD samples. In this paper, we formulate OOD generalization in VQA as a compositional generalization problem and propose a graph generative modeling-based training scheme (X-GGM) to implicitly model the problem. X-GGM leverages graph generative modeling to iteratively generate a relation matrix and node representations for the predefined graph that utilizes attribute-object pairs as nodes. Furthermore, to alleviate the unstable training issue in graph generative modeling, we propose a gradient distribution consistency loss to constrain the data distribution with adversarial perturbations and the generated distribution. The baseline VQA model (LXMERT) trained with the X-GGM scheme achieves state-of-the-art OOD performance on two standard VQA OOD benchmarks, \\ie, VQA-CP v2 and GQA-OOD. Extensive ablation studies demonstrate the effectiveness of X-GGM components. Code is available at \\url{https://github.com/jingjing12110/x-ggm}.", "comment": " Comments: Accepted by ACM MM2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.09262", "id": "2107.09262", "pdf": "https://arxiv.org/pdf/2107.09262", "other": "https://arxiv.org/format/2107.09262"}, "title": "FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos", "author_info": ["Sanchita Ghose", "John J. Prevost"], "summary": "Deep learning based visual to sound generation systems essentially need to be developed particularly considering the synchronicity aspects of visual and audio features with time. In this research we introduce a novel task of guiding a class conditioned generative adversarial network with the temporal visual information of a video input for visual to sound generation task adapting the synchronicity traits between audio-visual modalities. Our proposed FoleyGAN model is capable of conditioning action sequences of visual events leading towards generating visually aligned realistic sound tracks. We expand our previously proposed Automatic Foley dataset to train with FoleyGAN and evaluate our synthesized sound through human survey that shows noteworthy (on average 81\\%) audio-visual synchronicity performance. Our approach also outperforms in statistical experiments compared with other baseline models and audio-visual datasets.", "comment": " MSC Class:           68T10 (primary) 68T07; 68U10(secondary)                              ACM Class:           I.5.4; I.2.10; J.5                "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.08221", "id": "2107.08221", "pdf": "https://arxiv.org/pdf/2107.08221", "other": "https://arxiv.org/format/2107.08221"}, "title": "Visual Representation Learning Does Not Generalize Strongly Within the Same Domain", "author_info": ["Lukas Schott", "Julius von K\u00fcgelgen", "Frederik Tr\u00e4uble", "Peter Gehler", "Chris Russell", "Matthias Bethge", "Bernhard Sch\u00f6lkopf", "Francesco Locatello", "Wieland Brendel"], "summary": "An important component for generalization in machine learning is to uncover underlying latent factors of variation as well as the mechanism through which each factor acts in the world. In this paper, we test whether 17 unsupervised, weakly supervised, and fully supervised representation learning approaches correctly infer the generative factors of variation in simple datasets (dSprites, Shapes3D, MPI3D) from controlled environments, and on our contributed CelebGlow dataset. In contrast to prior robustness work that introduces novel factors of variation during test time, such as blur or other (un)structured noise, we here recompose, interpolate, or extrapolate only existing factors of variation from the training data set (e.g., small and medium-sized objects during training and large objects during testing). Models that learn the correct mechanism should be able to generalize to this benchmark. In total, we train and test 2000+ models and observe that all of them struggle to learn the underlying mechanism regardless of supervision signal and architectural bias. Moreover, the generalization capabilities of all tested models drop significantly as we move from artificial datasets towards more realistic real-world datasets. Despite their inability to identify the correct mechanism, the models are quite modular as their ability to infer other in-distribution factors remains fairly stable, providing only a single factor is out-of-distribution. These results point to an important yet understudied problem of learning mechanistic models of observations that can facilitate generalization.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2107.08203", "id": "2107.08203", "pdf": "https://arxiv.org/pdf/2107.08203", "other": "https://arxiv.org/format/2107.08203"}, "title": "PI2: Generating Visual Analysis Interfaces From Queries", "author_info": ["Yiru Chen", "Eugene Wu"], "summary": "Interactive visual analysis interfaces are critical in nearly every data task. However, creating new interfaces is deeply challenging, as it requires the developer to understand the queries needed to express the desired analysis task, design the appropriate interface to express those queries for the task, and implement the interface using a combination of visualization, browser, server, and database technologies. Although prior work generates a set of interactive widgets that can express an input query log, this paper presents PI2, the first system to generate fully functional visual analysis interfaces from an example sequence of analysis queries. PI2 analyzes queries syntactically and represents a set of queries using a novel Difftree structure that encodes systematic variations between query abstract syntax trees. PI2 then maps each Difftree to a visualization that renders its results, the variations in each Difftree to interactions, and generates a good layout for the interface. We show that PI2 can express data-oriented interactions in existing visualization interaction taxonomies, reproduce or improve several real-world visual analysis interfaces, generate interfaces in 2-19s (median 6s), and scale linearly with the number of queries.", "comment": " ACM Class:           H.2; H.5.2                "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.07589", "id": "2107.07589", "pdf": "https://arxiv.org/pdf/2107.07589", "other": "https://arxiv.org/format/2107.07589"}, "title": "A Comparison of Modern General-Purpose Visual SLAM Approaches", "author_info": ["Alexey Merzlyakov", "Steve Macenski"], "summary": "Advancing maturity in mobile and legged robotics technologies is changing the landscapes where robots are being deployed and found. This innovation calls for a transformation in simultaneous localization and mapping (SLAM) systems to support this new generation of service and consumer robots. No longer can traditionally robust 2D lidar systems dominate while robots are being deployed in multi-story indoor, outdoor unstructured, and urban domains with increasingly inexpensive stereo and RGB-D cameras. Visual SLAM (VSLAM) systems have been a topic of study for decades and a small number of openly available implementations have stood out: ORB-SLAM3, OpenVSLAM and RTABMap.", "comment": " Journal ref:         2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)       "}, {"arxiv": {"page": "https://arxiv.org/abs/2107.05468", "id": "2107.05468", "pdf": "https://arxiv.org/pdf/2107.05468", "other": "https://arxiv.org/format/2107.05468"}, "title": "Visual-Tactile Cross-Modal Data Generation using Residue-Fusion GAN with Feature-Matching and Perceptual Losses", "author_info": ["Shaoyu Cai", "Kening Zhu", "Yuki Ban", "Takuji Narumi"], "summary": "Existing psychophysical studies have revealed that the cross-modal visual-tactile perception is common for humans performing daily activities. However, it is still challenging to build the algorithmic mapping from one modality space to another, namely the cross-modal visual-tactile data translation/generation, which could be potentially important for robotic operation. In this paper, we propose a deep-learning-based approach for cross-modal visual-tactile data generation by leveraging the framework of the generative adversarial networks (GANs). Our approach takes the visual image of a material surface as the visual data, and the accelerometer signal induced by the pen-sliding movement on the surface as the tactile data. We adopt the conditional-GAN (cGAN) structure together with the residue-fusion (RF) module, and train the model with the additional feature-matching (FM) and perceptual losses to achieve the cross-modal data generation. The experimental results show that the inclusion of the RF module, and the FM and the perceptual losses significantly improves cross-modal data generation performance in terms of the classification accuracy upon the generated data and the visual similarity between the ground-truth and the generated data.", "comment": " Comments: 8 pages, 6 figures, Accepted by IEEE Robotics and Automation Letters "}, {"arxiv": {"page": "https://arxiv.org/abs/2106.15550", "id": "2106.15550", "pdf": "https://arxiv.org/pdf/2106.15550", "other": "https://arxiv.org/format/2106.15550"}, "title": "Unified Questioner Transformer for Descriptive Question Generation in Goal-Oriented Visual Dialogue", "author_info": ["Shoya Matsumori", "Kosuke Shingyouchi", "Yuki Abe", "Yosuke Fukuchi", "Komei Sugiura", "Michita Imai"], "summary": "Building an interactive artificial intelligence that can ask questions about the real world is one of the biggest challenges for vision and language problems. In particular, goal-oriented visual dialogue, where the aim of the agent is to seek information by asking questions during a turn-taking dialogue, has been gaining scholarly attention recently. While several existing models based on the GuessWhat?! dataset have been proposed, the Questioner typically asks simple category-based questions or absolute spatial questions. This might be problematic for complex scenes where the objects share attributes or in cases where descriptive questions are required to distinguish objects. In this paper, we propose a novel Questioner architecture, called Unified Questioner Transformer (UniQer), for descriptive question generation with referring expressions. In addition, we build a goal-oriented visual dialogue task called CLEVR Ask. It synthesizes complex scenes that require the Questioner to generate descriptive questions. We train our model with two variants of CLEVR Ask datasets. The results of the quantitative and qualitative evaluations show that UniQer outperforms the baseline.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2106.13409", "id": "2106.13409", "pdf": "https://arxiv.org/pdf/2106.13409", "other": "https://arxiv.org/format/2106.13409"}, "title": "Generative Modeling for Multi-task Visual Learning", "author_info": ["Zhipeng Bao", "Martial Hebert", "Yu-Xiong Wang"], "summary": "Generative modeling has recently shown great promise in computer vision, but it has mostly focused on synthesizing visually realistic images. In this paper, motivated by multi-task learning of shareable feature representations, we consider a novel problem of learning a shared generative model that is useful across various visual perception tasks. Correspondingly, we propose a general multi-task oriented generative modeling (MGM) framework, by coupling a discriminative multi-task network with a generative network. While it is challenging to synthesize both RGB images and pixel-level annotations in multi-task scenarios, our framework enables us to use synthesized images paired with only weak annotations (i.e., image-level scene labels) to facilitate multiple visual tasks. Experimental evaluation on challenging multi-task benchmarks, including NYUv2 and Taskonomy, demonstrates that our MGM framework improves the performance of all the tasks by large margins, consistently outperforming state-of-the-art multi-task approaches.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2106.09678", "id": "2106.09678", "pdf": "https://arxiv.org/pdf/2106.09678", "other": "https://arxiv.org/format/2106.09678"}, "title": "SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual Policies", "author_info": ["Linxi Fan", "Guanzhi Wang", "De-An Huang", "Zhiding Yu", "Li Fei-Fei", "Yuke Zhu", "Anima Anandkumar"], "summary": "Generalization has been a long-standing challenge for reinforcement learning (RL). Visual RL, in particular, can be easily distracted by irrelevant factors in high-dimensional observation space. In this work, we consider robust policy learning which targets zero-shot generalization to unseen visual environments with large distributional shift. We propose SECANT, a novel self-expert cloning technique that leverages image augmentation in two stages to decouple robust representation learning from policy optimization. Specifically, an expert policy is first trained by RL from scratch with weak augmentations. A student network then learns to mimic the expert policy by supervised learning with strong augmentations, making its representation more robust against visual variations compared to the expert. Extensive experiments demonstrate that SECANT significantly advances the state of the art in zero-shot generalization across 4 challenging domains. Our average reward improvements over prior SOTAs are: DeepMind Control (+26.5%), robotic manipulation (+337.8%), vision-based autonomous driving (+47.7%), and indoor object navigation (+15.8%). Code release and video are available at https://linxifan.github.io/secant-site/.", "comment": " Comments: ICML 2021. Website: https://linxifan.github.io/secant-site/ "}, {"arxiv": {"page": "https://arxiv.org/abs/2106.04258", "id": "2106.04258", "pdf": "https://arxiv.org/pdf/2106.04258", "other": "https://arxiv.org/format/2106.04258"}, "title": "Interpretable agent communication from scratch (with a generic visual processor emerging on the side)", "author_info": ["Roberto Dess\u00ec", "Eugene Kharitonov", "Marco Baroni"], "summary": "As deep networks begin to be deployed as autonomous agents, the issue of how they can communicate with each other becomes important. Here, we train two deep nets from scratch to perform realistic referent identification through unsupervised emergent communication. We show that the largely interpretable emergent protocol allows the nets to successfully communicate even about object types they did not see at training time. The visual representations induced as a by-product of our training regime, moreover, show comparable quality, when re-used as generic visual features, to a recent self-supervised learning model. Our results provide concrete evidence of the viability of (interpretable) emergent deep net communication in a more realistic scenario than previously considered, as well as establishing an intriguing link between this field and self-supervised visual learning.", "comment": " Comments: Accepted at NeurIPS 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2106.01326", "id": "2106.01326", "pdf": "https://arxiv.org/pdf/2106.01326"}, "title": "Robust Voxelization and Visualization by Improved Tetrahedral Mesh Generation", "author_info": ["Joseph Chen", "Ko-Wei Tai", "Wen-Chin Chen", "Ming Ouhyoung"], "summary": "When obtaining interior 3D voxel data from triangular meshes, most existing methods fail to handle low quality meshes which happens to take up a big portion on the internet. In this work we present a robust voxelization method that is based on tetrahedral mesh generation within a user defined error bound. Comparing to other tetrahedral mesh generation methods, our method produces much higher quality tetrahedral meshes as the intermediate outcome, which allows us to utilize a faster voxelization algorithm that is based on a stronger assumption. We show the results comparing to various methods including the state-of-the-art. Our contribution includes a framework which takes triangular mesh as an input and produces voxelized data, a proof to an unproved algorithm that performs better than the state-of-the-art, and various experiments including parallelization built on the GPU and CPU. We further tested our method on various dataset including Princeton ModelNet and Thingi10k to show the robustness of the framework, where near 100% availability is achieved, while others can only achieve around 50%.", "comment": " Comments: 11 pages, 7 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2105.14445", "id": "2105.14445", "pdf": "https://arxiv.org/pdf/2105.14445", "other": "https://arxiv.org/format/2105.14445"}, "title": "Modeling Text-visual Mutual Dependency for Multi-modal Dialog Generation", "author_info": ["Shuhe Wang", "Yuxian Meng", "Xiaofei Sun", "Fei Wu", "Rongbin Ouyang", "Rui Yan", "Tianwei Zhang", "Jiwei Li"], "summary": "Multi-modal dialog modeling is of growing interest. In this work, we propose frameworks to resolve a specific case of multi-modal dialog generation that better mimics multi-modal dialog generation in the real world, where each dialog turn is associated with the visual context in which it takes place. Specifically, we propose to model the mutual dependency between text-visual features, where the model not only needs to learn the probability of generating the next dialog utterance given preceding dialog utterances and visual contexts, but also the probability of predicting the visual features in which a dialog utterance takes place, leading the generated dialog utterance specific to the visual context. We observe significant performance boosts over vanilla models when the mutual dependency between text and visual features is modeled. Code is available at https://github.com/ShannonAI/OpenViDial.", "comment": " Comments: arXiv admin note: text overlap with arXiv:2012.15015 "}, {"arxiv": {"page": "https://arxiv.org/abs/2105.13994", "id": "2105.13994", "pdf": "https://arxiv.org/pdf/2105.13994", "other": "https://arxiv.org/format/2105.13994"}, "title": "Linguistic Structures as Weak Supervision for Visual Scene Graph Generation", "author_info": ["Keren Ye", "Adriana Kovashka"], "summary": "Prior work in scene graph generation requires categorical supervision at the level of triplets - subjects and objects, and predicates that relate them, either with or without bounding box information. However, scene graph generation is a holistic task: thus holistic, contextual supervision should intuitively improve performance. In this work, we explore how linguistic structures in captions can benefit scene graph generation. Our method captures the information provided in captions about relations between individual triplets, and context for subjects and objects (e.g. visual properties are mentioned). Captions are a weaker type of supervision than triplets since the alignment between the exhaustive list of human-annotated subjects and objects in triplets, and the nouns in captions, is weak. However, given the large and diverse sources of multimodal data on the web (e.g. blog posts with images and captions), linguistic supervision is more scalable than crowdsourced triplets. We show extensive experimental comparisons against prior methods which leverage instance- and image-level supervision, and ablate our method to show the impact of leveraging phrasal and sequential context, and techniques to improve localization of subjects and objects.", "comment": " Comments: To appear in CVPR 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2105.10026", "id": "2105.10026", "pdf": "https://arxiv.org/pdf/2105.10026", "other": "https://arxiv.org/format/2105.10026"}, "title": "Improving Generation and Evaluation of Visual Stories via Semantic Consistency", "author_info": ["Adyasha Maharana", "Darryl Hannan", "Mohit Bansal"], "summary": "Story visualization is an under-explored task that falls at the intersection of many important research directions in both computer vision and natural language processing. In this task, given a series of natural language captions which compose a story, an agent must generate a sequence of images that correspond to the captions. Prior work has introduced recurrent generative models which outperform text-to-image synthesis models on this task. However, there is room for improvement of generated images in terms of visual quality, coherence and relevance. We present a number of improvements to prior modeling approaches, including (1) the addition of a dual learning framework that utilizes video captioning to reinforce the semantic alignment between the story and generated images, (2) a copy-transform mechanism for sequentially-consistent story visualization, and (3) MART-based transformers to model complex interactions between frames. We present ablation studies to demonstrate the effect of each of these techniques on the generative power of the model for both individual images as well as the entire narrative. Furthermore, due to the complexity and generative nature of the task, standard evaluation metrics do not accurately reflect performance. Therefore, we also provide an exploration of evaluation metrics for the model, focused on aspects of the generated frames such as the presence/quality of generated characters, the relevance to captions, and the diversity of the generated images. We also present correlation experiments of our proposed automated metrics with human evaluations. Code and data available at: https://github.com/adymaharana/StoryViz", "comment": " Comments: NAACL 2021 (16 pages) "}, {"arxiv": {"page": "https://arxiv.org/abs/2105.04780", "id": "2105.04780", "pdf": "https://arxiv.org/pdf/2105.04780", "other": "https://arxiv.org/format/2105.04780"}, "title": "Cross-Modal Generative Augmentation for Visual Question Answering", "author_info": ["Zixu Wang", "Yishu Miao", "Lucia Specia"], "summary": "Data augmentation has been shown to effectively improve the performance of multimodal machine learning models. This paper introduces a generative model for data augmentation by leveraging the correlations among multiple modalities. Different from conventional data augmentation approaches that apply low-level operations with deterministic heuristics, our method learns a generator that generates samples of the target modality conditioned on observed modalities in the variational auto-encoder framework. Additionally, the proposed model is able to quantify the confidence of augmented data by its generative probability, and can be jointly optimised with a downstream task. Experiments on Visual Question Answering as downstream task demonstrate the effectiveness of the proposed generative model, which is able to improve strong UpDn-based models to achieve state-of-the-art performance.", "comment": " Comments: BMVC 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2105.00708", "id": "2105.00708", "pdf": "https://arxiv.org/pdf/2105.00708", "other": "https://arxiv.org/format/2105.00708"}, "title": "Exploiting Audio-Visual Consistency with Partial Supervision for Spatial Audio Generation", "author_info": ["Yan-Bo Lin", "Yu-Chiang Frank Wang"], "summary": "Human perceives rich auditory experience with distinct sound heard by ears. Videos recorded with binaural audio particular simulate how human receives ambient sound. However, a large number of videos are with monaural audio only, which would degrade the user experience due to the lack of ambient information. To address this issue, we propose an audio spatialization framework to convert a monaural video into a binaural one exploiting the relationship across audio and visual components. By preserving the left-right consistency in both audio and visual modalities, our learning strategy can be viewed as a self-supervised learning technique, and alleviates the dependency on a large amount of video data with ground truth binaural audio data during training. Experiments on benchmark datasets confirm the effectiveness of our proposed framework in both semi-supervised and fully supervised scenarios, with ablation studies and visualization further support the use of our model for audio spatialization.", "comment": " Comments: AAAI'21 "}, {"arxiv": {"page": "https://arxiv.org/abs/2105.00162", "id": "2105.00162", "pdf": "https://arxiv.org/pdf/2105.00162", "other": "https://arxiv.org/format/2105.00162"}, "title": "Generative Art Using Neural Visual Grammars and Dual Encoders", "author_info": ["Chrisantha Fernando", "S. M. Ali Eslami", "Jean-Baptiste Alayrac", "Piotr Mirowski", "Dylan Banarse", "Simon Osindero"], "summary": "Whilst there are perhaps only a few scientific methods, there seem to be almost as many artistic methods as there are artists. Artistic processes appear to inhabit the highest order of open-endedness. To begin to understand some of the processes of art making it is helpful to try to automate them even partially. In this paper, a novel algorithm for producing generative art is described which allows a user to input a text string, and which in a creative response to this string, outputs an image which interprets that string. It does so by evolving images using a hierarchical neural Lindenmeyer system, and evaluating these images along the way using an image text dual encoder trained on billions of images and their associated text from the internet. In doing so we have access to and control over an instance of an artistic process, allowing analysis of which aspects of the artistic process become the task of the algorithm, and which elements remain the responsibility of the artist.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2104.11116", "id": "2104.11116", "pdf": "https://arxiv.org/pdf/2104.11116", "other": "https://arxiv.org/format/2104.11116"}, "title": "Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation", "author_info": ["Hang Zhou", "Yasheng Sun", "Wayne Wu", "Chen Change Loy", "Xiaogang Wang", "Ziwei Liu"], "summary": "While accurate lip synchronization has been achieved for arbitrary-subject audio-driven talking face generation, the problem of how to efficiently drive the head pose remains. Previous methods rely on pre-estimated structural information such as landmarks and 3D parameters, aiming to generate personalized rhythmic movements. However, the inaccuracy of such estimated information under extreme conditions would lead to degradation problems. In this paper, we propose a clean yet effective framework to generate pose-controllable talking faces. We operate on raw face images, using only a single photo as an identity reference. The key is to modularize audio-visual representations by devising an implicit low-dimension pose code. Substantially, both speech content and head pose information lie in a joint non-identity embedding space. While speech content information can be defined by learning the intrinsic synchronization between audio-visual modalities, we identify that a pose code will be complementarily learned in a modulated convolution-based reconstruction framework.", "comment": " Comments: Accepted to IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Code and models are available at https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS "}, {"arxiv": {"page": "https://arxiv.org/abs/2104.09163", "id": "2104.09163", "pdf": "https://arxiv.org/pdf/2104.09163", "other": "https://arxiv.org/format/2104.09163"}, "title": "Bidirectional Interaction between Visual and Motor Generative Models using Predictive Coding and Active Inference", "author_info": ["Louis Annabi", "Alexandre Pitti", "Mathias Quoy"], "summary": "In this work, we build upon the Active Inference (AIF) and Predictive Coding (PC) frameworks to propose a neural architecture comprising a generative model for sensory prediction, and a distinct generative model for motor trajectories. We highlight how sequences of sensory predictions can act as rails guiding learning, control and online adaptation of motor trajectories. We furthermore inquire the effects of bidirectional interactions between the motor and the visual modules. The architecture is tested on the control of a simulated robotic arm learning to reproduce handwritten letters.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2104.06162", "id": "2104.06162", "pdf": "https://arxiv.org/pdf/2104.06162", "other": "https://arxiv.org/format/2104.06162"}, "title": "Visually Informed Binaural Audio Generation without Binaural Audios", "author_info": ["Xudong Xu", "Hang Zhou", "Ziwei Liu", "Bo Dai", "Xiaogang Wang", "Dahua Lin"], "summary": "Stereophonic audio, especially binaural audio, plays an essential role in immersive viewing environments. Recent research has explored generating visually guided stereophonic audios supervised by multi-channel audio collections. However, due to the requirement of professional recording devices, existing datasets are limited in scale and variety, which impedes the generalization of supervised methods in real-world scenarios. In this work, we propose PseudoBinaural, an effective pipeline that is free of binaural recordings. The key insight is to carefully build pseudo visual-stereo pairs with mono data for training. Specifically, we leverage spherical harmonic decomposition and head-related impulse response (HRIR) to identify the relationship between spatial locations and received binaural audios. Then in the visual modality, corresponding visual cues of the mono data are manually placed at sound source positions to form the pairs. Compared to fully-supervised paradigms, our binaural-recording-free pipeline shows great stability in cross-dataset evaluation and achieves comparable performance under subjective preference. Moreover, combined with binaural recordings, our method is able to further boost the performance of binaural audio generation under supervised settings.", "comment": " Comments: Accepted by CVPR 2021. Code, models, and demo video are available on our webpage: \\<https://sheldontsui.github.io/projects/PseudoBinaural> "}, {"arxiv": {"page": "https://arxiv.org/abs/2104.04785", "id": "2104.04785", "pdf": "https://arxiv.org/pdf/2104.04785", "other": "https://arxiv.org/format/2104.04785"}, "title": "Physically-Consistent Generative Adversarial Networks for Coastal Flood Visualization", "author_info": ["Bj\u00f6rn L\u00fctjens", "Brandon Leshchinskiy", "Christian Requena-Mesa", "Farrukh Chishtie", "Natalia D\u00edaz-Rodr\u00edguez", "Oc\u00e9ane Boulais", "Aruna Sankaranarayanan", "Aaron Pi\u00f1a", "Yarin Gal", "Chedy Ra\u00efssi", "Alexander Lavin", "Dava Newman"], "summary": "As climate change increases the intensity of natural disasters, society needs better tools for adaptation. Floods, for example, are the most frequent natural disaster, and better tools for flood risk communication could increase the support for flood-resilient infrastructure development. Our work aims to enable more visual communication of large-scale climate impacts via visualizing the output of coastal flood models as satellite imagery. We propose the first deep learning pipeline to ensure physical-consistency in synthetic visual satellite imagery. We advanced a state-of-the-art GAN called pix2pixHD, such that it produces imagery that is physically-consistent with the output of an expert-validated storm surge model (NOAA SLOSH). By evaluating the imagery relative to physics-based flood maps, we find that our proposed framework outperforms baseline models in both physical-consistency and photorealism. We envision our work to be the first step towards a global visualization of how climate change shapes our landscape. Continuing on this path, we show that the proposed pipeline generalizes to visualize arctic sea ice melt. We also publish a dataset of over 25k labelled image-pairs to study image-to-image translation in Earth observation.", "comment": " Comments: arXiv admin note: text overlap with arXiv:2010.08103 "}, {"arxiv": {"page": "https://arxiv.org/abs/2104.01935", "id": "2104.01935", "pdf": "https://arxiv.org/pdf/2104.01935", "other": "https://arxiv.org/format/2104.01935"}, "title": "Mining Customers' Opinions for Online Reputation Generation and Visualization in e-Commerce Platforms", "author_info": ["Abdessamad Benlahbib"], "summary": "Customer reviews represent a very rich data source from which we can extract very valuable information about different online shopping experiences. The amount of the collected data may be very large especially for trendy items (products, movies, TV shows, hotels, services...), where the number of available customers' opinions could easily surpass thousands. In fact, while a good number of reviews could indeed give a hint about the quality of an item, a potential customer may not have time or effort to read all reviews for the purpose of making an informed decision (buying, renting, booking...). Thus, the need for the right tools and technologies to help in such a task becomes a necessity for the buyer as for the seller. My research goal in this thesis is to develop reputation systems that can automatically provide E-commerce customers with valuable information to support them during their online decision-making process by mining online reviews expressed in natural language.", "comment": " Comments: PhD Thesis "}, {"arxiv": {"page": "https://arxiv.org/abs/2103.15365", "id": "2103.15365", "pdf": "https://arxiv.org/pdf/2103.15365", "other": "https://arxiv.org/format/2103.15365"}, "title": "Visual Distant Supervision for Scene Graph Generation", "author_info": ["Yuan Yao", "Ao Zhang", "Xu Han", "Mengdi Li", "Cornelius Weber", "Zhiyuan Liu", "Stefan Wermter", "Maosong Sun"], "summary": "Scene graph generation aims to identify objects and their relations in images, providing structured image representations that can facilitate numerous applications in computer vision. However, scene graph models usually require supervised learning on large quantities of labeled data with intensive human annotation. In this work, we propose visual distant supervision, a novel paradigm of visual relation learning, which can train scene graph models without any human-labeled data. The intuition is that by aligning commonsense knowledge bases and images, we can automatically create large-scale labeled data to provide distant supervision for visual relation learning. To alleviate the noise in distantly labeled data, we further propose a framework that iteratively estimates the probabilistic relation labels and eliminates the noisy ones. Comprehensive experimental results show that our distantly supervised model outperforms strong weakly supervised and semi-supervised baselines. By further incorporating human-labeled data in a semi-supervised fashion, our model outperforms state-of-the-art fully supervised models by a large margin (e.g., 8.3 micro- and 7.8 macro-recall@50 improvements for predicate classification in Visual Genome evaluation). We make the data and code for this paper publicly available at https://github.com/thunlp/VisualDS.", "comment": " Comments: Accepted by ICCV 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2103.15279", "id": "2103.15279", "pdf": "https://arxiv.org/pdf/2103.15279", "other": "https://arxiv.org/format/2103.15279"}, "title": "Generalizing to the Open World: Deep Visual Odometry with Online Adaptation", "author_info": ["Shunkai Li", "Xin Wu", "Yingdian Cao", "Hongbin Zha"], "summary": "Despite learning-based visual odometry (VO) has shown impressive results in recent years, the pretrained networks may easily collapse in unseen environments. The large domain gap between training and testing data makes them difficult to generalize to new scenes. In this paper, we propose an online adaptation framework for deep VO with the assistance of scene-agnostic geometric computations and Bayesian inference. In contrast to learning-based pose estimation, our method solves pose from optical flow and depth while the single-view depth estimation is continuously improved with new observations by online learned uncertainties. Meanwhile, an online learned photometric uncertainty is used for further depth and pose optimization by a differentiable Gauss-Newton layer. Our method enables fast adaptation of deep VO networks to unseen environments in a self-supervised manner. Extensive experiments including Cityscapes to KITTI and outdoor KITTI to indoor TUM demonstrate that our method achieves state-of-the-art generalization ability among self-supervised VO methods.", "comment": " Comments: Accepted by CVPR 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2103.15182", "id": "2103.15182", "pdf": "https://arxiv.org/pdf/2103.15182"}, "title": "Verifying Design through Generative Visualization of Neural Activities", "author_info": ["Pan Wang", "Danlin Peng", "Simiao Yu", "Chao Wu", "Peter Childs", "Yike Guo", "Ling Li"], "summary": "Current neuroscience focused approaches for evaluating the effectiveness of a design do not use direct visualisation of mental activity. A recurrent neural network is used as the encoder to learn latent representation from electroencephalogram (EEG) signals, recorded while subjects looked at 50 categories of images. A generative adversarial network (GAN) conditioned on the EEG latent representation is trained for reconstructing these images. After training, the neural network is able to reconstruct images from brain activity recordings. To demonstrate the proposed method in the context of the mental association with a design, we performed a study that indicates an iconic design image could inspire the subject to create cognitive associations with branding and valued products. The proposed method could have the potential in verifying designs by visualizing the cognitive understanding of underlying brain activity.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2103.11727", "id": "2103.11727", "pdf": "https://arxiv.org/pdf/2103.11727"}, "title": "A New Efficient Numbering System : Application to Numbers Generation and Visual Markers Design", "author_info": ["Messaoud Mostefai", "Salah Khodja", "Youssef Chahir"], "summary": "This short paper introduces a recently patented line based numbering system. The last allows a best concordance with decimal digits values, and open up new opportunities, which are not possible with the classical decimal numeration system. Proposed OILU symbolic allows generating a new type of number series, based on multi facets numbers splitting process. On the other hand, this new symbolic is used in the development of new visual markers, highly required in augmented reality and UAV's navigation applications.", "comment": " MSC Class:           11A67 (Primary)                              ACM Class:           I.4; I.5; I.1; J.0                "}, {"arxiv": {"page": "https://arxiv.org/abs/2103.10191", "id": "2103.10191", "pdf": "https://arxiv.org/pdf/2103.10191", "other": "https://arxiv.org/format/2103.10191"}, "title": "Decoupled Spatial Temporal Graphs for Generic Visual Grounding", "author_info": ["Qianyu Feng", "Yunchao Wei", "Mingming Cheng", "Yi Yang"], "summary": "Visual grounding is a long-lasting problem in vision-language understanding due to its diversity and complexity. Current practices concentrate mostly on performing visual grounding in still images or well-trimmed video clips. This work, on the other hand, investigates into a more general setting, generic visual grounding, aiming to mine all the objects satisfying the given expression, which is more challenging yet practical in real-world scenarios. Importantly, grounding results are expected to accurately localize targets in both space and time. Whereas, it is tricky to make trade-offs between the appearance and motion features. In real scenarios, model tends to fail in distinguishing distractors with similar attributes. Motivated by these considerations, we propose a simple yet effective approach, named DSTG, which commits to 1) decomposing the spatial and temporal representations to collect all-sided cues for precise grounding; 2) enhancing the discriminativeness from distractors and the temporal consistency with a contrastive learning routing strategy. We further elaborate a new video dataset, GVG, that consists of challenging referring cases with far-ranging videos. Empirical experiments well demonstrate the superiority of DSTG over state-of-the-art on Charades-STA, ActivityNet-Caption and GVG datasets. Code and dataset will be made available.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2102.03424", "id": "2102.03424", "pdf": "https://arxiv.org/pdf/2102.03424", "other": "https://arxiv.org/format/2102.03424"}, "title": "Learning Audio-Visual Correlations from Variational Cross-Modal Generation", "author_info": ["Ye Zhu", "Yu Wu", "Hugo Latapie", "Yi Yang", "Yan Yan"], "summary": "People can easily imagine the potential sound while seeing an event. This natural synchronization between audio and visual signals reveals their intrinsic correlations. To this end, we propose to learn the audio-visual correlations from the perspective of cross-modal generation in a self-supervised manner, the learned correlations can be then readily applied in multiple downstream tasks such as the audio-visual cross-modal localization and retrieval. We introduce a novel Variational AutoEncoder (VAE) framework that consists of Multiple encoders and a Shared decoder (MS-VAE) with an additional Wasserstein distance constraint to tackle the problem. Extensive experiments demonstrate that the optimized latent representation of the proposed MS-VAE can effectively learn the audio-visual correlations and can be readily applied in multiple audio-visual downstream tasks to achieve competitive performance even without any given label information during training.", "comment": " Comments: Accepted to ICASSP 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2102.02599", "id": "2102.02599", "pdf": "https://arxiv.org/pdf/2102.02599", "other": "https://arxiv.org/format/2102.02599"}, "title": "VSEGAN: Visual Speech Enhancement Generative Adversarial Network", "author_info": ["Xinmeng Xu", "Yang Wang", "Dongxiang Xu", "Yiyuan Peng", "Cong Zhang", "Jie Jia", "Binbin Chen"], "summary": "Speech enhancement is an essential task of improving speech quality in noise scenario. Several state-of-the-art approaches have introduced visual information for speech enhancement,since the visual aspect of speech is essentially unaffected by acoustic environment. This paper proposes a novel frameworkthat involves visual information for speech enhancement, by in-corporating a Generative Adversarial Network (GAN). In par-ticular, the proposed visual speech enhancement GAN consistof two networks trained in adversarial manner, i) a generator that adopts multi-layer feature fusion convolution network to enhance input noisy speech, and ii) a discriminator that attemptsto minimize the discrepancy between the distributions of the clean speech signal and enhanced speech signal. Experiment re-sults demonstrated superior performance of the proposed modelagainst several state-of-the-art", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2102.00424", "id": "2102.00424", "pdf": "https://arxiv.org/pdf/2102.00424", "other": "https://arxiv.org/format/2102.00424"}, "title": "An Empirical Study on the Generalization Power of Neural Representations Learned via Visual Guessing Games", "author_info": ["Alessandro Suglia", "Yonatan Bisk", "Ioannis Konstas", "Antonio Vergari", "Emanuele Bastianelli", "Andrea Vanzo", "Oliver Lemon"], "summary": "Guessing games are a prototypical instance of the \"learning by interacting\" paradigm. This work investigates how well an artificial agent can benefit from playing guessing games when later asked to perform on novel NLP downstream tasks such as Visual Question Answering (VQA). We propose two ways to exploit playing guessing games: 1) a supervised learning scenario in which the agent learns to mimic successful guessing games and 2) a novel way for an agent to play by itself, called Self-play via Iterated Experience Learning (SPIEL).", "comment": " Comments: Accepted paper for the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2021) "}, {"arxiv": {"page": "https://arxiv.org/abs/2101.12338", "id": "2101.12338", "pdf": "https://arxiv.org/pdf/2101.12338", "other": "https://arxiv.org/format/2101.12338"}, "title": "Enabling Robots to Draw and Tell: Towards Visually Grounded Multimodal Description Generation", "author_info": ["Ting Han", "Sina Zarrie\u00df"], "summary": "Socially competent robots should be equipped with the ability to perceive the world that surrounds them and communicate about it in a human-like manner. Representative skills that exhibit such ability include generating image descriptions and visually grounded referring expressions. In the NLG community, these generation tasks are largely investigated in non-interactive and language-only settings. However, in face-to-face interaction, humans often deploy multiple modalities to communicate, forming seamless integration of natural language, hand gestures and other modalities like sketches. To enable robots to describe what they perceive with speech and sketches/gestures, we propose to model the task of generating natural language together with free-hand sketches/hand gestures to describe visual scenes and real life objects, namely, visually-grounded multimodal description generation. In this paper, we discuss the challenges and evaluation metrics of the task, and how the task can benefit from progress recently made in the natural language processing and computer vision realms, where related topics such as visually grounded NLG, distributional semantics, and photo-based sketch generation have been extensively studied.", "comment": " Comments: The 2nd Workshop on NLG for HRI colocated with The 13th International Conference on Natural Language Generation "}, {"arxiv": {"page": "https://arxiv.org/abs/2101.08978", "id": "2101.08978", "pdf": "https://arxiv.org/pdf/2101.08978", "other": "https://arxiv.org/format/2101.08978"}, "title": "Visual Question Answering based on Local-Scene-Aware Referring Expression Generation", "author_info": ["Jung-Jun Kim", "Dong-Gyu Lee", "Jialin Wu", "Hong-Gyu Jung", "Seong-Whan Lee"], "summary": "Visual question answering requires a deep understanding of both images and natural language. However, most methods mainly focus on visual concept; such as the relationships between various objects. The limited use of object categories combined with their relationships or simple question embedding is insufficient for representing complex scenes and explaining decisions. To address this limitation, we propose the use of text expressions generated for images, because such expressions have few structural constraints and can provide richer descriptions of images. The generated expressions can be incorporated with visual features and question embedding to obtain the question-relevant answer. A joint-embedding multi-head attention network is also proposed to model three different information modalities with co-attention. We quantitatively and qualitatively evaluated the proposed method on the VQA v2 dataset and compared it with state-of-the-art methods in terms of answer prediction. The quality of the generated expressions was also evaluated on the RefCOCO, RefCOCO+, and RefCOCOg datasets. Experimental results demonstrate the effectiveness of the proposed method and reveal that it outperformed all of the competing methods in terms of both quantitative and qualitative results.", "comment": " Comments: 32 pages, 8 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2101.04251", "id": "2101.04251", "pdf": "https://arxiv.org/pdf/2101.04251", "other": "https://arxiv.org/format/2101.04251"}, "title": "Vis Ex Machina: An Analysis of Trust in Human versus Algorithmically Generated Visualization Recommendations", "author_info": ["Rachael Zehrung", "Astha Singhal", "Michael Correll", "Leilani Battle"], "summary": "More visualization systems are simplifying the data analysis process by automatically suggesting relevant visualizations. However, little work has been done to understand if users trust these automated recommendations. In this paper, we present the results of a crowd-sourced study exploring preferences and perceived quality of recommendations that have been positioned as either human-curated or algorithmically generated. We observe that while participants initially prefer human recommenders, their actions suggest an indifference for recommendation source when evaluating visualization recommendations. The relevance of presented information (e.g., the presence of certain data fields) was the most critical factor, followed by a belief in the recommender's ability to create accurate visualizations. Our findings suggest a general indifference towards the provenance of recommendations, and point to idiosyncratic definitions of visualization quality and trustworthiness that may not be captured by simple measures. We suggest that recommendation systems should be tailored to the information-foraging strategies of specific users.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2101.00419", "id": "2101.00419", "pdf": "https://arxiv.org/pdf/2101.00419", "other": "https://arxiv.org/format/2101.00419"}, "title": "KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation", "author_info": ["Yiran Xing", "Zai Shi", "Zhao Meng", "Gerhard Lakemeyer", "Yunpu Ma", "Roger Wattenhofer"], "summary": "We present Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformer-based sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts. We adapt the generative BART architecture to a multimodal model with visual and textual inputs. We further develop novel pretraining tasks to improve the model performance on the Visual Commonsense Generation (VCG) task. In particular, our pretraining task of Knowledge-based Commonsense Generation (KCG) boosts model performance on the VCG task by leveraging commonsense knowledge from a large language model pretrained on external commonsense knowledge graphs. To the best of our knowledge, we are the first to propose a dedicated task for improving model performance on the VCG task. Experimental results show that our model reaches state-of-the-art performance on the VCG task by applying these novel pretraining tasks.", "comment": " Comments: ACL-IJCNLP 2021 main conference. The first three authors contribute equally to this work "}, {"arxiv": {"page": "https://arxiv.org/abs/2012.14070", "id": "2012.14070", "pdf": "https://arxiv.org/pdf/2012.14070", "other": "https://arxiv.org/format/2012.14070"}, "title": "Generative Partial Visual-Tactile Fused Object Clustering", "author_info": ["Tao Zhang", "Yang Cong", "Gan Sun", "Jiahua Dong", "Yuyang Liu", "Zhengming Ding"], "summary": "Visual-tactile fused sensing for object clustering has achieved significant progresses recently, since the involvement of tactile modality can effectively improve clustering performance. However, the missing data (i.e., partial data) issues always happen due to occlusion and noises during the data collecting process. This issue is not well solved by most existing partial multi-view clustering methods for the heterogeneous modality challenge. Naively employing these methods would inevitably induce a negative effect and further hurt the performance. To solve the mentioned challenges, we propose a Generative Partial Visual-Tactile Fused (i.e., GPVTF) framework for object clustering. More specifically, we first do partial visual and tactile features extraction from the partial visual and tactile data, respectively, and encode the extracted features in modality-specific feature subspaces. A conditional cross-modal clustering generative adversarial network is then developed to synthesize one modality conditioning on the other modality, which can compensate missing samples and align the visual and tactile modalities naturally by adversarial learning. To the end, two pseudo-label based KL-divergence losses are employed to update the corresponding modality-specific encoders. Extensive comparative experiments on three public visual-tactile datasets prove the effectiveness of our method.", "comment": " Comments: 9 pages; 8 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2012.13191", "id": "2012.13191", "pdf": "https://arxiv.org/pdf/2012.13191", "other": "https://arxiv.org/format/2012.13191"}, "title": "Appearance-Invariant 6-DoF Visual Localization using Generative Adversarial Networks", "author_info": ["Yimin Lin", "Jianfeng Huang", "Shiguo Lian"], "summary": "We propose a novel visual localization network when outside environment has changed such as different illumination, weather and season. The visual localization network is composed of a feature extraction network and pose regression network. The feature extraction network is made up of an encoder network based on the Generative Adversarial Network CycleGAN, which can capture intrinsic appearance-invariant feature maps from unpaired samples of different weathers and seasons. With such an invariant feature, we use a 6-DoF pose regression network to tackle long-term visual localization in the presence of outdoor illumination, weather and season changes. A variety of challenging datasets for place recognition and localization are used to prove our visual localization network, and the results show that our method outperforms state-of-the-art methods in the scenarios with various environment changes.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2012.11552", "id": "2012.11552", "pdf": "https://arxiv.org/pdf/2012.11552", "other": "https://arxiv.org/format/2012.11552"}, "title": "OBoW: Online Bag-of-Visual-Words Generation for Self-Supervised Learning", "author_info": ["Spyros Gidaris", "Andrei Bursuc", "Gilles Puy", "Nikos Komodakis", "Matthieu Cord", "Patrick P\u00e9rez"], "summary": "Learning image representations without human supervision is an important and active research field. Several recent approaches have successfully leveraged the idea of making such a representation invariant under different types of perturbations, especially via contrastive-based instance discrimination training. Although effective visual representations should indeed exhibit such invariances, there are other important characteristics, such as encoding contextual reasoning skills, for which alternative reconstruction-based approaches might be better suited.", "comment": " Comments: Accepted to CVPR2021. Code at https://github.com/valeoai/obow "}, {"arxiv": {"page": "https://arxiv.org/abs/2012.07332", "id": "2012.07332", "pdf": "https://arxiv.org/pdf/2012.07332", "other": "https://arxiv.org/format/2012.07332"}, "title": "Combining Similarity and Adversarial Learning to Generate Visual Explanation: Application to Medical Image Classification", "author_info": ["Martin Charachon", "C\u00e9line Hudelot", "Paul-Henry Courn\u00e8de", "Camille Ruppli", "Roberto Ardon"], "summary": "Explaining decisions of black-box classifiers is paramount in sensitive domains such as medical imaging since clinicians confidence is necessary for adoption. Various explanation approaches have been proposed, among which perturbation based approaches are very promising. Within this class of methods, we leverage a learning framework to produce our visual explanations method. From a given classifier, we train two generators to produce from an input image the so called similar and adversarial images. The similar image shall be classified as the input image whereas the adversarial shall not. Visual explanation is built as the difference between these two generated images. Using metrics from the literature, our method outperforms state-of-the-art approaches. The proposed approach is model-agnostic and has a low computation burden at prediction time. Thus, it is adapted for real-time systems. Finally, we show that random geometric augmentations applied to the original image play a regularization role that improves several previously proposed explanation methods. We validate our approach on a large chest X-ray database.", "comment": " Comments: To be published in ICPR 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2012.05649", "id": "2012.05649", "pdf": "https://arxiv.org/pdf/2012.05649", "other": "https://arxiv.org/format/2012.05649"}, "title": "Concept Generalization in Visual Representation Learning", "author_info": ["Mert Bulent Sariyildiz", "Yannis Kalantidis", "Diane Larlus", "Karteek Alahari"], "summary": "Measuring concept generalization, i.e., the extent to which models trained on a set of (seen) visual concepts can be leveraged to recognize a new set of (unseen) concepts, is a popular way of evaluating visual representations, especially in a self-supervised learning framework. Nonetheless, the choice of unseen concepts for such an evaluation is usually made arbitrarily, and independently from the seen concepts used to train representations, thus ignoring any semantic relationships between the two. In this paper, we argue that the semantic relationships between seen and unseen concepts affect generalization performance and propose ImageNet-CoG, a novel benchmark on the ImageNet-21K (IN-21K) dataset that enables measuring concept generalization in a principled way. Our benchmark leverages expert knowledge that comes from WordNet in order to define a sequence of unseen IN-21K concept sets that are semantically more and more distant from the ImageNet-1K (IN-1K) subset, a ubiquitous training set. This allows us to benchmark visual representations learned on IN-1K out-of-the box. We conduct a large-scale study encompassing 31 convolution and transformer-based models and show how different architectures, levels of supervision, regularization techniques and use of web data impact the concept generalization performance.", "comment": " Comments: Accepted to ICCV 2021. See our project website: https://europe.naverlabs.com/cog-benchmark for code and ImageNet-CoG level files "}, {"arxiv": {"page": "https://arxiv.org/abs/2012.05446", "id": "2012.05446", "pdf": "https://arxiv.org/pdf/2012.05446", "other": "https://arxiv.org/format/2012.05446"}, "title": "Visual Perception Generalization for Vision-and-Language Navigation via Meta-Learning", "author_info": ["Ting Wang", "Zongkai Wu", "Donglin Wang"], "summary": "Vision-and-language navigation (VLN) is a challenging task that requires an agent to navigate in real-world environments by understanding natural language instructions and visual information received in real-time. Prior works have implemented VLN tasks on continuous environments or physical robots, all of which use a fixed camera configuration due to the limitations of datasets, such as 1.5 meters height, 90 degrees horizontal field of view (HFOV), etc. However, real-life robots with different purposes have multiple camera configurations, and the huge gap in visual information makes it difficult to directly transfer the learned navigation model between various robots. In this paper, we propose a visual perception generalization strategy based on meta-learning, which enables the agent to fast adapt to a new camera configuration with a few shots. In the training phase, we first locate the generalization problem to the visual perception module, and then compare two meta-learning algorithms for better generalization in seen and unseen environments. One of them uses the Model-Agnostic Meta-Learning (MAML) algorithm that requires a few shot adaptation, and the other refers to a metric-based meta-learning method with a feature-wise affine transformation layer. The experiment results show that our strategy successfully adapts the learned navigation model to a new camera configuration, and the two algorithms show their advantages in seen and unseen environments respectively.", "comment": " Comments: 8 pages, 4 figures, preprinted version "}, {"arxiv": {"page": "https://arxiv.org/abs/2011.07953", "id": "2011.07953", "pdf": "https://arxiv.org/pdf/2011.07953", "other": "https://arxiv.org/format/2011.07953"}, "title": "Shimon the Robot Film Composer and DeepScore: An LSTM for Generation of Film Scores based on Visual Analysis", "author_info": ["Richard Savery", "Gil Weinberg"], "summary": "Composing for a film requires developing an understanding of the film, its characters and the film aesthetic choices made by the director. We propose using existing visual analysis systems as a core technology for film music generation. We extract film features including main characters and their emotions to develop a computer understanding of the film's narrative arc. This arc is combined with visually analyzed director aesthetic choices including pacing and levels of movement. Two systems are presented, the first using a robotic film composer and marimbist to generate film scores in real-time performance. The second software-based system builds on the results from the robot film composer to create narrative driven film scores.", "comment": " Comments: Computer Simulation of Musical Creativity, 20th-22nd August, University College Dublin "}, {"arxiv": {"page": "https://arxiv.org/abs/2011.05600", "id": "2011.05600", "pdf": "https://arxiv.org/pdf/2011.05600", "other": "https://arxiv.org/format/2011.05600"}, "title": "Documentation Generation as Information Visualization", "author_info": ["Will Crichton"], "summary": "Automatic documentation generation tools, or auto docs, are widely used to visualize information about APIs. However, each auto doc tool comes with its own unique representation of API information. In this paper, I use an information visualization analysis of auto docs to generate potential design principles for improving their usability. Developers use auto docs as a reference by looking up relevant API primitives given partial information, or leads, about its name, type, or behavior. I discuss how auto docs can better support searching and scanning on these leads, e.g. by providing more information-dense visualizations of method signatures.", "comment": " Comments: To appear at PLATEAU @ SPLASH 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2011.05513", "id": "2011.05513", "pdf": "https://arxiv.org/pdf/2011.05513", "other": "https://arxiv.org/format/2011.05513"}, "title": "Zero-Shot Terrain Generalization for Visual Locomotion Policies", "author_info": ["Alejandro Escontrela", "George Yu", "Peng Xu", "Atil Iscen", "Jie Tan"], "summary": "Legged robots have unparalleled mobility on unstructured terrains. However, it remains an open challenge to design locomotion controllers that can operate in a large variety of environments. In this paper, we address this challenge of automatically learning locomotion controllers that can generalize to a diverse collection of terrains often encountered in the real world. We frame this challenge as a multi-task reinforcement learning problem and define each task as a type of terrain that the robot needs to traverse. We propose an end-to-end learning approach that makes direct use of the raw exteroceptive inputs gathered from a simulated 3D LiDAR sensor, thus circumventing the need for ground-truth heightmaps or preprocessing of perception information. As a result, the learned controller demonstrates excellent zero-shot generalization capabilities and can navigate 13 different environments, including stairs, rugged land, cluttered offices, and indoor spaces with humans.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2011.04554", "id": "2011.04554", "pdf": "https://arxiv.org/pdf/2011.04554", "other": "https://arxiv.org/format/2011.04554"}, "title": "Refer, Reuse, Reduce: Generating Subsequent References in Visual and Conversational Contexts", "author_info": ["Ece Takmaz", "Mario Giulianelli", "Sandro Pezzelle", "Arabella Sinclair", "Raquel Fern\u00e1ndez"], "summary": "Dialogue participants often refer to entities or situations repeatedly within a conversation, which contributes to its cohesiveness. Subsequent references exploit the common ground accumulated by the interlocutors and hence have several interesting properties, namely, they tend to be shorter and reuse expressions that were effective in previous mentions. In this paper, we tackle the generation of first and subsequent references in visually grounded dialogue. We propose a generation model that produces referring utterances grounded in both the visual and the conversational context. To assess the referring effectiveness of its output, we also implement a reference resolution system. Our experiments and analyses show that the model produces better, more effective referring utterances than a model not grounded in the dialogue context, and generates subsequent references that exhibit linguistic patterns akin to humans.", "comment": " Comments: In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020) "}, {"arxiv": {"page": "https://arxiv.org/abs/2011.04260", "id": "2011.04260", "pdf": "https://arxiv.org/pdf/2011.04260", "other": "https://arxiv.org/format/2011.04260"}, "title": "Robust Visual Tracking via Statistical Positive Sample Generation and Gradient Aware Learning", "author_info": ["Lijian Lin", "Haosheng Chen", "Yanjie Liang", "Yan Yan", "Hanzi Wang"], "summary": "In recent years, Convolutional Neural Network (CNN) based trackers have achieved state-of-the-art performance on multiple benchmark datasets. Most of these trackers train a binary classifier to distinguish the target from its background. However, they suffer from two limitations. Firstly, these trackers cannot effectively handle significant appearance variations due to the limited number of positive samples. Secondly, there exists a significant imbalance of gradient contributions between easy and hard samples, where the easy samples usually dominate the computation of gradient. In this paper, we propose a robust tracking method via Statistical Positive sample generation and Gradient Aware learning (SPGA) to address the above two limitations. To enrich the diversity of positive samples, we present an effective and efficient statistical positive sample generation algorithm to generate positive samples in the feature space. Furthermore, to handle the issue of imbalance between easy and hard samples, we propose a gradient sensitive loss to harmonize the gradient contributions between easy and hard samples. Extensive experiments on three challenging benchmark datasets including OTB50, OTB100 and VOT2016 demonstrate that the proposed SPGA performs favorably against several state-of-the-art trackers.", "comment": " Journal ref:         ACM MM Asia2019       "}, {"arxiv": {"page": "https://arxiv.org/abs/2011.00569", "id": "2011.00569", "pdf": "https://arxiv.org/pdf/2011.00569", "other": "https://arxiv.org/format/2011.00569"}, "title": "DeepOpht: Medical Report Generation for Retinal Images via Deep Models and Visual Explanation", "author_info": ["Jia-Hong Huang", "Chao-Han Huck Yang", "Fangyu Liu", "Meng Tian", "Yi-Chieh Liu", "Ting-Wei Wu", "I-Hung Lin", "Kang Wang", "Hiromasa Morikawa", "Hernghua Chang", "Jesper Tegner", "Marcel Worring"], "summary": "In this work, we propose an AI-based method that intends to improve the conventional retinal disease treatment procedure and help ophthalmologists increase diagnosis efficiency and accuracy. The proposed method is composed of a deep neural networks-based (DNN-based) module, including a retinal disease identifier and clinical description generator, and a DNN visual explanation module. To train and validate the effectiveness of our DNN-based module, we propose a large-scale retinal disease image dataset. Also, as ground truth, we provide a retinal image dataset manually labeled by ophthalmologists to qualitatively show, the proposed AI-based method is effective. With our experimental results, we show that the proposed method is quantitatively and qualitatively effective. Our method is capable of creating meaningful retinal image descriptions and visual explanations that are clinically relevant.", "comment": " Comments: Accepted to IEEE WACV 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2011.00307", "id": "2011.00307", "pdf": "https://arxiv.org/pdf/2011.00307", "other": "https://arxiv.org/format/2011.00307"}, "title": "General Data Analytics with Applications to Visual Information Analysis: A Provable Backward-Compatible Semisimple Paradigm over T-Algebra", "author_info": ["Liang Liao", "Stephen John Maybank"], "summary": "We consider a novel backward-compatible paradigm of general data analytics over a recently-reported semisimple algebra (called t-algebra). We study the abstract algebraic framework over the t-algebra by representing the elements of t-algebra by fix-sized multi-way arrays of complex numbers and the algebraic structure over the t-algebra by a collection of direct-product constituents. Over the t-algebra, many algorithms are generalized in a straightforward manner using this new semisimple paradigm. To demonstrate the new paradigm's performance and its backward-compatibility, we generalize some canonical algorithms for visual pattern analysis. Experiments on public datasets show that the generalized algorithms compare favorably with their canonical counterparts.", "comment": " Comments: 38 page, 12 figures. two typos are removed. Official code repository: https://github.com/liaoliang2020/talgebra "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.15137", "id": "2010.15137", "pdf": "https://arxiv.org/pdf/2010.15137", "other": "https://arxiv.org/format/2010.15137"}, "title": "Visualizing Quasiparticles from Quantum Entanglement for general 1D phases", "author_info": ["Elisabeth Wybo", "Frank Pollmann", "S. L. Sondhi", "Yizhi You"], "summary": "In this work, we present a quantum information framework for the entanglement behavior of the low energy quasiparticle (QP) excitations in various quantum phases in one-dimensional (1D) systems. We first establish an exact correspondence between the correlation matrix and the QP entanglement Hamiltonian for free fermions and find an extended in-gap state in the QP entanglement Hamiltonian as a consequence of the position uncertainty of the QP. A more general understanding of such an in-gap state can be extended to a Kramers theorem for the QP entanglement Hamiltonian, which also applies to strongly interacting systems. Further, we present a set of ubiquitous entanglement spectrum features, dubbed entanglement fragmentation, conditional mutual information, and measurement induced non-local entanglement for QPs in 1D symmetry protected topological phases. Our result thus provides a new framework to identify different phases of matter in terms of their QP entanglement.", "comment": " Journal ref:         Phys. Rev. B 103, 115120 (2021)       "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.12852", "id": "2010.12852", "pdf": "https://arxiv.org/pdf/2010.12852", "other": "https://arxiv.org/format/2010.12852"}, "title": "Beyond VQA: Generating Multi-word Answer and Rationale to Visual Questions", "author_info": ["Radhika Dua", "Sai Srinivas Kancheti", "Vineeth N Balasubramanian"], "summary": "Visual Question Answering is a multi-modal task that aims to measure high-level visual understanding. Contemporary VQA models are restrictive in the sense that answers are obtained via classification over a limited vocabulary (in the case of open-ended VQA), or via classification over a set of multiple-choice-type answers. In this work, we present a completely generative formulation where a multi-word answer is generated for a visual query. To take this a step forward, we introduce a new task: ViQAR (Visual Question Answering and Reasoning), wherein a model must generate the complete answer and a rationale that seeks to justify the generated answer. We propose an end-to-end architecture to solve this task and describe how to evaluate it. We show that our model generates strong answers and rationales through qualitative and quantitative evaluation, as well as through a human Turing Test.", "comment": " Comments: MULA Workshop, CVPR 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.09975", "id": "2010.09975", "pdf": "https://arxiv.org/pdf/2010.09975", "other": "https://arxiv.org/format/2010.09975"}, "title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "author_info": ["Danqing Shi", "Xinyue Xu", "Fuling Sun", "Yang Shi", "Nan Cao"], "summary": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2010.06740", "id": "2010.06740", "pdf": "https://arxiv.org/pdf/2010.06740", "other": "https://arxiv.org/format/2010.06740"}, "title": "Measuring Visual Generalization in Continuous Control from Pixels", "author_info": ["Jake Grigsby", "Yanjun Qi"], "summary": "Self-supervised learning and data augmentation have significantly reduced the performance gap between state and image-based reinforcement learning agents in continuous control tasks. However, it is still unclear whether current techniques can face a variety of visual conditions required by real-world environments. We propose a challenging benchmark that tests agents' visual generalization by adding graphical variety to existing continuous control domains. Our empirical analysis shows that current methods struggle to generalize across a diverse set of visual changes, and we examine the specific factors of variation that make these tasks difficult. We find that data augmentation techniques outperform self-supervised learning approaches and that more significant image transformations provide better visual generalization \\footnote{The benchmark and our augmented actor-critic implementation are open-sourced @ https://github.com/QData/dmc_remastered)", "comment": " Comments: A total of 20 pages, 8 pages as the main text "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.03644", "id": "2010.03644", "pdf": "https://arxiv.org/pdf/2010.03644", "other": "https://arxiv.org/format/2010.03644"}, "title": "Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations", "author_info": ["Wanrong Zhu", "Xin Eric Wang", "Pradyumna Narayana", "Kazoo Sone", "Sugato Basu", "William Yang Wang"], "summary": "A major challenge in visually grounded language generation is to build robust benchmark datasets and models that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and benchmarks are reliable. In this work, we set forth to design a set of experiments to understand an important but often ignored problem in visually grounded language generation: given that humans have different utilities and visual attention, how will the sample variance in multi-reference datasets affect the models' performance? Empirically, we study several multi-reference datasets and corresponding vision-and-language tasks. We show that it is of paramount importance to report variance in experiments; that human-generated references could vary drastically in different datasets/tasks, revealing the nature of each task; that metric-wise, CIDEr has shown systematically larger variances than others. Our evaluations on reference-per-instance shed light on the design of reliable datasets in the future.", "comment": " Comments: EMNLP 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2010.01558", "id": "2010.01558", "pdf": "https://arxiv.org/pdf/2010.01558", "other": "https://arxiv.org/format/2010.01558"}, "title": "Space-Time Computation and Visualization of the Electromagnetic Fields and Potentials Generated by Moving Point Charges", "author_info": ["Matthew J. Filipovich", "Stephen Hughes"], "summary": "We present a computational methodology to directly calculate and visualize the directional components of the Coulomb, radiation, and total electromagnetic fields, as well as the scalar and vector potentials, generated by moving point charges in arbitrary motion with varying speeds. Our method explicitly calculates the retarded time of the point charge along a discretized grid which is then used to determine the fields and potentials. The computational approach, implemented in Python, provides an intuitive understanding of the electromagnetic waves generated by moving point charges and can be used as a pedagogical tool for undergraduate and graduate-level electromagnetic theory courses. Our computer code, freely available for download, can also approximate complicated time-varying continuous charge and current densities, and can be used in conjunction with grid-based numerical modeling methods to solve real-world computational electromagnetics problems, such as experiments with high-energy electron sources. We simulate and discuss several interesting example applications and lab experiments including electric and magnetic dipoles, oscillating and linear accelerating point charges, synchrotron radiation, and Bremsstrahlung.", "comment": " Comments: 9 pages, 15 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2009.14509", "id": "2009.14509", "pdf": "https://arxiv.org/pdf/2009.14509", "other": "https://arxiv.org/format/2009.14509"}, "title": "Towards Target-Driven Visual Navigation in Indoor Scenes via Generative Imitation Learning", "author_info": ["Qiaoyun Wu", "Xiaoxi Gong", "Kai Xu", "Dinesh Manocha", "Jingxuan Dong", "Jun Wang"], "summary": "We present a target-driven navigation system to improve mapless visual navigation in indoor scenes. Our method takes a multi-view observation of a robot and a target as inputs at each time step to provide a sequence of actions that move the robot to the target without relying on odometry or GPS at runtime. The system is learned by optimizing a combinational objective encompassing three key designs. First, we propose that an agent conceives the next observation before making an action decision. This is achieved by learning a variational generative module from expert demonstrations. We then propose predicting static collision in advance, as an auxiliary task to improve safety during navigation. Moreover, to alleviate the training data imbalance problem of termination action prediction, we also introduce a target checking module to differentiate from augmenting navigation policy with a termination action. The three proposed designs all contribute to the improved training data efficiency, static collision avoidance, and navigation generalization performance, resulting in a novel target-driven mapless navigation system. Through experiments on a TurtleBot, we provide evidence that our model can be integrated into a robotic system and navigate in the real world. Videos and models can be found in the supplementary material.", "comment": " Comments: 11 pages, accepted by IEEE Robotics and Automation Letters "}, {"arxiv": {"page": "https://arxiv.org/abs/2009.09500", "id": "2009.09500", "pdf": "https://arxiv.org/pdf/2009.09500"}, "title": "3D Primitives Gpgpu Generation for Volume Visualization in 3D Graphics Systems", "author_info": ["Anas M. Al-Oraiqat", "Sergii A. Zori"], "summary": "This article discusses the study of 3D graphic volume primitive computer system generation (3D segments) based on General Purpose Graphics Processing Unit (GPGPU) technology for 3D volume visualization systems. It is based on the general method of Volume 3D primitive generation and an algorithm for the voxelization of 3D lines, previously proposed and studied by the authors. We considered the Compute Unified Device Architect (CUDA) implementation of a parametric method for generating 3D line segments and characteristics of generation on modern Graphics Processing Units. Experiments on the test bench showed the relative inefficiency of generating a single 3D line segment and the efficiency of generating both fixed and arbitrary length of 3D segments on a Graphics Processing Unit (GPU). Experimental studies have proven the effectiveness and the quality of produced solutions by our method, when compared to existing state-of-the-art approaches.", "comment": " Journal ref:         journal of king abdulaziz university computing and information technology sciences, Vol. 9, issue 2, 2020       "}, {"arxiv": {"page": "https://arxiv.org/abs/2009.09384", "id": "2009.09384", "pdf": "https://arxiv.org/pdf/2009.09384", "other": "https://arxiv.org/format/2009.09384"}, "title": "Deriving Visual Semantics from Spatial Context: An Adaptation of LSA and Word2Vec to generate Object and Scene Embeddings from Images", "author_info": ["Matthias S. Treder", "Juan Mayor-Torres", "Christoph Teufel"], "summary": "Embeddings are an important tool for the representation of word meaning. Their effectiveness rests on the distributional hypothesis: words that occur in the same context carry similar semantic information. Here, we adapt this approach to index visual semantics in images of scenes. To this end, we formulate a distributional hypothesis for objects and scenes: Scenes that contain the same objects (object context) are semantically related. Similarly, objects that appear in the same spatial context (within a scene or subregions of a scene) are semantically related. We develop two approaches for learning object and scene embeddings from annotated images. In the first approach, we adapt LSA and Word2vec's Skipgram and CBOW models to generate two sets of embeddings from object co-occurrences in whole images, one for objects and one for scenes. The representational space spanned by these embeddings suggests that the distributional hypothesis holds for images. In an initial application of this approach, we show that our image-based embeddings improve scene classification models such as ResNet18 and VGG-11 (3.72\\% improvement on Top5 accuracy, 4.56\\% improvement on Top1 accuracy). In the second approach, rather than analyzing whole images of scenes, we focus on co-occurrences of objects within subregions of an image. We illustrate that this method yields a sensible hierarchical decomposition of a scene into collections of semantically related objects. Overall, these results suggest that object and scene embeddings from object co-occurrences and spatial context yield semantically meaningful representations as well as computational improvements for downstream applications such as scene classification.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2009.08566", "id": "2009.08566", "pdf": "https://arxiv.org/pdf/2009.08566", "other": "https://arxiv.org/format/2009.08566"}, "title": "MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering", "author_info": ["Tejas Gokhale", "Pratyay Banerjee", "Chitta Baral", "Yezhou Yang"], "summary": "While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization. In this paper, we present MUTANT, a training paradigm that exposes the model to perceptually similar, yet semantically distinct mutations of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, MUTANT does not rely on the knowledge about the nature of train and test answer distributions. MUTANT establishes a new state-of-the-art accuracy on VQA-CP with a 10.57% improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.", "comment": " Comments: Accepted to EMNLP 2020, Long Papers "}]}