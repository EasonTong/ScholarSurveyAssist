{"data": [{"arxiv": {"page": "https://arxiv.org/abs/2202.10830", "id": "2202.10830", "pdf": "https://arxiv.org/pdf/2202.10830"}, "title": "Co-opted marginality, a new type of anti-immigrant discourse on social media? Classifying social media messages about immigrants with BERT", "author_info": ["Claire Stravato Emes", "Anfan Chen"], "summary": "The article analyzes public discourse about immigrants in comments sections of 11 social media community platforms over six months in Singapore.", "comment": " Comments: 25 pages, 1 figure Submitted to ICA 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10787", "id": "2202.10787", "pdf": "https://arxiv.org/pdf/2202.10787", "other": "https://arxiv.org/format/2202.10787"}, "title": "VU-BERT: A Unified framework for Visual Dialog", "author_info": ["Tong Ye", "Shijing Si", "Jianzong Wang", "Rui Wang", "Ning Cheng", "Jing Xiao"], "summary": "The visual dialog task attempts to train an agent to answer multi-turn questions given an image, which requires the deep understanding of interactions between the image and dialog history. Existing researches tend to employ the modality-specific modules to model the interactions, which might be troublesome to use. To fill in this gap, we propose a unified framework for image-text joint embedding, named VU-BERT, and apply patch projection to obtain vision embedding firstly in visual dialog tasks to simplify the model. The model is trained over two tasks: masked language modeling and next utterance retrieval. These tasks help in learning visual concepts, utterances dependence, and the relationships between these two modalities. Finally, our VU-BERT achieves competitive performance (0.7287 NDCG scores) on VisDial v1.0 Datasets.", "comment": " Comments: 5 pages, 2 figures, accepted by 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2022) "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10101", "id": "2202.10101", "pdf": "https://arxiv.org/pdf/2202.10101", "other": "https://arxiv.org/format/2202.10101"}, "title": "BERT WEAVER: Using WEight AVERaging to Enable Lifelong Learning for Transformer-based Models", "author_info": ["Lisa Langnickel", "Alexander Schulz", "Barbara Hammer", "Juliane Fluck"], "summary": "Recent developments in transfer learning have boosted the advancements in natural language processing tasks. The performance is, however, dependent on high-quality, manually annotated training data. Especially in the biomedical domain, it has been shown that one training corpus is not enough to learn generic models that are able to efficiently predict on new data. Therefore, state-of-the-art models need the ability of lifelong learning in order to improve performance as soon as new data are available - without the need of retraining the whole model from scratch. We present WEAVER, a simple, yet efficient post-processing method that infuses old knowledge into the new model, thereby reducing catastrophic forgetting. We show that applying WEAVER in a sequential manner results in similar word embedding distributions as doing a combined training on all data at once, while being computationally more efficient. Because there is no need of data sharing, the presented method is also easily applicable to federated learning settings and can for example be beneficial for the mining of electronic health records from different clinics.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08974", "id": "2202.08974", "pdf": "https://arxiv.org/pdf/2202.08974", "other": "https://arxiv.org/format/2202.08974"}, "title": "Multimodal Emotion Recognition using Transfer Learning from Speaker Recognition and BERT-based models", "author_info": ["Sarala Padi", "Seyed Omid Sadjadi", "Dinesh Manocha", "Ram D. Sriram"], "summary": "Automatic emotion recognition plays a key role in computer-human interaction as it has the potential to enrich the next-generation artificial intelligence with emotional intelligence. It finds applications in customer and/or representative behavior analysis in call centers, gaming, personal assistants, and social robots, to mention a few. Therefore, there has been an increasing demand to develop robust automatic methods to analyze and recognize the various emotions. In this paper, we propose a neural network-based emotion recognition framework that uses a late fusion of transfer-learned and fine-tuned models from speech and text modalities. More specifically, we i) adapt a residual network (ResNet) based model trained on a large-scale speaker recognition task using transfer learning along with a spectrogram augmentation approach to recognize emotions from speech, and ii) use a fine-tuned bidirectional encoder representations from transformers (BERT) based model to represent and recognize emotions from the text. The proposed system then combines the ResNet and BERT-based model scores using a late fusion strategy to further improve the emotion recognition performance. The proposed multimodal solution addresses the data scarcity limitation in emotion recognition using transfer learning, data augmentation, and fine-tuning, thereby improving the generalization performance of the emotion recognition models. We evaluate the effectiveness of our proposed multimodal approach on the interactive emotional dyadic motion capture (IEMOCAP) dataset. Experimental results indicate that both audio and text-based models improve the emotion recognition performance and that the proposed multimodal solution achieves state-of-the-art results on the IEMOCAP benchmark.", "comment": " Comments: arXiv admin note: substantial text overlap with arXiv:2108.02510 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08625", "id": "2202.08625", "pdf": "https://arxiv.org/pdf/2202.08625", "other": "https://arxiv.org/format/2202.08625"}, "title": "Revisiting Over-smoothing in BERT from the Perspective of Graph", "author_info": ["Han Shi", "Jiahui Gao", "Hang Xu", "Xiaodan Liang", "Zhenguo Li", "Lingpeng Kong", "Stephen M. S. Lee", "James T. Kwok"], "summary": "Recently over-smoothing phenomenon of Transformer-based models is observed in both vision and language fields. However, no existing work has delved deeper to further investigate the main cause of this phenomenon. In this work, we make the attempt to analyze the over-smoothing problem from the perspective of graph, where such problem was first discovered and explored. Intuitively, the self-attention matrix can be seen as a normalized adjacent matrix of a corresponding graph. Based on the above connection, we provide some theoretical analysis and find that layer normalization plays a key role in the over-smoothing issue of Transformer-based models. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace and result in over-smoothing. To alleviate the over-smoothing problem, we consider hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse. Extensive experiment results on various data sets illustrate the effect of our fusion method.", "comment": " Comments: Accepted by ICLR 2022 (Spotlight) "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06769", "id": "2202.06769", "pdf": "https://arxiv.org/pdf/2202.06769", "other": "https://arxiv.org/format/2202.06769"}, "title": "Punctuation restoration in Swedish through fine-tuned KB-BERT", "author_info": ["John Bj\u00f6rkman Nilsson"], "summary": "Presented here is a method for automatic punctuation restoration in Swedish using a BERT model. The method is based on KB-BERT, a publicly available, neural network language model pre-trained on a Swedish corpus by National Library of Sweden. This model has then been fine-tuned for this specific task using a corpus of government texts. With a lower-case and unpunctuated Swedish text as input, the model is supposed to return a grammatically correct punctuated copy of the text as output. A successful solution to this problem brings benefits for an array of NLP domains, such as speech-to-text and automated text. Only the punctuation marks period, comma and question marks were considered for the project, due to a lack of data for more rare marks such as semicolon. Additionally, some marks are somewhat interchangeable with the more common, such as exclamation points and periods. Thus, the data set had all exclamation points replaced with periods. The fine-tuned Swedish BERT model, dubbed prestoBERT, achieved an overall F1-score of 78.9. The proposed model scored similarly to international counterparts, with Hungarian and Chinese models obtaining F1-scores of 82.2 and 75.6 respectively. As further comparison, a human evaluation case study was carried out. The human test group achieved an overall F1-score of 81.7, but scored substantially worse than prestoBERT on both period and comma. Inspecting output sentences from the model and humans show satisfactory results, despite the difference in F1-score. The disconnect seems to stem from an unnecessary focus on replicating the exact same punctuation used in the test set, rather than providing any of the number of correct interpretations. If the loss function could be rewritten to reward all grammatically correct outputs, rather than only the one original example, the performance could improve significantly for both prestoBERT and the human group.", "comment": " Journal ref:         TRITA-EECS-EX ; 2021:526       "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06335", "id": "2202.06335", "pdf": "https://arxiv.org/pdf/2202.06335", "other": "https://arxiv.org/format/2202.06335"}, "title": "ET-BERT: A Contextualized Datagram Representation with Pre-training Transformers for Encrypted Traffic Classification", "author_info": ["Xinjie Lin", "Gang Xiong", "Gaopeng Gou", "Zhen Li", "Junzheng Shi", "Jing Yu"], "summary": "Encrypted traffic classification requires discriminative and robust traffic representation captured from content-invisible and imbalanced traffic data for accurate classification, which is challenging but indispensable to achieve network security and network management. The major limitation of existing solutions is that they highly rely on the deep features, which are overly dependent on data size and hard to generalize on unseen data. How to leverage the open-domain unlabeled traffic data to learn representation with strong generalization ability remains a key challenge. In this paper,we propose a new traffic representation model called Encrypted Traffic Bidirectional Encoder Representations from Transformer (ET-BERT), which pre-trains deep contextualized datagram-level representation from large-scale unlabeled data. The pre-trained model can be fine-tuned on a small number of task-specific labeled data and achieves state-of-the-art performance across five encrypted traffic classification tasks, remarkably pushing the F1 of ISCX-Tor to 99.2% (4.4% absolute improvement), ISCX-VPN-Service to 98.9% (5.2% absolute improvement), Cross-Platform (Android) to 92.5% (5.4% absolute improvement), CSTNET-TLS 1.3 to 97.4% (10.0% absolute improvement). Notably, we provide explanation of the empirically powerful pre-training model by analyzing the randomness of ciphers. It gives us insights in understanding the boundary of classification ability over encrypted traffic. The code is available at: https://github.com/linwhitehat/ET-BERT.", "comment": " Comments: This work has been accepted in Security, Privacy, and Trust track at The Web Conference 2022 (WWW'22)(see https://www2022.thewebconf.org/cfp/research/security/) "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05778", "id": "2202.05778", "pdf": "https://arxiv.org/pdf/2202.05778", "other": "https://arxiv.org/format/2202.05778"}, "title": "White-Box Attacks on Hate-speech BERT Classifiers in German with Explicit and Implicit Character Level Defense", "author_info": ["Shahrukh Khan", "Mahnoor Shahid", "Navdeeppal Singh"], "summary": "In this work, we evaluate the adversarial robustness of BERT models trained on German Hate Speech datasets. We also complement our evaluation with two novel white-box character and word level attacks thereby contributing to the range of attacks available. Furthermore, we also perform a comparison of two novel character-level defense strategies and evaluate their robustness with one another.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05317", "id": "2202.05317", "pdf": "https://arxiv.org/pdf/2202.05317", "other": "https://arxiv.org/format/2202.05317"}, "title": "A Multi-task Learning Framework for Product Ranking with BERT", "author_info": ["Xuyang Wu", "Alessandro Magnani", "Suthee Chaidaroon", "Ajit Puthenputhussery", "Ciya Liao", "Yi Fang"], "summary": "Product ranking is a crucial component for many e-commerce services. One of the major challenges in product search is the vocabulary mismatch between query and products, which may be a larger vocabulary gap problem compared to other information retrieval domains. While there is a growing collection of neural learning to match methods aimed specifically at overcoming this issue, they do not leverage the recent advances of large language models for product search. On the other hand, product ranking often deals with multiple types of engagement signals such as clicks, add-to-cart, and purchases, while most of the existing works are focused on optimizing one single metric such as click-through rate, which may suffer from data sparsity. In this work, we propose a novel end-to-end multi-task learning framework for product ranking with BERT to address the above challenges. The proposed model utilizes domain-specific BERT with fine-tuning to bridge the vocabulary gap and employs multi-task learning to optimize multiple objectives simultaneously, which yields a general end-to-end learning framework for product search. We conduct a set of comprehensive experiments on a real-world e-commerce dataset and demonstrate significant improvement of the proposed approach over the state-of-the-art baseline methods.", "comment": " Comments: accepted by WWW22 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03651", "id": "2202.03651", "pdf": "https://arxiv.org/pdf/2202.03651", "other": "https://arxiv.org/format/2202.03651"}, "title": "Causal Scene BERT: Improving object detection by searching for challenging groups of data", "author_info": ["Cinjon Resnick", "Or Litany", "Amlan Kar", "Karsten Kreis", "James Lucas", "Kyunghyun Cho", "Sanja Fidler"], "summary": "Modern computer vision applications rely on learning-based perception modules parameterized with neural networks for tasks like object detection. These modules frequently have low expected error overall but high error on atypical groups of data due to biases inherent in the training process. In building autonomous vehicles (AV), this problem is an especially important challenge because their perception modules are crucial to the overall system performance. After identifying failures in AV, a human team will comb through the associated data to group perception failures that share common causes. More data from these groups is then collected and annotated before retraining the model to fix the issue. In other words, error groups are found and addressed in hindsight. Our main contribution is a pseudo-automatic method to discover such groups in foresight by performing causal interventions on simulated scenes. To keep our interventions on the data manifold, we utilize masked language models. We verify that the prioritized groups found via intervention are challenging for the object detector and show that retraining with data collected from these groups helps inordinately compared to adding more IID data. We also plan to release software to run interventions in simulated scenes, which we hope will benefit the causality community.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03480", "id": "2202.03480", "pdf": "https://arxiv.org/pdf/2202.03480"}, "title": "Universal Spam Detection using Transfer Learning of BERT Model", "author_info": ["Vijay Srinivas Tida", "Sonya Hsu"], "summary": "Deep learning transformer models become important by training on text data based on self-attention mechanisms. This manuscript demonstrated a novel universal spam detection model using pre-trained Google's Bidirectional Encoder Representations from Transformers (BERT) base uncased models with four datasets by efficiently classifying ham or spam emails in real-time scenarios. Different methods for Enron, Spamassain, Lingspam, and Spamtext message classification datasets, were used to train models individually in which a single model was obtained with acceptable performance on four datasets. The Universal Spam Detection Model (USDM) was trained with four datasets and leveraged hyperparameters from each model. The combined model was finetuned with the same hyperparameters from these four models separately. When each model using its corresponding dataset, an F1-score is at and above 0.9 in individual models. An overall accuracy reached 97%, with an F1 score of 0.96. Research results and implications were discussed.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.01159", "id": "2202.01159", "pdf": "https://arxiv.org/pdf/2202.01159", "other": "https://arxiv.org/format/2202.01159"}, "title": "L3Cube-MahaCorpus and MahaBERT: Marathi Monolingual Corpus, Marathi BERT Language Models, and Resources", "author_info": ["Raviraj Joshi"], "summary": "We present L3Cube-MahaCorpus a Marathi monolingual data set scraped from different internet sources. We expand the existing Marathi monolingual corpus with 24.8M sentences and 289M tokens. We further present, MahaBERT, MahaAlBERT, and MahaRoBerta all BERT-based masked language models, and MahaFT, the fast text word embeddings both trained on full Marathi corpus with 752M tokens. We show the effectiveness of these resources on downstream classification and NER tasks. Marathi is a popular language in India but still lacks these resources. This work is a step forward in building open resources for the Marathi language. The data and models are available at https://github.com/l3cube-pune/MarathiNLP .", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.01094", "id": "2202.01094", "pdf": "https://arxiv.org/pdf/2202.01094", "other": "https://arxiv.org/format/2202.01094"}, "title": "RescoreBERT: Discriminative Speech Recognition Rescoring with BERT", "author_info": ["Liyan Xu", "Yile Gu", "Jari Kolehmainen", "Haidar Khan", "Ankur Gandhe", "Ariya Rastrow", "Andreas Stolcke", "Ivan Bulyko"], "summary": "Second-pass rescoring is an important component in automatic speech recognition (ASR) systems that is used to improve the outputs from a first-pass decoder by implementing a lattice rescoring or n-best re-ranking. While pretraining with a masked language model (MLM) objective has received great success in various natural language understanding (NLU) tasks, it has not gained traction as a rescoring model for ASR. Specifically, training a bidirectional model like BERT on a discriminative objective such as minimum WER (MWER) has not been explored. Here we show how to train a BERT-based rescoring model with MWER loss, to incorporate the improvements of a discriminative loss into fine-tuning of deep bidirectional pretrained models for ASR. Specifically, we propose a fusion strategy that incorporates the MLM into the discriminative training process to effectively distill knowledge from a pretrained model. We further propose an alternative discriminative loss. This approach, which we call RescoreBERT, reduces WER by 6.6%/3.4% relative on the LibriSpeech clean/other test sets over a BERT baseline without discriminative objective. We also evaluate our method on an internal dataset from a conversational agent and find that it reduces both latency and WER (by 3 to 8% relative) over an LSTM rescoring model.", "comment": " Comments: Accepted to ICASSP 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.00795", "id": "2202.00795", "pdf": "https://arxiv.org/pdf/2202.00795", "other": "https://arxiv.org/format/2202.00795"}, "title": "Disaster Tweets Classification using BERT-Based Language Model", "author_info": ["Anh Duc Le"], "summary": "Social networking services have became an important communication channel in time of emergency. The aim of this study is to create a machine learning language model that is able to investigate if a person or area was in danger or not. The ubiquitousness of smartphones enables people to announce an emergency they are observing in real-time. Because of this, more agencies are interested in programmatically monitoring Twitter (i.e. disaster relief organizations and news agencies). Design a language model that is able to understand and acknowledge when a disaster is happening based on the social network posts will become more and more necessary over time.", "comment": " Comments: arXiv admin note: text overlap with arXiv:2102.12162 by other authors "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.00477", "id": "2202.00477", "pdf": "https://arxiv.org/pdf/2202.00477"}, "title": "Detection of Increased Time Intervals of Anti-Vaccine Tweets for COVID-19 Vaccine with BERT Model", "author_info": ["\u00dclk\u00fc Tuncer K\u00fc\u00e7\u00fckta\u015f", "Fatih Uysal", "F\u0131rat Hardala\u00e7", "\u0130smail Biri"], "summary": "The most effective of the solutions against Covid-19 is the various vaccines developed. Distrust of vaccines can hinder the rapid and effective use of this remedy. One of the means of expressing the thoughts of society is social media. Determining the time intervals during which anti-vaccination increases in social media can help institutions determine the strategy to be used in combating anti-vaccination. Recording and tracking every tweet entered with human labor would be inefficient, so various automation solutions are needed. In this study, The Bidirectional Encoder Representations from Transformers (BERT) model, which is a deep learning-based natural language processing (NLP) model, was used. In a dataset of 1506 tweets divided into four different categories as news, irrelevant, anti-vaccine, and vaccine supporters, the model was trained with a learning rate of 5e-6 for 25 epochs. To determine the intervals in which anti-vaccine tweets are concentrated, the categories to which 652840 tweets belong were determined by using the trained model. The change of the determined categories overtime was visualized and the events that could cause the change were determined. As a result of model training, in the test dataset, the f-score of 0.81 and AUC values for different classes were obtained as 0.99,0.91, 0.92, 0.92, respectively. In this model, unlike the studies in the literature, an auxiliary system is designed that provides data that institutions can use when determining their strategy by measuring and visualizing the frequency of anti-vaccine tweets in a time interval, different from detecting and censoring such tweets.", "comment": " Comments: in Turkish language. Accepted at 1st International Congress on Artificial Intelligence and Data Science (ICADA 2021) "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.00373", "id": "2202.00373", "pdf": "https://arxiv.org/pdf/2202.00373", "other": "https://arxiv.org/format/2202.00373"}, "title": "Improving BERT-based Query-by-Document Retrieval with Multi-Task Optimization", "author_info": ["Amin Abolghasemi", "Suzan Verberne", "Leif Azzopardi"], "summary": "Query-by-document (QBD) retrieval is an Information Retrieval task in which a seed document acts as the query and the goal is to retrieve related documents -- it is particular common in professional search tasks. In this work we improve the retrieval effectiveness of the BERT re-ranker, proposing an extension to its fine-tuning step to better exploit the context of queries. To this end, we use an additional document-level representation learning objective besides the ranking objective when fine-tuning the BERT re-ranker. Our experiments on two QBD retrieval benchmarks show that the proposed multi-task optimization significantly improves the ranking effectiveness without changing the BERT re-ranker or using additional training samples. In future work, the generalizability of our approach to other retrieval tasks should be further investigated.", "comment": " Comments: Accepted for publication in the 44th European Conference on Information Retrieval (ECIR2022) "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11176", "id": "2201.11176", "pdf": "https://arxiv.org/pdf/2201.11176", "other": "https://arxiv.org/format/2201.11176"}, "title": "DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence", "author_info": ["Wei Zhao", "Michael Strube", "Steffen Eger"], "summary": "Recently, there has been a growing interest in designing text generation systems from a discourse coherence perspective, e.g., modeling the interdependence between sentences. Still, recent BERT-based evaluation metrics cannot recognize coherence and fail to punish incoherent elements in system outputs. In this work, we introduce DiscoScore, a parametrized discourse metric, which uses BERT to model discourse coherence from different perspectives, driven by Centering theory. Our experiments encompass 16 non-discourse and discourse metrics, including DiscoScore and popular coherence models, evaluated on summarization and document-level machine translation (MT). We find that (i) the majority of BERT-based metrics correlate much worse with human rated coherence than early discourse metrics, invented a decade ago; (ii) the recent state-of-the-art BARTScore is weak when operated at system level -- which is particularly problematic as systems are typically compared in this manner. DiscoScore, in contrast, achieves strong system-level correlation with human ratings, not only in coherence but also in factual consistency and other aspects, and surpasses BARTScore by over 10 correlation points on average. Further, aiming to understand DiscoScore, we provide justifications to the importance of discourse coherence for evaluation metrics, and explain the superiority of one variant over another. Our code is available at \\url{https://github.com/AIPHES/DiscoScore}.", "comment": " Comments: v2: small fixes in the abstract "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.05363", "id": "2201.05363", "pdf": "https://arxiv.org/pdf/2201.05363", "other": "https://arxiv.org/format/2201.05363"}, "title": "Polarity and Subjectivity Detection with Multitask Learning and BERT Embedding", "author_info": ["Ranjan Satapathy", "Shweta Pardeshi", "Erik Cambria"], "summary": "Multitask learning often helps improve the performance of related tasks as these often have inter-dependence on each other and perform better when solved in a joint framework. In this paper, we present a deep multitask learning framework that jointly performs polarity and subjective detection. We propose an attention-based multitask model for predicting polarity and subjectivity. The input sentences are transformed into vectors using pre-trained BERT and Glove embeddings, and the results depict that BERT embedding based model works better than the Glove based model. We compare our approach with state-of-the-art models in both subjective and polarity classification single-task and multitask frameworks. The proposed approach reports baseline performances for both polarity detection and subjectivity detection.", "comment": " Comments: 10 pages, 4 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.04843", "id": "2201.04843", "pdf": "https://arxiv.org/pdf/2201.04843", "other": "https://arxiv.org/format/2201.04843"}, "title": "LP-BERT: Multi-task Pre-training Knowledge Graph BERT for Link Prediction", "author_info": ["Da Li", "Ming Yi", "Yukai He"], "summary": "Link prediction plays an significant role in knowledge graph, which is an important resource for many artificial intelligence tasks, but it is often limited by incompleteness. In this paper, we propose knowledge graph BERT for link prediction, named LP-BERT, which contains two training stages: multi-task pre-training and knowledge graph fine-tuning. The pre-training strategy not only uses Mask Language Model (MLM) to learn the knowledge of context corpus, but also introduces Mask Entity Model (MEM) and Mask Relation Model (MRM), which can learn the relationship information from triples by predicting semantic based entity and relation elements. Structured triple relation information can be transformed into unstructured semantic information, which can be integrated into the pre-training model together with context corpus information. In the fine-tuning phase, inspired by contrastive learning, we carry out a triple-style negative sampling in sample batch, which greatly increased the proportion of negative sampling while keeping the training time almost unchanged. Furthermore, we propose a data augmentation method based on the inverse relationship of triples to further increase the sample diversity. We achieve state-of-the-art results on WN18RR and UMLS datasets, especially the Hits@10 indicator improved by 5\\% from the previous state-of-the-art result on WN18RR dataset.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.04458", "id": "2201.04458", "pdf": "https://arxiv.org/pdf/2201.04458", "other": "https://arxiv.org/format/2201.04458"}, "title": "Diagnosing BERT with Retrieval Heuristics", "author_info": ["Arthur C\u00e2mara", "Claudia Hauff"], "summary": "Word embeddings, made widely popular in 2013 with the release of word2vec, have become a mainstay of NLP engineering pipelines. Recently, with the release of BERT, word embeddings have moved from the term-based embedding space to the contextual embedding space -- each term is no longer represented by a single low-dimensional vector but instead each term and \\emph{its context} determine the vector weights. BERT's setup and architecture have been shown to be general enough to be applicable to many natural language tasks. Importantly for Information Retrieval (IR), in contrast to prior deep learning solutions to IR problems which required significant tuning of neural net architectures and training regimes, \"vanilla BERT\" has been shown to outperform existing retrieval algorithms by a wide margin, including on tasks and corpora that have long resisted retrieval effectiveness gains over traditional IR baselines (such as Robust04). In this paper, we employ the recently proposed axiomatic dataset analysis technique -- that is, we create diagnostic datasets that each fulfil a retrieval heuristic (both term matching and semantic-based) -- to explore what BERT is able to learn. In contrast to our expectations, we find BERT, when applied to a recently released large-scale web corpus with ad-hoc topics, to \\emph{not} adhere to any of the explored axioms. At the same time, BERT outperforms the traditional query likelihood retrieval model by 40\\%. This means that the axiomatic approach to IR (and its extension of diagnostic datasets created for retrieval heuristics) may in its current form not be applicable to large-scale corpora. Additional -- different -- axioms are needed.", "comment": " Journal ref:         Advances in Information Retrieval. 2020;12035:605-618. Published 2020 Mar 17       "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.04337", "id": "2201.04337", "pdf": "https://arxiv.org/pdf/2201.04337", "other": "https://arxiv.org/format/2201.04337"}, "title": "PromptBERT: Improving BERT Sentence Embeddings with Prompts", "author_info": ["Ting Jiang", "Shaohan Huang", "Zihan Zhang", "Deqing Wang", "Fuzhen Zhuang", "Furu Wei", "Haizhen Huang", "Liangjie Zhang", "Qi Zhang"], "summary": "The poor performance of the original BERT for sentence semantic similarity has been widely discussed in previous works. We find that unsatisfactory performance is mainly due to the static token embeddings biases and the ineffective BERT layers, rather than the high cosine similarity of the sentence embeddings. To this end, we propose a prompt based sentence embeddings method which can reduce token embeddings biases and make the original BERT layers more effective. By reformulating the sentence embeddings task as the fillin-the-blanks problem, our method significantly improves the performance of original BERT. We discuss two prompt representing methods and three prompt searching methods for prompt based sentence embeddings. Moreover, we propose a novel unsupervised training objective by the technology of template denoising, which substantially shortens the performance gap between the supervised and unsupervised setting. For experiments, we evaluate our method on both non fine-tuned and fine-tuned settings. Even a non fine-tuned method can outperform the fine-tuned methods like unsupervised ConSERT on STS tasks. Our fine-tuned method outperforms the state-of-the-art method SimCSE in both unsupervised and supervised settings. Compared to SimCSE, we achieve 2.29 and 2.58 points improvements on BERT and RoBERTa respectively under the unsupervised setting.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.03382", "id": "2201.03382", "pdf": "https://arxiv.org/pdf/2201.03382", "other": "https://arxiv.org/format/2201.03382"}, "title": "BERT for Sentiment Analysis: Pre-trained and Fine-Tuned Alternatives", "author_info": ["Frederico Souza", "Jo\u00e3o Filho"], "summary": "BERT has revolutionized the NLP field by enabling transfer learning with large language models that can capture complex textual patterns, reaching the state-of-the-art for an expressive number of NLP applications. For text classification tasks, BERT has already been extensively explored. However, aspects like how to better cope with the different embeddings provided by the BERT output layer and the usage of language-specific instead of multilingual models are not well studied in the literature, especially for the Brazilian Portuguese language. The purpose of this article is to conduct an extensive experimental study regarding different strategies for aggregating the features produced in the BERT output layer, with a focus on the sentiment analysis task. The experiments include BERT models trained with Brazilian Portuguese corpora and the multilingual version, contemplating multiple aggregation strategies and open-source datasets with predefined training, validation, and test partitions to facilitate the reproducibility of the results. BERT achieved the highest ROC-AUC values for the majority of cases as compared to TF-IDF. Nonetheless, TF-IDF represents a good trade-off between the predictive performance and computational cost.", "comment": " Comments: 10 pages, 1 figure, 3 tables. Accepted at International Conference on the Computational Processing of Portuguese (PROPOR 2022), but not yet published "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.03327", "id": "2201.03327", "pdf": "https://arxiv.org/pdf/2201.03327", "other": "https://arxiv.org/format/2201.03327"}, "title": "TiltedBERT: Resource Adjustable Version of BERT", "author_info": ["Sajjad Kachuee", "Mohammad Sharifkhani"], "summary": "In this paper, we proposed a novel adjustable finetuning method that improves the training and inference time of the BERT model on downstream tasks. In the proposed method, we first detect more important word vectors in each layer by our proposed redundancy metric and then eliminate the less important word vectors with our proposed strategy. In our method, the word vector elimination rate in each layer is controlled by the Tilt-Rate hyper-parameter, and the model learns to work with a considerably lower number of Floating Point Operations (FLOPs) than the original BERTbase model. Our proposed method does not need any extra training steps, and also it can be generalized to other transformer-based models. We perform extensive experiments that show the word vectors in higher layers have an impressive amount of redundancy that can be eliminated and decrease the training and inference time. Experimental results on extensive sentiment analysis, classification and regression datasets, and benchmarks like IMDB and GLUE showed that our proposed method is effective in various datasets. By applying our method on the BERTbase model, we decrease the inference time up to 5.3 times with less than 0.85% accuracy degradation on average. After the fine-tuning stage, the inference time of our model can be adjusted with our method offline-tuning property for a wide range of the Tilt-Rate value selections. Also, we propose a mathematical speedup analysis that can estimate the speedup of our method accurately. With the help of this analysis, the proper Tilt-Rate value can be selected before fine-tuning or while offline-tuning stages.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.03115", "id": "2201.03115", "pdf": "https://arxiv.org/pdf/2201.03115", "other": "https://arxiv.org/format/2201.03115"}, "title": "Semantic and sentiment analysis of selected Bhagavad Gita translations using BERT-based language framework", "author_info": ["Rohitash Chandra", "Venkatesh Kulkarni"], "summary": "It is well known that translations of songs and poems not only break rhythm and rhyming patterns, but can also result in loss of semantic information. The Bhagavad Gita is an ancient Hindu philosophical text originally written in Sanskrit that features a conversation between Lord Krishna and Arjuna prior to the Mahabharata war. The Bhagavad Gita is also one of the key sacred texts in Hinduism and is known as the forefront of the Vedic corpus of Hinduism. In the last two centuries, there has been a lot of interest in Hindu philosophy from western scholars; hence, the Bhagavad Gita has been translated in a number of languages. However, there is not much work that validates the quality of the English translations. Recent progress of language models powered by deep learning has enabled not only translations but a better understanding of language and texts with semantic and sentiment analysis. Our work is motivated by the recent progress of language models powered by deep learning methods. In this paper, we present a framework that compares selected translations (from Sanskrit to English) of the Bhagavad Gita using semantic and sentiment analyses. We use hand-labelled sentiment dataset for tuning state-of-art deep learning-based language model known as bidirectional encoder representations from transformers (BERT). We provide sentiment and semantic analysis for selected chapters and verses across translations. Our results show that although the style and vocabulary in the respective translations vary widely, the sentiment analysis and semantic similarity shows that the message conveyed are mostly similar.", "comment": " Journal ref:         IEEE Access, 2022       "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.02119", "id": "2201.02119", "pdf": "https://arxiv.org/pdf/2201.02119"}, "title": "An Opinion Mining of Text in COVID-19 Issues along with Comparative Study in ML, BERT & RNN", "author_info": ["Md. Mahadi Hasan Sany", "Mumenunnesa Keya", "Sharun Akter Khushbu", "Akm Shahariar Azad Rabby", "Abu Kaisar Mohammad Masum"], "summary": "The global world is crossing a pandemic situation where this is a catastrophic outbreak of Respiratory Syndrome recognized as COVID-19. This is a global threat all over the 212 countries that people every day meet with mighty situations. On the contrary, thousands of infected people live rich in mountains. Mental health is also affected by this worldwide coronavirus situation. Due to this situation online sources made a communicative place that common people shares their opinion in any agenda. Such as affected news related positive and negative, financial issues, country and family crisis, lack of import and export earning system etc. different kinds of circumstances are recent trendy news in anywhere. Thus, vast amounts of text are produced within moments therefore, in subcontinent areas the same as situation in other countries and peoples opinion of text and situation also same but the language is different. This article has proposed some specific inputs along with Bangla text comments from individual sources which can assure the goal of illustration that machine learning outcome capable of building an assistive system. Opinion mining assistive system can be impactful in all language preferences possible. To the best of our knowledge, the article predicted the Bangla input text on COVID-19 issues proposed ML algorithms and deep learning models analysis also check the future reachability with a comparative analysis. Comparative analysis states a report on text prediction accuracy is 91% along with ML algorithms and 79% along with Deep Learning Models.", "comment": " Journal ref:         3rd International Conference on Deep Learning, Artificial Intelligence and Robotics, (ICDLAIR) 2021       "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.02010", "id": "2201.02010", "pdf": "https://arxiv.org/pdf/2201.02010", "other": "https://arxiv.org/format/2201.02010"}, "title": "Self-Training Vision Language BERTs with a Unified Conditional Model", "author_info": ["Xiaofeng Yang", "Fengmao Lv", "Fayao Liu", "Guosheng Lin"], "summary": "Natural language BERTs are trained with language corpus in a self-supervised manner. Unlike natural language BERTs, vision language BERTs need paired data to train, which restricts the scale of VL-BERT pretraining. We propose a self-training approach that allows training VL-BERTs from unlabeled image data. The proposed method starts with our unified conditional model -- a vision language BERT model that can perform zero-shot conditional generation. Given different conditions, the unified conditional model can generate captions, dense captions, and even questions. We use the labeled image data to train a teacher model and use the trained model to generate pseudo captions on unlabeled image data. We then combine the labeled data and pseudo labeled data to train a student model. The process is iterated by putting the student model as a new teacher. By using the proposed self-training approach and only 300k unlabeled extra data, we are able to get competitive or even better performances compared to the models of similar model size trained with 3 million extra image data.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.00558", "id": "2201.00558", "pdf": "https://arxiv.org/pdf/2201.00558", "other": "https://arxiv.org/format/2201.00558"}, "title": "Which Student is Best? A Comprehensive Knowledge Distillation Exam for Task-Specific BERT Models", "author_info": ["Made Nindyatama Nityasya", "Haryo Akbarianto Wibowo", "Rendi Chevi", "Radityo Eko Prasojo", "Alham Fikri Aji"], "summary": "We perform knowledge distillation (KD) benchmark from task-specific BERT-base teacher models to various student models: BiLSTM, CNN, BERT-Tiny, BERT-Mini, and BERT-Small. Our experiment involves 12 datasets grouped in two tasks: text classification and sequence labeling in the Indonesian language. We also compare various aspects of distillations including the usage of word embeddings and unlabeled data augmentation. Our experiments show that, despite the rising popularity of Transformer-based models, using BiLSTM and CNN student models provide the best trade-off between performance and computational resource (CPU, RAM, and storage) compared to pruned BERT models. We further propose some quick wins on performing KD to produce small NLP models via efficient KD training mechanisms involving simple choices of loss functions, word embeddings, and unlabeled data preparation.", "comment": " MSC Class:           68T50                              ACM Class:           I.2.7; I.2.6                "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.14938", "id": "2112.14938", "pdf": "https://arxiv.org/pdf/2112.14938", "other": "https://arxiv.org/format/2112.14938"}, "title": "Automatic Mixed-Precision Quantization Search of BERT", "author_info": ["Changsheng Zhao", "Ting Hua", "Yilin Shen", "Qian Lou", "Hongxia Jin"], "summary": "Pre-trained language models such as BERT have shown remarkable effectiveness in various natural language processing tasks. However, these models usually contain millions of parameters, which prevents them from practical deployment on resource-constrained devices. Knowledge distillation, Weight pruning, and Quantization are known to be the main directions in model compression. However, compact models obtained through knowledge distillation may suffer from significant accuracy drop even for a relatively small compression ratio. On the other hand, there are only a few quantization attempts that are specifically designed for natural language processing tasks. They suffer from a small compression ratio or a large error rate since manual setting on hyper-parameters is required and fine-grained subgroup-wise quantization is not supported. In this paper, we proposed an automatic mixed-precision quantization framework designed for BERT that can simultaneously conduct quantization and pruning in a subgroup-wise level. Specifically, our proposed method leverages Differentiable Neural Architecture Search to assign scale and precision for parameters in each sub-group automatically, and at the same time pruning out redundant groups of parameters. Extensive evaluations on BERT downstream tasks reveal that our proposed method outperforms baselines by providing the same performance with much smaller model size. We also show the feasibility of obtaining the extremely light-weight model by combining our solution with orthogonal methods such as DistilBERT.", "comment": " Journal ref:         Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI, 2021       "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.14169", "id": "2112.14169", "pdf": "https://arxiv.org/pdf/2112.14169", "other": "https://arxiv.org/format/2112.14169"}, "title": "Fast Changeset-based Bug Localization with BERT", "author_info": ["Agnieszka Ciborowska", "Kostadin Damevski"], "summary": "Automatically localizing software bugs to the changesets that induced them has the potential to improve software developer efficiency and to positively affect software quality. To facilitate this automation, a bug report has to be effectively matched with source code changes, even when a significant lexical gap exists between natural language used to describe the bug and identifier naming practices used by developers. To bridge this gap, we need techniques that are able to capture software engineering-specific and project-specific semantics in order to detect relatedness between the two types of documents that goes beyond exact term matching. Popular transformer-based deep learning architectures, such as BERT, excel at leveraging contextual information, hence appear to be a suitable candidate for the task. However, BERT-like models are computationally expensive, which precludes them from being used in an environment where response time is important. In this paper, we describe how BERT can be made fast enough to be applicable to changeset-based bug localization. We also explore several design decisions in using BERT for this purpose, including how best to encode changesets and how to match bug reports to individual changes for improved accuracy. We compare the accuracy and performance of our model to a non-contextual baseline (i.e., vector space model) and BERT-based architectures previously used in software engineering. Our evaluation results demonstrate advantages in using the proposed BERT model compared to the baselines, especially for bug reports that lack any hints about related code elements.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.12650", "id": "2112.12650", "pdf": "https://arxiv.org/pdf/2112.12650", "other": "https://arxiv.org/format/2112.12650"}, "title": "Distilling the Knowledge of Romanian BERTs Using Multiple Teachers", "author_info": ["Andrei-Marius Avram", "Darius Catrina", "Dumitru-Clementin Cercel", "Mihai Dasc\u0103lu", "Traian Rebedea", "Vasile P\u0103i\u015f", "Dan Tufi\u015f"], "summary": "Running large-scale pre-trained language models in computationally constrained environments remains a challenging problem yet to be addressed, while transfer learning from these models has become prevalent in Natural Language Processing tasks. Several solutions, including knowledge distillation, network quantization, or network pruning have been previously proposed; however, these approaches focus mostly on the English language, thus widening the gap when considering low-resource languages. In this work, we introduce three light and fast versions of distilled BERT models for the Romanian language: Distil-BERT-base-ro, Distil-RoBERT-base, and DistilMulti-BERT-base-ro. The first two models resulted from the individual distillation of knowledge from two base versions of Romanian BERTs available in literature, while the last one was obtained by distilling their ensemble. To our knowledge, this is the first attempt to create publicly available Romanian distilled BERT models, which were thoroughly evaluated on five tasks: part-of-speech tagging, named entity recognition, sentiment analysis, semantic textual similarity, and dialect identification. Our experimental results argue that the three distilled models maintain most performance in terms of accuracy with their teachers, while being twice as fast on a GPU and ~35% smaller. In addition, we further test the similarity between the predictions of our students versus their teachers by measuring their label and probability loyalty, together with regression loyalty - a new metric introduced in this work.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.10925", "id": "2112.10925", "pdf": "https://arxiv.org/pdf/2112.10925", "other": "https://arxiv.org/format/2112.10925"}, "title": "DB-BERT: a Database Tuning Tool that \"Reads the Manual\"", "author_info": ["Immanuel Trummer"], "summary": "DB-BERT is a database tuning tool that exploits information gained via natural language analysis of manuals and other relevant text documents. It uses text to identify database system parameters to tune as well as recommended parameter values. DB-BERT applies large, pre-trained language models (specifically, the BERT model) for text analysis. During an initial training phase, it fine-tunes model weights in order to translate natural language hints into recommended settings. At run time, DB-BERT learns to aggregate, adapt, and prioritize hints to achieve optimal performance for a specific database system and benchmark. Both phases are iterative and use reinforcement learning to guide the selection of tuning settings to evaluate (penalizing settings that the database system rejects while rewarding settings that improve performance). In our experiments, we leverage hundreds of text documents about database tuning as input for DB-BERT. We compare DB-BERT against various baselines, considering different benchmarks (TPC-C and TPC-H), metrics (throughput and run time), as well as database systems (Postgres and MySQL). In all cases, DB-BERT finds the best parameter settings among all compared methods. The code of DB-BERT is available online at https://itrummer.github.io/dbbert/.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.10553", "id": "2112.10553", "pdf": "https://arxiv.org/pdf/2112.10553", "other": "https://arxiv.org/format/2112.10553"}, "title": "Training dataset and dictionary sizes matter in BERT models: the case of Baltic languages", "author_info": ["Matej Ul\u010dar", "Marko Robnik-\u0160ikonja"], "summary": "Large pretrained masked language models have become state-of-the-art solutions for many NLP problems. While studies have shown that monolingual models produce better results than multilingual models, the training datasets must be sufficiently large. We trained a trilingual LitLat BERT-like model for Lithuanian, Latvian, and English, and a monolingual Est-RoBERTa model for Estonian. We evaluate their performance on four downstream tasks: named entity recognition, dependency parsing, part-of-speech tagging, and word analogy. To analyze the importance of focusing on a single language and the importance of a large training set, we compare created models with existing monolingual and multilingual BERT models for Estonian, Latvian, and Lithuanian. The results show that the newly created LitLat BERT and Est-RoBERTa models improve the results of existing models on all tested tasks in most situations.", "comment": " Comments: 12 pages. To be published in proceedings of the AIST 2021 conference "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.09939", "id": "2112.09939", "pdf": "https://arxiv.org/pdf/2112.09939", "other": "https://arxiv.org/format/2112.09939"}, "title": "Syntactic-GCN Bert based Chinese Event Extraction", "author_info": ["Jiangwei Liu", "Jingshu Zhang", "Xiaohong Huang", "Liangyu Min"], "summary": "With the rapid development of information technology, online platforms (e.g., news portals and social media) generate enormous web information every moment. Therefore, it is crucial to extract structured representations of events from social streams. Generally, existing event extraction research utilizes pattern matching, machine learning, or deep learning methods to perform event extraction tasks. However, the performance of Chinese event extraction is not as good as English due to the unique characteristics of the Chinese language. In this paper, we propose an integrated framework to perform Chinese event extraction. The proposed approach is a multiple channel input neural framework that integrates semantic features and syntactic features. The semantic features are captured by BERT architecture. The Part of Speech (POS) features and Dependency Parsing (DP) features are captured by profiling embeddings and Graph Convolutional Network (GCN), respectively. We also evaluate our model on a real-world dataset. Experimental results show that the proposed method outperforms the benchmark approaches significantly.", "comment": " Comments: 9 pages, 4 figures, 3 tables. arXiv admin note: text overlap with arXiv:2111.03212 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07515", "id": "2112.07515", "pdf": "https://arxiv.org/pdf/2112.07515", "other": "https://arxiv.org/format/2112.07515"}, "title": "CoCo-BERT: Improving Video-Language Pre-training with Contrastive Cross-modal Matching and Denoising", "author_info": ["Jianjie Luo", "Yehao Li", "Yingwei Pan", "Ting Yao", "Hongyang Chao", "Tao Mei"], "summary": "BERT-type structure has led to the revolution of vision-language pre-training and the achievement of state-of-the-art results on numerous vision-language downstream tasks. Existing solutions dominantly capitalize on the multi-modal inputs with mask tokens to trigger mask-based proxy pre-training tasks (e.g., masked language modeling and masked object/frame prediction). In this work, we argue that such masked inputs would inevitably introduce noise for cross-modal matching proxy task, and thus leave the inherent vision-language association under-explored. As an alternative, we derive a particular form of cross-modal proxy objective for video-language pre-training, i.e., Contrastive Cross-modal matching and denoising (CoCo). By viewing the masked frame/word sequences as the noisy augmentation of primary unmasked ones, CoCo strengthens video-language association by simultaneously pursuing inter-modal matching and intra-modal denoising between masked and unmasked inputs in a contrastive manner. Our CoCo proxy objective can be further integrated into any BERT-type encoder-decoder structure for video-language pre-training, named as Contrastive Cross-modal BERT (CoCo-BERT). We pre-train CoCo-BERT on TV dataset and a newly collected large-scale GIF video dataset (ACTION). Through extensive experiments over a wide range of downstream tasks (e.g., cross-modal retrieval, video question answering, and video captioning), we demonstrate the superiority of CoCo-BERT as a pre-trained structure.", "comment": " Comments: ACM Multimedia 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07209", "id": "2112.07209", "pdf": "https://arxiv.org/pdf/2112.07209", "other": "https://arxiv.org/format/2112.07209"}, "title": "ACE-BERT: Adversarial Cross-modal Enhanced BERT for E-commerce Retrieval", "author_info": ["Boxuan Zhang", "Chao Wei", "Yan Jin", "Weiru Zhang"], "summary": "Nowadays on E-commerce platforms, products are presented to the customers with multiple modalities. These multiple modalities are significant for a retrieval system while providing attracted products for customers. Therefore, how to take into account those multiple modalities simultaneously to boost the retrieval performance is crucial. This problem is a huge challenge to us due to the following reasons: (1) the way of extracting patch features with the pre-trained image model (e.g., CNN-based model) has much inductive bias. It is difficult to capture the efficient information from the product image in E-commerce. (2) The heterogeneity of multimodal data makes it challenging to construct the representations of query text and product including title and image in a common subspace. We propose a novel Adversarial Cross-modal Enhanced BERT (ACE-BERT) for efficient E-commerce retrieval. In detail, ACE-BERT leverages the patch features and pixel features as image representation. Thus the Transformer architecture can be applied directly to the raw image sequences. With the pre-trained enhanced BERT as the backbone network, ACE-BERT further adopts adversarial learning by adding a domain classifier to ensure the distribution consistency of different modality representations for the purpose of narrowing down the representation gap between query and product. Experimental results demonstrate that ACE-BERT outperforms the state-of-the-art approaches on the retrieval task. It is remarkable that ACE-BERT has already been deployed in our E-commerce's search engine, leading to 1.46% increase in revenue.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.06736", "id": "2112.06736", "pdf": "https://arxiv.org/pdf/2112.06736", "other": "https://arxiv.org/format/2112.06736"}, "title": "Roof-BERT: Divide Understanding Labour and Join in Work", "author_info": ["Wei-Lin Liao", "Wei-Yun Ma"], "summary": "Recent work on enhancing BERT-based language representation models with knowledge graphs (KGs) has promising results on multiple NLP tasks. State-of-the-art approaches typically integrate the original input sentences with triples in KGs, and feed the combined representation into a BERT model. However, as the sequence length of a BERT model is limited, the framework can not contain too much knowledge besides the original input sentences and is thus forced to discard some knowledge. The problem is especially severe for those downstream tasks that input is a long paragraph or even a document, such as QA or reading comprehension tasks. To address the problem, we propose Roof-BERT, a model with two underlying BERTs and a fusion layer on them. One of the underlying BERTs encodes the knowledge resources and the other one encodes the original input sentences, and the fusion layer like a roof integrates both BERTs' encodings. Experiment results on QA task reveal the effectiveness of the proposed model.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.05125", "id": "2112.05125", "pdf": "https://arxiv.org/pdf/2112.05125", "other": "https://arxiv.org/format/2112.05125"}, "title": "Transferring BERT-like Transformers' Knowledge for Authorship Verification", "author_info": ["Andrei Manolache", "Florin Brad", "Elena Burceanu", "Antonio Barbalau", "Radu Ionescu", "Marius Popescu"], "summary": "The task of identifying the author of a text spans several decades and was tackled using linguistics, statistics, and, more recently, machine learning. Inspired by the impressive performance gains across a broad range of natural language processing tasks and by the recent availability of the PAN large-scale authorship dataset, we first study the effectiveness of several BERT-like transformers for the task of authorship verification. Such models prove to achieve very high scores consistently. Next, we empirically show that they focus on topical clues rather than on author writing style characteristics, taking advantage of existing biases in the dataset. To address this problem, we provide new splits for PAN-2020, where training and test data are sampled from disjoint topics or authors. Finally, we introduce DarkReddit, a dataset with a different input data distribution. We further use it to analyze the domain generalization performance of models in a low-data regime and how performance varies when using the proposed PAN-2020 splits for fine-tuning. We show that those splits can enhance the models' capability to transfer knowledge over a new, significantly different dataset.", "comment": " Comments: 16 pages, 3 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.04329", "id": "2112.04329", "pdf": "https://arxiv.org/pdf/2112.04329", "other": "https://arxiv.org/format/2112.04329"}, "title": "JABER and SABER: Junior and Senior Arabic BERt", "author_info": ["Abbas Ghaddar", "Yimeng Wu", "Ahmad Rashid", "Khalil Bibi", "Mehdi Rezagholizadeh", "Chao Xing", "Yasheng Wang", "Duan Xinyu", "Zhefeng Wang", "Baoxing Huai", "Xin Jiang", "Qun Liu", "Philippe Langlais"], "summary": "Language-specific pre-trained models have proven to be more accurate than multilingual ones in a monolingual evaluation setting, Arabic is no exception. However, we found that previously released Arabic BERT models were significantly under-trained. In this technical report, we present JABER and SABER, Junior and Senior Arabic BERt respectively, our pre-trained language model prototypes dedicated for Arabic. We conduct an empirical study to systematically evaluate the performance of models across a diverse set of existing Arabic NLU tasks. Experimental results show that JABER and SABER achieve state-of-the-art performances on ALUE, a new benchmark for Arabic Language Understanding Evaluation, as well as on a well-established NER benchmark.", "comment": " Comments: Technical Report; v2: add SABER and CAMeLBERT evaluation; v3: fix minor typos and grammatical errors "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.03271", "id": "2112.03271", "pdf": "https://arxiv.org/pdf/2112.03271", "other": "https://arxiv.org/format/2112.03271"}, "title": "Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks", "author_info": ["Zixuan Ke", "Hu Xu", "Bing Liu"], "summary": "This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks. Although some CL techniques have been proposed for document sentiment classification, we are not aware of any CL work on ASC. A CL system that incrementally learns a sequence of ASC tasks should address the following two issues: (1) transfer knowledge learned from previous tasks to the new task to help it learn a better model, and (2) maintain the performance of the models for previous tasks so that they are not forgotten. This paper proposes a novel capsule network based model called B-CL to address these issues. B-CL markedly improves the ASC performance on both the new task and the old tasks via forward and backward knowledge transfer. The effectiveness of B-CL is demonstrated through extensive experiments.", "comment": " Journal ref:         NAACL 2021       "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.03004", "id": "2112.03004", "pdf": "https://arxiv.org/pdf/2112.03004", "other": "https://arxiv.org/format/2112.03004"}, "title": "CU-UD: text-mining drug and chemical-protein interactions with ensembles of BERT-based models", "author_info": ["Mehmet Efruz Karabulut", "K. Vijay-Shanker", "Yifan Peng"], "summary": "Identifying the relations between chemicals and proteins is an important text mining task. BioCreative VII track 1 DrugProt task aims to promote the development and evaluation of systems that can automatically detect relations between chemical compounds/drugs and genes/proteins in PubMed abstracts. In this paper, we describe our submission, which is an ensemble system, including multiple BERT-based language models. We combine the outputs of individual models using majority voting and multilayer perceptron. Our system obtained 0.7708 in precision and 0.7770 in recall, for an F1 score of 0.7739, demonstrating the effectiveness of using ensembles of BERT-based language models for automatically detecting relations between chemicals and proteins. Our code is available at https://github.com/bionlplab/drugprot_bcvii.", "comment": " Comments: Proceedings of the BioCreative VII Challenge Evaluation Workshop "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.02955", "id": "2112.02955", "pdf": "https://arxiv.org/pdf/2112.02955"}, "title": "Does constituency analysis enhance domain-specific pre-trained BERT models for relation extraction?", "author_info": ["Anfu Tang", "Louise Del\u00e9ger", "Robert Bossy", "Pierre Zweigenbaum", "Claire N\u00e9dellec"], "summary": "Recently many studies have been conducted on the topic of relation extraction. The DrugProt track at BioCreative VII provides a manually-annotated corpus for the purpose of the development and evaluation of relation extraction systems, in which interactions between chemicals and genes are studied. We describe the ensemble system that we used for our submission, which combines predictions of fine-tuned bioBERT, sciBERT and const-bioBERT models by majority voting. We specifically tested the contribution of syntactic information to relation extraction with BERT. We observed that adding constituentbased syntactic information to BERT improved precision, but decreased recall, since relations rarely seen in the train set were less likely to be predicted by BERT models in which the syntactic information is infused. Our code is available online [https://github.com/Maple177/drugprot-relation-extraction].", "comment": " Journal ref:         BioCreative VII Challenge Evaluation Workshop, Nov 2021, on-line, Spain       "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.02682", "id": "2112.02682", "pdf": "https://arxiv.org/pdf/2112.02682", "other": "https://arxiv.org/format/2112.02682"}, "title": "BERTMap: A BERT-based Ontology Alignment System", "author_info": ["Yuan He", "Jiaoyan Chen", "Denvar Antonyrajah", "Ian Horrocks"], "summary": "Ontology alignment (a.k.a ontology matching (OM)) plays a critical role in knowledge integration. Owing to the success of machine learning in many domains, it has been applied in OM. However, the existing methods, which often adopt ad-hoc feature engineering or non-contextual word embeddings, have not yet outperformed rule-based systems especially in an unsupervised setting. In this paper, we propose a novel OM system named BERTMap which can support both unsupervised and semi-supervised settings. It first predicts mappings using a classifier based on fine-tuning the contextual embedding model BERT on text semantics corpora extracted from ontologies, and then refines the mappings through extension and repair by utilizing the ontology structure and logic. Our evaluation with three alignment tasks on biomedical ontologies demonstrates that BERTMap can often perform better than the leading OM systems LogMap and AML.", "comment": " Comments: Full version (with appendix) of the accepted paper in 36th AAAI Conference on Artificial Intelligence 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.01810", "id": "2112.01810", "pdf": "https://arxiv.org/pdf/2112.01810", "other": "https://arxiv.org/format/2112.01810"}, "title": "Siamese BERT-based Model for Web Search Relevance Ranking Evaluated on a New Czech Dataset", "author_info": ["Mat\u011bj Koci\u00e1n", "Jakub N\u00e1plava", "Daniel \u0160tancl", "Vladim\u00edr Kadlec"], "summary": "Web search engines focus on serving highly relevant results within hundreds of milliseconds. Pre-trained language transformer models such as BERT are therefore hard to use in this scenario due to their high computational demands. We present our real-time approach to the document ranking problem leveraging a BERT-based siamese architecture. The model is already deployed in a commercial search engine and it improves production performance by more than 3%. For further research and evaluation, we release DaReCzech, a unique data set of 1.6 million Czech user query-document pairs with manually assigned relevance levels. We also release Small-E-Czech, an Electra-small language model pre-trained on a large Czech corpus. We believe this data will support endeavours both of search relevance and multilingual-focused research communities.", "comment": " Comments: Accepted at the Thirty-Fourth Annual Conference on Innovative Applications of Artificial Intelligence (IAAI-22). IAAI Innovative Application Award. 9 pages, 3 figures, 8 tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.01529", "id": "2112.01529", "pdf": "https://arxiv.org/pdf/2112.01529", "other": "https://arxiv.org/format/2112.01529"}, "title": "BEVT: BERT Pretraining of Video Transformers", "author_info": ["Rui Wang", "Dongdong Chen", "Zuxuan Wu", "Yinpeng Chen", "Xiyang Dai", "Mengchen Liu", "Yu-Gang Jiang", "Luowei Zhou", "Lu Yuan"], "summary": "This paper studies the BERT pretraining of video transformers. It is a straightforward but worth-studying extension given the recent success from BERT pretraining of image transformers. We introduce BEVT which decouples video representation learning into spatial representation learning and temporal dynamics learning. In particular, BEVT first performs masked image modeling on image data, and then conducts masked image modeling jointly with masked video modeling on video data. This design is motivated by two observations: 1) transformers learned on image datasets provide decent spatial priors that can ease the learning of video transformers, which are often times computationally-intensive if trained from scratch; 2) discriminative clues, i.e., spatial and temporal information, needed to make correct predictions vary among different videos due to large intra-class and inter-class variations. We conduct extensive experiments on three challenging video benchmarks where BEVT achieves very promising results. On Kinetics 400, for which recognition mostly relies on discriminative spatial representations, BEVT achieves comparable results to strong supervised baselines. On Something-Something-V2 and Diving 48, which contain videos relying on temporal dynamics, BEVT outperforms by clear margins all alternative baselines and achieves state-of-the-art performance with a 71.4% and 87.2% Top-1 accuracy respectively.", "comment": " Comments: Updated SOTA results with ImageNet-1k pretrained PeCo tokenizer "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.00567", "id": "2112.00567", "pdf": "https://arxiv.org/pdf/2112.00567", "other": "https://arxiv.org/format/2112.00567"}, "title": "DPRK-BERT: The Supreme Language Model", "author_info": ["Arda Akdemir", "Yeojoo Jeon"], "summary": "Deep language models have achieved remarkable success in the NLP domain. The standard way to train a deep language model is to employ unsupervised learning from scratch on a large unlabeled corpus. However, such large corpora are only available for widely-adopted and high-resource languages and domains. This study presents the first deep language model, DPRK-BERT, for the DPRK language. We achieve this by compiling the first unlabeled corpus for the DPRK language and fine-tuning a preexisting the ROK language model. We compare the proposed model with existing approaches and show significant improvements on two DPRK datasets. We also present a cross-lingual version of this model which yields better generalization across the two Korean languages. Finally, we provide various NLP tools related to the DPRK language that would foster future research.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.00405", "id": "2112.00405", "pdf": "https://arxiv.org/pdf/2112.00405", "other": "https://arxiv.org/format/2112.00405"}, "title": "NER-BERT: A Pre-trained Model for Low-Resource Entity Tagging", "author_info": ["Zihan Liu", "Feijun Jiang", "Yuxiang Hu", "Chen Shi", "Pascale Fung"], "summary": "Named entity recognition (NER) models generally perform poorly when large training datasets are unavailable for low-resource domains. Recently, pre-training a large-scale language model has become a promising direction for coping with the data scarcity issue. However, the underlying discrepancies between the language modeling and NER task could limit the models' performance, and pre-training for the NER task has rarely been studied since the collected NER datasets are generally small or large but with low quality. In this paper, we construct a massive NER corpus with a relatively high quality, and we pre-train a NER-BERT model based on the created dataset. Experimental results show that our pre-trained model can significantly outperform BERT as well as other strong baselines in low-resource scenarios across nine diverse domains. Moreover, a visualization of entity representations further indicates the effectiveness of NER-BERT for categorizing a variety of entities.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.15622", "id": "2111.15622", "pdf": "https://arxiv.org/pdf/2111.15622", "other": "https://arxiv.org/format/2111.15622"}, "title": "Chemical Identification and Indexing in PubMed Articles via BERT and Text-to-Text Approaches", "author_info": ["Virginia Adams", "Hoo-Chang Shin", "Carol Anderson", "Bo Liu", "Anas Abidin"], "summary": "The Biocreative VII Track-2 challenge consists of named entity recognition, entity-linking (or entity-normalization), and topic indexing tasks -- with entities and topics limited to chemicals for this challenge. Named entity recognition is a well-established problem and we achieve our best performance with BERT-based BioMegatron models. We extend our BERT-based approach to the entity linking task. After the second stage of pretraining BioBERT with a metric-learning loss strategy called self-alignment pretraining (SAP), we link entities based on the cosine similarity between their SAP-BioBERT word embeddings. Despite the success of our named entity recognition experiments, we find the chemical indexing task generally more challenging.", "comment": " Comments: Submission to the BioCreative VII challenge - Track-2 "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.15617", "id": "2111.15617", "pdf": "https://arxiv.org/pdf/2111.15617", "other": "https://arxiv.org/format/2111.15617"}, "title": "Text Mining Drug/Chemical-Protein Interactions using an Ensemble of BERT and T5 Based Models", "author_info": ["Virginia Adams", "Hoo-Chang Shin", "Carol Anderson", "Bo Liu", "Anas Abidin"], "summary": "In Track-1 of the BioCreative VII Challenge participants are asked to identify interactions between drugs/chemicals and proteins. In-context named entity annotations for each drug/chemical and protein are provided and one of fourteen different interactions must be automatically predicted. For this relation extraction task, we attempt both a BERT-based sentence classification approach, and a more novel text-to-text approach using a T5 model. We find that larger BERT-based models perform better in general, with our BioMegatron-based model achieving the highest scores across all metrics, achieving 0.74 F1 score. Though our novel T5 text-to-text method did not perform as well as most of our BERT-based models, it outperformed those trained on similar data, showing promising results, achieving 0.65 F1 score. We believe a text-to-text approach to relation extraction has some competitive advantages and there is a lot of room for research advancement.", "comment": " Comments: Submission to the BioCreative VII challenge, Track-1 "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.15379", "id": "2111.15379"}, "title": "Text classification problems via BERT embedding method and graph convolutional neural network", "author_info": ["Loc Hoang Tran", "Tuan Tran", "An Mai"], "summary": "This paper presents the novel way combining the BERT embedding method and the graph convolutional neural network. This combination is employed to solve the text classification problem. Initially, we apply the BERT embedding method to the texts (in the BBC news dataset and the IMDB movie reviews dataset) in order to transform all the texts to numerical vector. Then, the graph convolutional neural network will be applied to these numerical vectors to classify these texts into their ap-propriate classes/labels. Experiments show that the performance of the graph convolutional neural network model is better than the perfor-mances of the combination of the BERT embedding method with clas-sical machine learning models.", "comment": " Comments: There are some errors in the experiments. I will re-submit it later "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.14830", "id": "2111.14830", "pdf": "https://arxiv.org/pdf/2111.14830", "other": "https://arxiv.org/format/2111.14830"}, "title": "Abusive and Threatening Language Detection in Urdu using Boosting based and BERT based models: A Comparative Approach", "author_info": ["Mithun Das", "Somnath Banerjee", "Punyajoy Saha"], "summary": "Online hatred is a growing concern on many social media platforms. To address this issue, different social media platforms have introduced moderation policies for such content. They also employ moderators who can check the posts violating moderation policies and take appropriate action. Academicians in the abusive language research domain also perform various studies to detect such content better. Although there is extensive research in abusive language detection in English, there is a lacuna in abusive language detection in low resource languages like Hindi, Urdu etc. In this FIRE 2021 shared task - \"HASOC- Abusive and Threatening language detection in Urdu\" the organizers propose an abusive language detection dataset in Urdu along with threatening language detection. In this paper, we explored several machine learning models such as XGboost, LGBM, m-BERT based models for abusive and threatening content detection in Urdu based on the shared task. We observed the Transformer model specifically trained on abusive language dataset in Arabic helps in getting the best performance. Our model came First for both abusive and threatening content detection with an F1scoreof 0.88 and 0.54, respectively.", "comment": " Comments: Accepted in FIRE'21 (Track Abusive and Threatening Language Detection Task in Urdu). arXiv admin note: text overlap with arXiv:2111.13974 "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.14819", "id": "2111.14819", "pdf": "https://arxiv.org/pdf/2111.14819", "other": "https://arxiv.org/format/2111.14819"}, "title": "Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling", "author_info": ["Xumin Yu", "Lulu Tang", "Yongming Rao", "Tiejun Huang", "Jie Zhou", "Jiwen Lu"], "summary": "We present Point-BERT, a new paradigm for learning Transformers to generalize the concept of BERT to 3D point cloud. Inspired by BERT, we devise a Masked Point Modeling (MPM) task to pre-train point cloud Transformers. Specifically, we first divide a point cloud into several local point patches, and a point cloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to generate discrete point tokens containing meaningful local information. Then, we randomly mask out some patches of input point clouds and feed them into the backbone Transformers. The pre-training objective is to recover the original point tokens at the masked locations under the supervision of point tokens obtained by the Tokenizer. Extensive experiments demonstrate that the proposed BERT-style pre-training strategy significantly improves the performance of standard point cloud Transformers. Equipped with our pre-training strategy, we show that a pure Transformer architecture attains 93.8% accuracy on ModelNet40 and 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully designed point cloud models with much fewer hand-made designs. We also demonstrate that the representations learned by Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task. The code and pre-trained models are available at https://github.com/lulutang0608/Point-BERT", "comment": " Comments: Project page: https://point-bert.ivg-research.xyz/ "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.13972", "id": "2111.13972", "pdf": "https://arxiv.org/pdf/2111.13972", "other": "https://arxiv.org/format/2111.13972"}, "title": "Tapping BERT for Preposition Sense Disambiguation", "author_info": ["Siddhesh Pawar", "Shyam Thombre", "Anirudh Mittal", "Girishkumar Ponkiya", "Pushpak Bhattacharyya"], "summary": "Prepositions are frequently occurring polysemous words. Disambiguation of prepositions is crucial in tasks like semantic role labelling, question answering, text entailment, and noun compound paraphrasing. In this paper, we propose a novel methodology for preposition sense disambiguation (PSD), which does not use any linguistic tools. In a supervised setting, the machine learning model is presented with sentences wherein prepositions have been annotated with senses. These senses are IDs in what is called The Preposition Project (TPP). We use the hidden layer representations from pre-trained BERT and BERT variants. The latent representations are then classified into the correct sense ID using a Multi Layer Perceptron. The dataset used for this task is from SemEval-2007 Task-6. Our methodology gives an accuracy of 86.85% which is better than the state-of-the-art.", "comment": " ACM Class:           I.2.7                "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.12939", "id": "2111.12939", "pdf": "https://arxiv.org/pdf/2111.12939"}, "title": "Probabilistic Impact Score Generation using Ktrain-BERT to Identify Hate Words from Twitter Discussions", "author_info": ["Sourav Das", "Prasanta Mandal", "Sanjay Chatterji"], "summary": "Social media has seen a worrying rise in hate speech in recent times. Branching to several distinct categories of cyberbullying, gender discrimination, or racism, the combined label for such derogatory content can be classified as toxic content in general. This paper presents experimentation with a Keras wrapped lightweight BERT model to successfully identify hate speech and predict probabilistic impact score for the same to extract the hateful words within sentences. The dataset used for this task is the Hate Speech and Offensive Content Detection (HASOC 2021) data from FIRE 2021 in English. Our system obtained a validation accuracy of 82.60%, with a maximum F1-Score of 82.68%. Subsequently, our predictive cases performed significantly well in generating impact scores for successful identification of the hate tweets as well as the hateful words from tweet pools.", "comment": " MSC Class:           15-04                              ACM Class:           I.2.7; I.2.6                "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.12710", "id": "2111.12710", "pdf": "https://arxiv.org/pdf/2111.12710", "other": "https://arxiv.org/format/2111.12710"}, "title": "PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers", "author_info": ["Xiaoyi Dong", "Jianmin Bao", "Ting Zhang", "Dongdong Chen", "Weiming Zhang", "Lu Yuan", "Dong Chen", "Fang Wen", "Nenghai Yu"], "summary": "This paper explores a better codebook for BERT pre-training of vision transformers. The recent work BEiT successfully transfers BERT pre-training from NLP to the vision field. It directly adopts one simple discrete VAE as the visual tokenizer, but has not considered the semantic level of the resulting visual tokens. By contrast, the discrete tokens in NLP field are naturally highly semantic. This difference motivates us to learn a perceptual codebook. And we surprisingly find one simple yet effective idea: enforcing perceptual similarity during the dVAE training. We demonstrate that the visual tokens generated by the proposed perceptual codebook do exhibit better semantic meanings, and subsequently help pre-training achieve superior transfer performance in various downstream tasks. For example, we achieve 84.5% Top-1 accuracy on ImageNet-1K with ViT-B backbone, outperforming the competitive method BEiT by +1.3 with the same pre-training epochs. It can also improve the performance of object detection and segmentation tasks on COCO val by +1.3 box AP and +1.0 mask AP, semantic segmentation on ADE20k by +1.0 mIoU. Equipped with a larger backbone ViT-H, we achieve the state-of-the-art performance (88.3% Top-1 accuracy) among the methods using only ImageNet-1K data. The code and models will be available at https://github.com/microsoft/PeCo.", "comment": " Comments: Updated SOTA 88.3% Top-1 Acc on ViT-H model, and ablation about tokenzier size "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.10951", "id": "2111.10951", "pdf": "https://arxiv.org/pdf/2111.10951", "other": "https://arxiv.org/format/2111.10951"}, "title": "Can depth-adaptive BERT perform better on binary classification tasks", "author_info": ["Jing Fan", "Xin Zhang", "Sheng Zhang", "Yan Pan", "Lixiang Guo"], "summary": "In light of the success of transferring language models into NLP tasks, we ask whether the full BERT model is always the best and does it exist a simple but effective method to find the winning ticket in state-of-the-art deep neural networks without complex calculations. We construct a series of BERT-based models with different size and compare their predictions on 8 binary classification tasks. The results show there truly exist smaller sub-networks performing better than the full model. Then we present a further study and propose a simple method to shrink BERT appropriately before fine-tuning. Some extended experiments indicate that our method could save time and storage overhead extraordinarily with little even no accuracy loss.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.10100", "id": "2111.10100", "pdf": "https://arxiv.org/pdf/2111.10100"}, "title": "Does BERT look at sentiment lexicon?", "author_info": ["Elena Razova", "Sergey Vychegzhanin", "Evgeny Kotelnikov"], "summary": "The main approaches to sentiment analysis are rule-based methods and ma-chine learning, in particular, deep neural network models with the Trans-former architecture, including BERT. The performance of neural network models in the tasks of sentiment analysis is superior to the performance of rule-based methods. The reasons for this situation remain unclear due to the poor interpretability of deep neural network models. One of the main keys to understanding the fundamental differences between the two approaches is the analysis of how sentiment lexicon is taken into account in neural network models. To this end, we study the attention weights matrices of the Russian-language RuBERT model. We fine-tune RuBERT on sentiment text corpora and compare the distributions of attention weights for sentiment and neutral lexicons. It turns out that, on average, 3/4 of the heads of various model var-iants statistically pay more attention to the sentiment lexicon compared to the neutral one.", "comment": " Comments: 14 pages, 3 tables, 3 figures. Accepted to AIST-2021 conference "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.10097", "id": "2111.10097", "pdf": "https://arxiv.org/pdf/2111.10097", "other": "https://arxiv.org/format/2111.10097"}, "title": "Lexicon-based Methods vs. BERT for Text Sentiment Analysis", "author_info": ["Anastasia Kotelnikova", "Danil Paschenko", "Klavdiya Bochenina", "Evgeny Kotelnikov"], "summary": "The performance of sentiment analysis methods has greatly increased in recent years. This is due to the use of various models based on the Transformer architecture, in particular BERT. However, deep neural network models are difficult to train and poorly interpretable. An alternative approach is rule-based methods using sentiment lexicons. They are fast, require no training, and are well interpreted. But recently, due to the widespread use of deep learning, lexicon-based methods have receded into the background. The purpose of the article is to study the performance of the SO-CAL and SentiStrength lexicon-based methods, adapted for the Russian language. We have tested these methods, as well as the RuBERT neural network model, on 16 text corpora and have analyzed their results. RuBERT outperforms both lexicon-based methods on average, but SO-CAL surpasses RuBERT for four corpora out of 16.", "comment": " Comments: 14 pages, 4 tables, 3 figures. Accepted to AIST-2021 conference "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.09564", "id": "2111.09564", "pdf": "https://arxiv.org/pdf/2111.09564", "other": "https://arxiv.org/format/2111.09564"}, "title": "LAnoBERT : System Log Anomaly Detection based on BERT Masked Language Model", "author_info": ["Yukyung Lee", "Jina Kim", "Pilsung Kang"], "summary": "The system log generated in a computer system refers to large-scale data that are collected simultaneously and used as the basic data for determining simple errors and detecting external adversarial intrusion or the abnormal behaviors of insiders. The aim of system log anomaly detection is to promptly identify anomalies while minimizing human intervention, which is a critical problem in the industry. Previous studies performed anomaly detection through algorithms after converting various forms of log data into a standardized template using a parser. These methods involved generating a template for refining the log key. Particularly, a template corresponding to a specific event should be defined in advance for all the log data using which the information within the log key may get lost.In this study, we propose LAnoBERT, a parser free system log anomaly detection method that uses the BERT model, exhibiting excellent natural language processing performance. The proposed method, LAnoBERT, learns the model through masked language modeling, which is a BERT-based pre-training method, and proceeds with unsupervised learning-based anomaly detection using the masked language modeling loss function per log key word during the inference process. LAnoBERT achieved better performance compared to previous methodology in an experiment conducted using benchmark log datasets, HDFS, and BGL, and also compared to certain supervised learning-based models.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.08585", "id": "2111.08585", "pdf": "https://arxiv.org/pdf/2111.08585", "other": "https://arxiv.org/format/2111.08585"}, "title": "CEHR-BERT: Incorporating temporal information from structured EHR data to improve prediction tasks", "author_info": ["Chao Pang", "Xinzhuo Jiang", "Krishna S Kalluri", "Matthew Spotnitz", "RuiJun Chen", "Adler Perotte", "Karthik Natarajan"], "summary": "Embedding algorithms are increasingly used to represent clinical concepts in healthcare for improving machine learning tasks such as clinical phenotyping and disease prediction. Recent studies have adapted state-of-the-art bidirectional encoder representations from transformers (BERT) architecture to structured electronic health records (EHR) data for the generation of contextualized concept embeddings, yet do not fully incorporate temporal data across multiple clinical domains. Therefore we developed a new BERT adaptation, CEHR-BERT, to incorporate temporal information using a hybrid approach by augmenting the input to BERT using artificial time tokens, incorporating time, age, and concept embeddings, and introducing a new second learning objective for visit type. CEHR-BERT was trained on a subset of Columbia University Irving Medical Center-York Presbyterian Hospital's clinical data, which includes 2.4M patients, spanning over three decades, and tested using 4-fold cross-validation on the following prediction tasks: hospitalization, death, new heart failure (HF) diagnosis, and HF readmission. Our experiments show that CEHR-BERT outperformed existing state-of-the-art clinical BERT adaptations and baseline models across all 4 prediction tasks in both ROC-AUC and PR-AUC. CEHR-BERT also demonstrated strong transfer learning capability, as our model trained on only 5% of data outperformed comparison models trained on the entire data set. Ablation studies to better understand the contribution of each time component showed incremental gains with every element, suggesting that CEHR-BERT's incorporation of artificial time tokens, time and age embeddings with concept embeddings, and the addition of the second learning objective represents a promising approach for future BERT-based clinical embeddings.", "comment": " Journal ref:         Proceedings of Machine Learning for Health, PMLR 158:239-260, 2021       "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.08510", "id": "2111.08510", "pdf": "https://arxiv.org/pdf/2111.08510", "other": "https://arxiv.org/format/2111.08510"}, "title": "CVSS-BERT: Explainable Natural Language Processing to Determine the Severity of a Computer Security Vulnerability from its Description", "author_info": ["Mustafizur Shahid", "Herv\u00e9 Debar"], "summary": "When a new computer security vulnerability is publicly disclosed, only a textual description of it is available. Cybersecurity experts later provide an analysis of the severity of the vulnerability using the Common Vulnerability Scoring System (CVSS). Specifically, the different characteristics of the vulnerability are summarized into a vector (consisting of a set of metrics), from which a severity score is computed. However, because of the high number of vulnerabilities disclosed everyday this process requires lot of manpower, and several days may pass before a vulnerability is analyzed. We propose to leverage recent advances in the field of Natural Language Processing (NLP) to determine the CVSS vector and the associated severity score of a vulnerability from its textual description in an explainable manner. To this purpose, we trained multiple BERT classifiers, one for each metric composing the CVSS vector. Experimental results show that our trained classifiers are able to determine the value of the metrics of the CVSS vector with high accuracy. The severity score computed from the predicted CVSS vector is also very close to the real severity score attributed by a human expert. For explainability purpose, gradient-based input saliency method was used to determine the most relevant input words for a given prediction made by our classifiers. Often, the top relevant words include terms in agreement with the rationales of a human cybersecurity expert, making the explanation comprehensible for end-users.", "comment": " Comments: 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA), Dec 2021, Pasadena, United States "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.07832", "id": "2111.07832", "pdf": "https://arxiv.org/pdf/2111.07832", "other": "https://arxiv.org/format/2111.07832"}, "title": "iBOT: Image BERT Pre-Training with Online Tokenizer", "author_info": ["Jinghao Zhou", "Chen Wei", "Huiyu Wang", "Wei Shen", "Cihang Xie", "Alan Yuille", "Tao Kong"], "summary": "The success of language Transformers is primarily attributed to the pretext task of masked language modeling (MLM), where texts are first tokenized into semantically meaningful pieces. In this work, we study masked image modeling (MIM) and indicate the advantages and challenges of using a semantically meaningful visual tokenizer. We present a self-supervised framework iBOT that can perform masked prediction with an online tokenizer. Specifically, we perform self-distillation on masked patch tokens and take the teacher network as the online tokenizer, along with self-distillation on the class token to acquire visual semantics. The online tokenizer is jointly learnable with the MIM objective and dispenses with a multi-stage training pipeline where the tokenizer needs to be pre-trained beforehand. We show the prominence of iBOT by achieving an 82.3% linear probing accuracy and an 87.8% fine-tuning accuracy evaluated on ImageNet-1K. Beyond the state-of-the-art image classification results, we underline emerging local semantic patterns, which helps the models to obtain strong robustness against common corruptions and achieve leading results on dense downstream tasks, eg., object detection, instance segmentation, and semantic segmentation.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.07137", "id": "2111.07137", "pdf": "https://arxiv.org/pdf/2111.07137", "other": "https://arxiv.org/format/2111.07137"}, "title": "Interpreting BERT architecture predictions for peptide presentation by MHC class I proteins", "author_info": ["Hans-Christof Gasser", "Georges Bedran", "Bo Ren", "David Goodlett", "Javier Alfaro", "Ajitha Rajan"], "summary": "The major histocompatibility complex (MHC) class-I pathway supports the detection of cancer and viruses by the immune system. It presents parts of proteins (peptides) from inside a cell on its membrane surface enabling visiting immune cells that detect non-self peptides to terminate the cell. The ability to predict whether a peptide will get presented on MHC Class I molecules helps in designing vaccines so they can activate the immune system to destroy the invading disease protein. We designed a prediction model using a BERT-based architecture (ImmunoBERT) that takes as input a peptide and its surrounding regions (N and C-terminals) along with a set of MHC class I (MHC-I) molecules. We present a novel application of well known interpretability techniques, SHAP and LIME, to this domain and we use these results along with 3D structure visualizations and amino acid frequencies to understand and identify the most influential parts of the input amino acid sequences contributing to the output. In particular, we find that amino acids close to the peptides' N- and C-terminals are highly relevant. Additionally, some positions within the MHC proteins (in particular in the A, B and F pockets) are often assigned a high importance ranking - which confirms biological studies and the distances in the structure visualizations.", "comment": " Comments: 10 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.05808", "id": "2111.05808", "pdf": "https://arxiv.org/pdf/2111.05808"}, "title": "BagBERT: BERT-based bagging-stacking for multi-topic classification", "author_info": ["Lo\u00efc Rakotoson", "Charles Letaillieur", "Sylvain Massip", "Fr\u00e9jus Laleye"], "summary": "This paper describes our submission on the COVID-19 literature annotation task at Biocreative VII. We proposed an approach that exploits the knowledge of the globally non-optimal weights, usually rejected, to build a rich representation of each label. Our proposed approach consists of two stages: (1) A bagging of various initializations of the training data that features weakly trained weights, (2) A stacking of heterogeneous vocabulary models based on BERT and RoBERTa Embeddings. The aggregation of these weak insights performs better than a classical globally efficient model. The purpose is the distillation of the richness of knowledge to a simpler and lighter model. Our system obtains an Instance-based F1 of 92.96 and a Label-based micro-F1 of 91.35.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.04933", "id": "2111.04933", "pdf": "https://arxiv.org/pdf/2111.04933", "other": "https://arxiv.org/format/2111.04933"}, "title": "DSBERT:Unsupervised Dialogue Structure learning with BERT", "author_info": ["Bingkun Chen", "Shaobing Dai", "Shenghua Zheng", "Lei Liao", "Yang Li"], "summary": "Unsupervised dialogue structure learning is an important and meaningful task in natural language processing. The extracted dialogue structure and process can help analyze human dialogue, and play a vital role in the design and evaluation of dialogue systems. The traditional dialogue system requires experts to manually design the dialogue structure, which is very costly. But through unsupervised dialogue structure learning, dialogue structure can be automatically obtained, reducing the cost of developers constructing dialogue process. The learned dialogue structure can be used to promote the dialogue generation of the downstream task system, and improve the logic and consistency of the dialogue robot's reply.In this paper, we propose a Bert-based unsupervised dialogue structure learning algorithm DSBERT (Dialogue Structure BERT). Different from the previous SOTA models VRNN and SVRNN, we combine BERT and AutoEncoder, which can effectively combine context information. In order to better prevent the model from falling into the local optimal solution and make the dialogue state distribution more uniform and reasonable, we also propose three balanced loss functions that can be used for dialogue structure learning. Experimental results show that DSBERT can generate a dialogue structure closer to the real structure, can distinguish sentences with different semantics and map them to different hidden states.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.04551", "id": "2111.04551", "pdf": "https://arxiv.org/pdf/2111.04551", "other": "https://arxiv.org/format/2111.04551"}, "title": "Sexism Prediction in Spanish and English Tweets Using Monolingual and Multilingual BERT and Ensemble Models", "author_info": ["Angel Felipe Magnoss\u00e3o de Paula", "Roberto Fray da Silva", "Ipek Baris Schlicht"], "summary": "The popularity of social media has created problems such as hate speech and sexism. The identification and classification of sexism in social media are very relevant tasks, as they would allow building a healthier social environment. Nevertheless, these tasks are considerably challenging. This work proposes a system to use multilingual and monolingual BERT and data points translation and ensemble strategies for sexism identification and classification in English and Spanish. It was conducted in the context of the sEXism Identification in Social neTworks shared 2021 (EXIST 2021) task, proposed by the Iberian Languages Evaluation Forum (IberLEF). The proposed system and its main components are described, and an in-depth hyperparameters analysis is conducted. The main results observed were: (i) the system obtained better results than the baseline model (multilingual BERT); (ii) ensemble models obtained better results than monolingual models; and (iii) an ensemble model considering all individual models and the best standardized values obtained the best accuracies and F1-scores for both tasks. This work obtained first place in both tasks at EXIST, with the highest accuracies (0.780 for task 1 and 0.658 for task 2) and F1-scores (F1-binary of 0.780 for task 1 and F1-macro of 0.579 for task 2).", "comment": " Comments: 18 pages, presented at IberLEF: http://ceur-ws.org/Vol-2943/exist_paper2.pdf, the best scoring system at EXIST "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.04198", "id": "2111.04198", "pdf": "https://arxiv.org/pdf/2111.04198", "other": "https://arxiv.org/format/2111.04198"}, "title": "TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning", "author_info": ["Yixuan Su", "Fangyu Liu", "Zaiqiao Meng", "Tian Lan", "Lei Shu", "Ehsan Shareghi", "Nigel Collier"], "summary": "Masked language models (MLMs) such as BERT and RoBERTa have revolutionized the field of Natural Language Understanding in the past few years. However, existing pre-trained MLMs often output an anisotropic distribution of token representations that occupies a narrow subset of the entire representation space. Such token representations are not ideal, especially for tasks that demand discriminative semantic meanings of distinct tokens. In this work, we propose TaCL (Token-aware Contrastive Learning), a novel continual pre-training approach that encourages BERT to learn an isotropic and discriminative distribution of token representations. TaCL is fully unsupervised and requires no additional data. We extensively test our approach on a wide range of English and Chinese benchmarks. The results show that TaCL brings consistent and notable improvements over the original BERT model. Furthermore, we conduct detailed analysis to reveal the merits and inner-workings of our approach.", "comment": " Comments: Work in progress; (v2 added BERT-large results) "}, {"arxiv": {"page": "https://arxiv.org/abs/2111.02188", "id": "2111.02188", "pdf": "https://arxiv.org/pdf/2111.02188", "other": "https://arxiv.org/format/2111.02188"}, "title": "BERT-DRE: BERT with Deep Recursive Encoder for Natural Language Sentence Matching", "author_info": ["Ehsan Tavan", "Ali Rahmati", "Maryam Najafi", "Saeed Bibak", "Zahed Rahmati"], "summary": "This paper presents a deep neural architecture, for Natural Language Sentence Matching (NLSM) by adding a deep recursive encoder to BERT so called BERT with Deep Recursive Encoder (BERT-DRE). Our analysis of model behavior shows that BERT still does not capture the full complexity of text, so a deep recursive encoder is applied on top of BERT. Three Bi-LSTM layers with residual connection are used to design a recursive encoder and an attention module is used on top of this encoder. To obtain the final vector, a pooling layer consisting of average and maximum pooling is used. We experiment our model on four benchmarks, SNLI, FarsTail, MultiNLI, SciTail, and a novel Persian religious questions dataset. This paper focuses on improving the BERT results in the NLSM task. In this regard, comparisons between BERT-DRE and BERT are conducted, and it is shown that in all cases, BERT-DRE outperforms BERT. The BERT algorithm on the religious dataset achieved an accuracy of 89.70%, and BERT-DRE architectures improved to 90.29% using the same dataset.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.01515", "id": "2111.01515", "pdf": "https://arxiv.org/pdf/2111.01515"}, "title": "Detection of Hate Speech using BERT and Hate Speech Word Embedding with Deep Model", "author_info": ["Hind Saleh", "Areej Alhothali", "Kawthar Moria"], "summary": "The enormous amount of data being generated on the web and social media has increased the demand for detecting online hate speech. Detecting hate speech will reduce their negative impact and influence on others. A lot of effort in the Natural Language Processing (NLP) domain aimed to detect hate speech in general or detect specific hate speech such as religion, race, gender, or sexual orientation. Hate communities tend to use abbreviations, intentional spelling mistakes, and coded words in their communication to evade detection, adding more challenges to hate speech detection tasks. Thus, word representation will play an increasingly pivotal role in detecting hate speech. This paper investigates the feasibility of leveraging domain-specific word embedding in Bidirectional LSTM based deep model to automatically detect/classify hate speech. Furthermore, we investigate the use of the transfer learning language model (BERT) on hate speech problem as a binary classification task. The experiments showed that domainspecific word embedding with the Bidirectional LSTM based deep model achieved a 93% f1-score while BERT achieved up to 96% f1-score on a combined balanced dataset from available hate speech datasets.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2111.00611", "id": "2111.00611", "pdf": "https://arxiv.org/pdf/2111.00611"}, "title": "R-BERT-CNN: Drug-target interactions extraction from biomedical literature", "author_info": ["Jehad Aldahdooh", "Ziaurrehman Tanoli", "Jing Tang"], "summary": "In this research, we present our work participation for the DrugProt task of BioCreative VII challenge. Drug-target interactions (DTIs) are critical for drug discovery and repurposing, which are often manually extracted from the experimental articles. There are >32M biomedical articles on PubMed and manually extracting DTIs from such a huge knowledge base is challenging. To solve this issue, we provide a solution for Track 1, which aims to extract 10 types of interactions between drug and protein entities. We applied an Ensemble Classifier model that combines BioMed-RoBERTa, a state of art language model, with Convolutional Neural Networks (CNN) to extract these relations. Despite the class imbalances in the BioCreative VII DrugProt test corpus, our model achieves a good performance compared to the average of other submissions in the challenge, with the micro F1 score of 55.67% (and 63% on BioCreative VI ChemProt test corpus). The results show the potential of deep learning in extracting various types of DTIs.", "comment": " Journal ref:         Proceedings of the BioCreative VII Workshop. (2021) 102-106       "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.15802", "id": "2110.15802", "pdf": "https://arxiv.org/pdf/2110.15802", "other": "https://arxiv.org/format/2110.15802"}, "title": "BERMo: What can BERT learn from ELMo?", "author_info": ["Sangamesh Kodge", "Kaushik Roy"], "summary": "We propose BERMo, an architectural modification to BERT, which makes predictions based on a hierarchy of surface, syntactic and semantic language features. We use linear combination scheme proposed in Embeddings from Language Models (ELMo) to combine the scaled internal representations from different network depths. Our approach has two-fold benefits: (1) improved gradient flow for the downstream task as every layer has a direct connection to the gradients of the loss function and (2) increased representative power as the model no longer needs to copy the features learned in the shallower layer which are necessary for the downstream task. Further, our model has a negligible parameter overhead as there is a single scalar parameter associated with each layer in the network. Experiments on the probing task from SentEval dataset show that our model performs up to 4.65% better in accuracy than the baseline with an average improvement of 2.67% on the semantic tasks. When subject to compression techniques, we find that our model enables stable pruning for compressing small datasets like SST-2, where the BERT model commonly diverges. We observe that our approach converges 1.67\u00d7 and 1.15\u00d7 faster than the baseline on MNLI and QQP tasks from GLUE dataset. Moreover, our results show that our approach can obtain better parameter efficiency for penalty based pruning approaches on QQP task.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.15733", "id": "2110.15733", "pdf": "https://arxiv.org/pdf/2110.15733", "other": "https://arxiv.org/format/2110.15733"}, "title": "Detecting Gender Bias in Transformer-based Models: A Case Study on BERT", "author_info": ["Bingbing Li", "Hongwu Peng", "Rajat Sainju", "Junhuan Yang", "Lei Yang", "Yueying Liang", "Weiwen Jiang", "Binghui Wang", "Hang Liu", "Caiwen Ding"], "summary": "In this paper, we propose a novel gender bias detection method by utilizing attention map for transformer-based models. We 1) give an intuitive gender bias judgement method by comparing the different relation degree between the genders and the occupation according to the attention scores, 2) design a gender bias detector by modifying the attention module, 3) insert the gender bias detector into different positions of the model to present the internal gender bias flow, and 4) draw the consistent gender bias conclusion by scanning the entire Wikipedia, a BERT pretraining dataset. We observe that 1) the attention matrices, Wq and Wk introduce much more gender bias than other modules (including the embedding layer) and 2) the bias degree changes periodically inside of the model (attention matrix Q, K, V, and the remaining part of the attention layer (including the fully-connected layer, the residual connection, and the layer normalization module) enhance the gender bias while the averaged attentions reduces the bias).", "comment": " ACM Class:           I.2; I.7; H.0                "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.14782", "id": "2110.14782", "pdf": "https://arxiv.org/pdf/2110.14782", "other": "https://arxiv.org/format/2110.14782"}, "title": "When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer", "author_info": ["Ameet Deshpande", "Partha Talukdar", "Karthik Narasimhan"], "summary": "While recent work on multilingual language models has demonstrated their capacity for cross-lingual zero-shot transfer on downstream tasks, there is a lack of consensus in the community as to what shared properties between languages enable such transfer. Analyses involving pairs of natural languages are often inconclusive and contradictory since languages simultaneously differ in many linguistic aspects. In this paper, we perform a large-scale empirical study to isolate the effects of various linguistic properties by measuring zero-shot transfer between four diverse natural languages and their counterparts constructed by modifying aspects such as the script, word order, and syntax. Among other things, our experiments show that the absence of sub-word overlap significantly affects zero-shot transfer when languages differ in their word order, and there is a strong correlation between transfer performance and word embedding alignment between languages (e.g., R=0.94 on the task of NLI). Our results call for focus in multilingual models on explicitly improving word embedding alignment between languages rather than relying on its implicit emergence.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.10027", "id": "2110.10027", "pdf": "https://arxiv.org/pdf/2110.10027"}, "title": "Clinical Trial Information Extraction with BERT", "author_info": ["Xiong Liu", "Greg L. Hersch", "Iya Khalil", "Murthy Devarakonda"], "summary": "Natural language processing (NLP) of clinical trial documents can be useful in new trial design. Here we identify entity types relevant to clinical trial design and propose a framework called CT-BERT for information extraction from clinical trial text. We trained named entity recognition (NER) models to extract eligibility criteria entities by fine-tuning a set of pre-trained BERT models. We then compared the performance of CT-BERT with recent baseline methods including attention-based BiLSTM and Criteria2Query. The results demonstrate the superiority of CT-BERT in clinical trial NLP.", "comment": " Comments: HealthNLP 2021, IEEE International Conference on Healthcare Informatics (ICHI 2021) "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.07552", "id": "2110.07552", "pdf": "https://arxiv.org/pdf/2110.07552", "other": "https://arxiv.org/format/2110.07552"}, "title": "BI-RADS BERT & Using Section Tokenization to Understand Radiology Reports", "author_info": ["Grey Kuling", "Dr. Belinda Curpen", "Anne L. Martel"], "summary": "Radiology reports are the main form of communication between radiologists and other clinicians, and contain important information for patient care. However in order to use this information for research it is necessary to convert the raw text into structured data suitable for analysis. Domain specific contextual word embeddings have been shown to achieve impressive accuracy at such natural language processing tasks in medicine. In this work we pre-trained a contextual embedding BERT model using breast radiology reports and developed a classifier that incorporated the embedding with auxiliary global textual features in order to perform a section tokenization task. This model achieved a 98% accuracy at segregating free text reports into sections of information outlined in the Breast Imaging Reporting and Data System (BI-RADS) lexicon, a significant improvement over the Classic BERT model without auxiliary information. We then evaluated whether using section tokenization improved the downstream extraction of the following fields: modality/procedure, previous cancer, menopausal status, purpose of exam, breast density and background parenchymal enhancement. Using the BERT model pre-trained on breast radiology reports combined with section tokenization resulted in an overall accuracy of 95.9% in field extraction. This is a 17% improvement compared to an overall accuracy of 78.9% for field extraction for models without section tokenization and with Classic BERT embeddings. Our work shows the strength of using BERT in radiology report analysis and the advantages of section tokenization in identifying key features of patient factors recorded in breast radiology reports.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.06490", "id": "2110.06490", "pdf": "https://arxiv.org/pdf/2110.06490", "other": "https://arxiv.org/format/2110.06490"}, "title": "Dict-BERT: Enhancing Language Model Pre-training with Dictionary", "author_info": ["Wenhao Yu", "Chenguang Zhu", "Yuwei Fang", "Donghan Yu", "Shuohang Wang", "Yichong Xu", "Michael Zeng", "Meng Jiang"], "summary": "Pre-trained language models (PLMs) aim to learn universal language representations by conducting self-supervised training tasks on large-scale corpora. Since PLMs capture word semantics in different contexts, the quality of word representations highly depends on word frequency, which usually follows a heavy-tailed distributions in the pre-training corpus. Therefore, the embeddings of rare words on the tail are usually poorly optimized. In this work, we focus on enhancing language model pre-training by leveraging definitions of the rare words in dictionaries (e.g., Wiktionary). To incorporate a rare word definition as a part of input, we fetch its definition from the dictionary and append it to the end of the input text sequence. In addition to training with the masked language modeling objective, we propose two novel self-supervised pre-training tasks on word and sentence-level alignment between input text sequence and rare word definitions to enhance language modeling representation with dictionary. We evaluate the proposed Dict-BERT model on the language understanding benchmark GLUE and eight specialized domain benchmark datasets. Extensive experiments demonstrate that Dict-BERT can significantly improve the understanding of rare words and boost model performance on various NLP downstream tasks.", "comment": " Comments: Under Review "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.06376", "id": "2110.06376", "pdf": "https://arxiv.org/pdf/2110.06376", "other": "https://arxiv.org/format/2110.06376"}, "title": "ALL Dolphins Are Intelligent and SOME Are Friendly: Probing BERT for Nouns' Semantic Properties and their Prototypicality", "author_info": ["Marianna Apidianaki", "Aina Gar\u00ed Soler"], "summary": "Large scale language models encode rich commonsense knowledge acquired through exposure to massive data during pre-training, but their understanding of entities and their semantic properties is unclear. We probe BERT (Devlin et al., 2019) for the properties of English nouns as expressed by adjectives that do not restrict the reference scope of the noun they modify (as in \"red car\"), but instead emphasise some inherent aspect (\"red strawberry\"). We base our study on psycholinguistics datasets that capture the association strength between nouns and their semantic features. We probe BERT using cloze tasks and in a classification setting, and show that the model has marginal knowledge of these features and their prevalence as expressed in these datasets. We discuss factors that make evaluation challenging and impede drawing general conclusions about the models' knowledge of noun properties. Finally, we show that when tested in a fine-tuning setting addressing entailment, BERT successfully leverages the information needed for reasoning about the meaning of adjective-noun constructions outperforming previous methods.", "comment": " Comments: Accepted to BlackboxNLP 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.05781", "id": "2110.05781", "pdf": "https://arxiv.org/pdf/2110.05781", "other": "https://arxiv.org/format/2110.05781"}, "title": "BERTraffic: A Robust BERT-Based Approach for Speaker Change Detection and Role Identification of Air-Traffic Communications", "author_info": ["Juan Zuluaga-Gomez", "Seyyed Saeed Sarfjoo", "Amrutha Prasad", "Iuliia Nigmatulina", "Petr Motlicek", "Oliver Ohneiser", "Hartmut Helmke"], "summary": "Automatic Speech Recognition (ASR) is gaining special interest in Air Traffic Control (ATC). ASR allows transcribing the communications between air traffic controllers (ATCOs) and pilots. These transcriptions are used to extract ATC command types and named entities such as aircraft callsigns. One common problem is when the Speech Activity Detection (SAD) or diarization system fails and then two or more single speaker segments are in the same recording, jeopardizing the overall system's performance. We developed a system that combines the segmentation of a SAD module with a BERT-based model that performs Speaker Change Detection (SCD) and Speaker Role Identification (SRI) based on ASR transcripts (i.e., diarization + SRI). This research demonstrates on a real-life ATC test set that performing diarization directly on textual data surpass acoustic level diarization. The proposed model reaches up to ~0.90/~0.95 F1-score on ATCO/pilot for SRI on several test sets. The text-based diarization system brings a 27% relative improvement on Diarization Error Rate (DER) compared to standard acoustic-based diarization. These results were on ASR transcripts of a challenging ATC test set with an estimated ~13% word error rate, validating the approach's robustness even on noisy ASR transcripts.", "comment": " Report number:           Idiap-RR-15-2021                                    "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.05133", "id": "2110.05133", "pdf": "https://arxiv.org/pdf/2110.05133", "other": "https://arxiv.org/format/2110.05133"}, "title": "Offensive Language Detection with BERT-based models, By Customizing Attention Probabilities", "author_info": ["Peyman Alavi", "Pouria Nikvand", "Mehrnoush Shamsfard"], "summary": "This paper describes a novel study on using `Attention Mask' input in transformers and using this approach for detecting offensive content in both English and Persian languages. The paper's principal focus is to suggest a methodology to enhance the performance of the BERT-based models on the `Offensive Language Detection' task. Therefore, we customize attention probabilities by changing the `Attention Mask' input to create more efficacious word embeddings. To do this, we firstly tokenize the training set of the exploited datasets (by BERT tokenizer). Then, we apply Multinomial Naive Bayes to map these tokens to two probabilities. These probabilities indicate the likelihood of making a text non-offensive or offensive, provided that it contains that token. Afterwards, we use these probabilities to define a new term, namely Offensive Score. Next, we create two separate (because of the differences in the types of the employed datasets) equations based on Offensive Scores for each language to re-distribute the `Attention Mask' input for paying more attention to more offensive phrases. Eventually, we put the F1-macro score as our evaluation metric and fine-tune several combinations of BERT with ANNs, CNNs and RNNs to examine the effect of using this methodology on various combinations. The results indicate that all models will enhance with this methodology. The most improvement was 2% and 10% for English and Persian languages, respectively.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.04711", "id": "2110.04711", "pdf": "https://arxiv.org/pdf/2110.04711", "other": "https://arxiv.org/format/2110.04711"}, "title": "SuperShaper: Task-Agnostic Super Pre-training of BERT Models with Variable Hidden Dimensions", "author_info": ["Vinod Ganesan", "Gowtham Ramesh", "Pratyush Kumar"], "summary": "Task-agnostic pre-training followed by task-specific fine-tuning is a default approach to train NLU models. Such models need to be deployed on devices across the cloud and the edge with varying resource and accuracy constraints. For a given task, repeating pre-training and fine-tuning across tens of devices is prohibitively expensive. We propose SuperShaper, a task agnostic pre-training approach which simultaneously pre-trains a large number of Transformer models by varying shapes, i.e., by varying the hidden dimensions across layers. This is enabled by a backbone network with linear bottleneck matrices around each Transformer layer which are sliced to generate differently shaped sub-networks. In spite of its simple design space and efficient implementation, SuperShaper discovers networks that effectively trade-off accuracy and model size: Discovered networks are more accurate than a range of hand-crafted and automatically searched networks on GLUE benchmarks. Further, we find two critical advantages of shape as a design variable for Neural Architecture Search (NAS): (a) heuristics of good shapes can be derived and networks found with these heuristics match and even improve on carefully searched networks across a range of parameter counts, and (b) the latency of networks across multiple CPUs and GPUs are insensitive to the shape and thus enable device-agnostic search. In summary, SuperShaper radically simplifies NAS for language models and discovers networks that generalize across tasks, parameter constraints, and devices.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.04504", "id": "2110.04504", "pdf": "https://arxiv.org/pdf/2110.04504", "other": "https://arxiv.org/format/2110.04504"}, "title": "An Isotropy Analysis in the Multilingual BERT Embedding Space", "author_info": ["Sara Rajaee", "Mohammad Taher Pilehvar"], "summary": "Several studies have explored various advantages of multilingual pre-trained models (e.g., multilingual BERT) in capturing shared linguistic knowledge. However, their limitations have not been paid enough attention. In this paper, we investigate the representation degeneration problem in multilingual contextual word representations (CWRs) of BERT and show that the embedding spaces of the selected languages suffer from anisotropy problem. Our experimental results demonstrate that, similarly to their monolingual counterparts, increasing the isotropy of multilingual embedding space can significantly improve its representation power and performance. Our analysis indicates that although the degenerated directions vary in different languages, they encode similar linguistic knowledge, suggesting a shared linguistic space among languages.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.04399", "id": "2110.04399", "pdf": "https://arxiv.org/pdf/2110.04399", "other": "https://arxiv.org/format/2110.04399"}, "title": "Global Explainability of BERT-Based Evaluation Metrics by Disentangling along Linguistic Factors", "author_info": ["Marvin Kaster", "Wei Zhao", "Steffen Eger"], "summary": "Evaluation metrics are a key ingredient for progress of text generation systems. In recent years, several BERT-based evaluation metrics have been proposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much better with human assessment of text generation quality than BLEU or ROUGE, invented two decades ago. However, little is known what these metrics, which are based on black-box language model representations, actually capture (it is typically assumed they model semantic similarity). In this work, we use a simple regression based global explainability technique to disentangle metric scores along linguistic factors, including semantics, syntax, morphology, and lexical overlap. We show that the different metrics capture all aspects to some degree, but that they are all substantially sensitive to lexical overlap, just like BLEU and ROUGE. This exposes limitations of these novelly proposed metrics, which we also highlight in an adversarial test scenario.", "comment": " Comments: EMNLP2021 Camera Ready "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.03895", "id": "2110.03895", "pdf": "https://arxiv.org/pdf/2110.03895", "other": "https://arxiv.org/format/2110.03895"}, "title": "ALL-IN-ONE: Multi-Task Learning BERT models for Evaluating Peer Assessments", "author_info": ["Qinjin Jia", "Jialin Cui", "Yunkai Xiao", "Chengyuan Liu", "Parvez Rashid", "Edward F. Gehringer"], "summary": "Peer assessment has been widely applied across diverse academic fields over the last few decades and has demonstrated its effectiveness. However, the advantages of peer assessment can only be achieved with high-quality peer reviews. Previous studies have found that high-quality review comments usually comprise several features (e.g., contain suggestions, mention problems, use a positive tone). Thus, researchers have attempted to evaluate peer-review comments by detecting different features using various machine learning and deep learning models. However, there is no single study that investigates using a multi-task learning (MTL) model to detect multiple features simultaneously. This paper presents two MTL models for evaluating peer-review comments by leveraging the state-of-the-art pre-trained language representation models BERT and DistilBERT. Our results demonstrate that BERT-based models significantly outperform previous GloVe-based methods by around 6% in F1-score on tasks of detecting a single feature, and MTL further improves performance while reducing model size.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2110.03183", "id": "2110.03183", "pdf": "https://arxiv.org/pdf/2110.03183", "other": "https://arxiv.org/format/2110.03183"}, "title": "Attention is All You Need? Good Embeddings with Statistics are enough:Large Scale Audio Understanding without Transformers/ Convolutions/ BERTs/ Mixers/ Attention/ RNNs or ....", "author_info": ["Prateek Verma"], "summary": "This paper presents a way of doing large scale audio understanding without traditional state of the art neural architectures. Ever since the introduction of deep learning for understanding audio signals in the past decade, convolutional architectures have been able to achieve state of the art results surpassing traditional hand-crafted features. In the recent past, there has been a similar shift away from traditional convolutional and recurrent neural networks towards purely end-to-end Transformer architectures. We, in this work, explore an approach, based on Bag-of-Words model. Our approach does not have any convolutions, recurrence, attention, transformers or other approaches such as BERT. We utilize micro and macro level clustered vanilla embeddings, and use a MLP head for classification. We only use feed-forward encoder-decoder models to get the bottlenecks of spectral envelops, spectral patches and slices as well as multi-resolution spectra. A classification head (a feed-forward layer), similar to the approach in SimCLR is trained on a learned representation. Using simple codes learned on latent representations, we show how we surpass traditional convolutional neural network architectures, and come strikingly close to outperforming powerful Transformer architectures. This work hopefully would pave way for exciting advancements in the field of representation learning without massive, end-to-end neural architectures.", "comment": " Comments: IEEE Copyright: written as told "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.02523", "id": "2110.02523", "pdf": "https://arxiv.org/pdf/2110.02523", "other": "https://arxiv.org/format/2110.02523"}, "title": "KNN-BERT: Fine-Tuning Pre-Trained Models with KNN Classifier", "author_info": ["Linyang Li", "Demin Song", "Ruotian Ma", "Xipeng Qiu", "Xuanjing Huang"], "summary": "Pre-trained models are widely used in fine-tuning downstream tasks with linear classifiers optimized by the cross-entropy loss, which might face robustness and stability problems. These problems can be improved by learning representations that focus on similarities in the same class and contradictions in different classes when making predictions. In this paper, we utilize the K-Nearest Neighbors Classifier in pre-trained model fine-tuning. For this KNN classifier, we introduce a supervised momentum contrastive learning framework to learn the clustered representations of the supervised downstream tasks. Extensive experiments on text classification tasks and robustness tests show that by incorporating KNNs with the traditional fine-tuning process, we can obtain significant improvements on the clean accuracy in both rich-source and few-shot settings and can improve the robustness against adversarial attacks. \\footnote{all codes is available at https://github.com/LinyangLee/KNN-BERT}", "comment": " Comments: preprint "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.02267", "id": "2110.02267", "pdf": "https://arxiv.org/pdf/2110.02267", "other": "https://arxiv.org/format/2110.02267"}, "title": "BERT Attends the Conversation: Improving Low-Resource Conversational ASR", "author_info": ["Pablo Ortiz", "Simen Burud"], "summary": "We propose new, data-efficient training tasks for BERT models that improve performance of automatic speech recognition (ASR) systems on conversational speech. We include past conversational context and fine-tune BERT on transcript disambiguation without external data to rescore ASR candidates. Our results show word error rate recoveries up to 37.2%. We test our methods in low-resource data domains, both in language (Norwegian), tone (spontaneous, conversational), and topics (parliament proceedings and customer service phone calls). These techniques are applicable to any ASR system and do not require any additional data, provided a pre-trained BERT model. We also show how the performance of our context-augmented rescoring methods strongly depends on the degree of spontaneity and nature of the conversation.", "comment": " Comments: 18 pages, 3 figures; new title and abstract, minor changes, results unchanged; prepared for submission to JMLR "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.02042", "id": "2110.02042", "pdf": "https://arxiv.org/pdf/2110.02042", "other": "https://arxiv.org/format/2110.02042"}, "title": "ur-iw-hnt at GermEval 2021: An Ensembling Strategy with Multiple BERT Models", "author_info": ["Hoai Nam Tran", "Udo Kruschwitz"], "summary": "This paper describes our approach (ur-iw-hnt) for the Shared Task of GermEval2021 to identify toxic, engaging, and fact-claiming comments. We submitted three runs using an ensembling strategy by majority (hard) voting with multiple different BERT models of three different types: German-based, Twitter-based, and multilingual models. All ensemble models outperform single models, while BERTweet is the winner of all individual models in every subtask. Twitter-based models perform better than GermanBERT models, and multilingual models perform worse but by a small margin.", "comment": " Journal ref:         In Proceedings of the GermEval 2021 Workshop on the Identification of Toxic, Engaging, and Fact-Claiming Comments: 17th Conference on Natural Language Processing KONVENS 2021, pages 83-87, Online (2021)       "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.01900", "id": "2110.01900", "pdf": "https://arxiv.org/pdf/2110.01900", "other": "https://arxiv.org/format/2110.01900"}, "title": "DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT", "author_info": ["Heng-Jui Chang", "Shu-wen Yang", "Hung-yi Lee"], "summary": "Self-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75% and 73% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.", "comment": " Comments: Accepted to ICASSP 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.00949", "id": "2110.00949", "pdf": "https://arxiv.org/pdf/2110.00949"}, "title": "Unsupervised paradigm for information extraction from transcripts using BERT", "author_info": ["Aravind Chandramouli", "Siddharth Shukla", "Neeti Nair", "Shiven Purohit", "Shubham Pandey", "Murali Mohana Krishna Dandu"], "summary": "Audio call transcripts are one of the valuable sources of information for multiple downstream use cases such as understanding the voice of the customer and analyzing agent performance. However, these transcripts are noisy in nature and in an industry setting, getting tagged ground truth data is a challenge. In this paper, we present a solution implemented in the industry using BERT Language Models as part of our pipeline to extract key topics and multiple open intents discussed in the call. Another problem statement we looked at was the automatic tagging of transcripts into predefined categories, which traditionally is solved using supervised approach. To overcome the lack of tagged data, all our proposed approaches use unsupervised methods to solve the outlined problems. We evaluate the results by quantitatively comparing the automatically extracted topics, intents and tagged categories with human tagged ground truth and by qualitatively measuring the valuable concepts and intents that are not present in the ground truth. We achieved near human accuracy in extraction of these topics and intents using our novel approach", "comment": " Journal ref:         ECML PKDD 2021       "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.00171", "id": "2110.00171", "pdf": "https://arxiv.org/pdf/2110.00171", "other": "https://arxiv.org/format/2110.00171"}, "title": "BERT4GCN: Using BERT Intermediate Layers to Augment GCN for Aspect-based Sentiment Classification", "author_info": ["Zeguan Xiao", "Jiarun Wu", "Qingliang Chen", "Congjian Deng"], "summary": "Graph-based Aspect-based Sentiment Classification (ABSC) approaches have yielded state-of-the-art results, expecially when equipped with contextual word embedding from pre-training language models (PLMs). However, they ignore sequential features of the context and have not yet made the best of PLMs. In this paper, we propose a novel model, BERT4GCN, which integrates the grammatical sequential features from the PLM of BERT, and the syntactic knowledge from dependency graphs. BERT4GCN utilizes outputs from intermediate layers of BERT and positional information between words to augment GCN (Graph Convolutional Network) to better encode the dependency graphs for the downstream classification. Experimental results demonstrate that the proposed BERT4GCN outperforms all state-of-the-art baselines, justifying that augmenting GCN with the grammatical features from intermediate layers of BERT can significantly empower ABSC models.", "comment": " Comments: To appear in EMNLP 2021, 8 pages, 2 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2110.00125", "id": "2110.00125", "pdf": "https://arxiv.org/pdf/2110.00125", "other": "https://arxiv.org/format/2110.00125"}, "title": "MemBERT: Injecting Unstructured Knowledge into BERT", "author_info": ["Federico Ruggeri", "Marco Lippi", "Paolo Torroni"], "summary": "Transformers changed modern NLP in many ways. However, they can hardly exploit domain knowledge, and like other blackbox models, they lack interpretability. Unfortunately, structured knowledge injection, in the long run, risks to suffer from a knowledge acquisition bottleneck. We thus propose a memory enhancement of transformer models that makes use of unstructured domain knowledge expressed in plain natural language. An experimental evaluation conducted on two challenging NLP tasks demonstrates that our approach yields better performance and model interpretability than baseline transformer-based architectures.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2109.14927", "id": "2109.14927"}, "title": "BERT got a Date: Introducing Transformers to Temporal Tagging", "author_info": ["Satya Almasian", "Dennis Aumiller", "Michael Gertz"], "summary": "Temporal expressions in text play a significant role in language understanding and correctly identifying them is fundamental to various retrieval and natural language processing systems. Previous works have slowly shifted from rule-based to neural architectures, capable of tagging expressions with higher accuracy. However, neural models can not yet distinguish between different expression types at the same level as their rule-based counterparts. In this work, we aim to identify the most suitable transformer architecture for joint temporal tagging and type classification, as well as, investigating the effect of semi-supervised training on the performance of these systems. Based on our study of token classification variants and encoder-decoder architectures, we present a transformer encoder-decoder model using the RoBERTa language model as our best performing system. By supplementing training resources with weakly labeled data from rule-based systems, our model surpasses previous works in temporal tagging and type classification, especially on rare classes. Our code and pre-trained experiments are available at: https://github.com/satya77/Transformer_Temporal_Tagger", "comment": " Comments: unreliable evaluation results for Seq2seq models "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.13890", "id": "2109.13890", "pdf": "https://arxiv.org/pdf/2109.13890"}, "title": "How Different Text-preprocessing Techniques Using The BERT Model Affect The Gender Profiling of Authors", "author_info": ["Esam Alzahrani", "Leon Jololian"], "summary": "Forensic author profiling plays an important role in indicating possible profiles for suspects. Among the many automated solutions recently proposed for author profiling, transfer learning outperforms many other state-of-the-art techniques in natural language processing. Nevertheless, the sophisticated technique has yet to be fully exploited for author profiling. At the same time, whereas current methods of author profiling, all largely based on features engineering, have spawned significant variation in each model used, transfer learning usually requires a preprocessed text to be fed into the model. We reviewed multiple references in the literature and determined the most common preprocessing techniques associated with authors' genders profiling. Considering the variations in potential preprocessing techniques, we conducted an experimental study that involved applying five such techniques to measure each technique's effect while using the BERT model, chosen for being one of the most-used stock pretrained models. We used the Hugging face transformer library to implement the code for each preprocessing case. In our five experiments, we found that BERT achieves the best accuracy in predicting the gender of the author when no preprocessing technique is applied. Our best case achieved 86.67% accuracy in predicting the gender of authors.", "comment": " Journal ref:         Advances in Machine Learning 3rd International Conference on Machine Learning & Applications (CMLA 2021), September 25~26, 2021, Toronto, Canada Volume Editors : David C. Wyld, Dhinaharan Nagamalai (Eds) ISBN : 978-1-925953-49-7       "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.13348", "id": "2109.13348", "pdf": "https://arxiv.org/pdf/2109.13348", "other": "https://arxiv.org/format/2109.13348"}, "title": "Evaluating Biomedical BERT Models for Vocabulary Alignment at Scale in the UMLS Metathesaurus", "author_info": ["Goonmeet Bajaj", "Vinh Nguyen", "Thilini Wijesiriwardene", "Hong Yung Yip", "Vishesh Javangula", "Srinivasan Parthasarathy", "Amit Sheth", "Olivier Bodenreider"], "summary": "The current UMLS (Unified Medical Language System) Metathesaurus construction process for integrating over 200 biomedical source vocabularies is expensive and error-prone as it relies on the lexical algorithms and human editors for deciding if the two biomedical terms are synonymous. Recent advances in Natural Language Processing such as Transformer models like BERT and its biomedical variants with contextualized word embeddings have achieved state-of-the-art (SOTA) performance on downstream tasks. We aim to validate if these approaches using the BERT models can actually outperform the existing approaches for predicting synonymy in the UMLS Metathesaurus. In the existing Siamese Networks with LSTM and BioWordVec embeddings, we replace the BioWordVec embeddings with the biomedical BERT embeddings extracted from each BERT model using different ways of extraction. In the Transformer architecture, we evaluate the use of the different biomedical BERT models that have been pre-trained using different datasets and tasks. Given the SOTA performance of these BERT models for other downstream tasks, our experiments yield surprisingly interesting results: (1) in both model architectures, the approaches employing these biomedical BERT-based models do not outperform the existing approaches using Siamese Network with BioWordVec embeddings for the UMLS synonymy prediction task, (2) the original BioBERT large model that has not been pre-trained with the UMLS outperforms the SapBERT models that have been pre-trained with the UMLS, and (3) using the Siamese Networks yields better performance for synonymy prediction when compared to using the biomedical BERT models.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2109.12683", "id": "2109.12683", "pdf": "https://arxiv.org/pdf/2109.12683", "other": "https://arxiv.org/format/2109.12683"}, "title": "On the Prunability of Attention Heads in Multilingual BERT", "author_info": ["Aakriti Budhraja", "Madhura Pande", "Pratyush Kumar", "Mitesh M. Khapra"], "summary": "Large multilingual models, such as mBERT, have shown promise in crosslingual transfer. In this work, we employ pruning to quantify the robustness and interpret layer-wise importance of mBERT. On four GLUE tasks, the relative drops in accuracy due to pruning have almost identical results on mBERT and BERT suggesting that the reduced attention capacity of the multilingual models does not affect robustness to pruning. For the crosslingual task XNLI, we report higher drops in accuracy with pruning indicating lower robustness in crosslingual transfer. Also, the importance of the encoder layers sensitively depends on the language family and the pre-training corpus size. The top layers, which are relatively more influenced by fine-tuning, encode important information for languages similar to English (SVO) while the bottom layers, which are relatively less influenced by fine-tuning, are particularly important for agglutinative and low-resource languages.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2109.11888", "id": "2109.11888", "pdf": "https://arxiv.org/pdf/2109.11888", "other": "https://arxiv.org/format/2109.11888"}, "title": "Robustness and Sensitivity of BERT Models Predicting Alzheimer's Disease from Text", "author_info": ["Jekaterina Novikova"], "summary": "Understanding robustness and sensitivity of BERT models predicting Alzheimer's disease from text is important for both developing better classification models and for understanding their capabilities and limitations. In this paper, we analyze how a controlled amount of desired and undesired text alterations impacts performance of BERT. We show that BERT is robust to natural linguistic variations in text. On the other hand, we show that BERT is not sensitive to removing clinically important information from text.", "comment": " Comments: Accepted to W-NUT @ EMNLP 2021 (upd: correction in Table 3) "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.11745", "id": "2109.11745", "pdf": "https://arxiv.org/pdf/2109.11745", "other": "https://arxiv.org/format/2109.11745"}, "title": "DACT-BERT: Differentiable Adaptive Computation Time for an Efficient BERT Inference", "author_info": ["Crist\u00f3bal Eyzaguirre", "Felipe del R\u00edo", "Vladimir Araujo", "\u00c1lvaro Soto"], "summary": "Large-scale pre-trained language models have shown remarkable results in diverse NLP applications. Unfortunately, these performance gains have been accompanied by a significant increase in computation time and model size, stressing the need to develop new or complementary strategies to increase the efficiency of these models. In this paper we propose DACT-BERT, a differentiable adaptive computation time strategy for BERT-like models. DACT-BERT adds an adaptive computational mechanism to BERT's regular processing pipeline, which controls the number of Transformer blocks that need to be executed at inference time. By doing this, the model learns to combine the most appropriate intermediate representations for the task at hand. Our experiments demonstrate that our approach, when compared to the baselines, excels on a reduced computational regime and is competitive in other less restrictive ones.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2109.11491", "id": "2109.11491", "pdf": "https://arxiv.org/pdf/2109.11491", "other": "https://arxiv.org/format/2109.11491"}, "title": "Putting Words in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords", "author_info": ["Taelin Karidi", "Yichu Zhou", "Nathan Schneider", "Omri Abend", "Vivek Srikumar"], "summary": "We present a method for exploring regions around individual points in a contextualized vector space (particularly, BERT space), as a way to investigate how these regions correspond to word senses. By inducing a contextualized \"pseudoword\" as a stand-in for a static embedding in the input layer, and then performing masked prediction of a word in the sentence, we are able to investigate the geometry of the BERT-space in a controlled manner around individual instances. Using our method on a set of carefully constructed sentences targeting ambiguous English words, we find substantial regularity in the contextualized space, with regions that correspond to distinct word senses; but between these regions there are occasionally \"sense voids\" -- regions that do not correspond to any intelligible sense.", "comment": " Comments: EMNLP 2021 camera-ready version "}, {"arxiv": {"page": "https://arxiv.org/abs/2109.11308", "id": "2109.11308", "pdf": "https://arxiv.org/pdf/2109.11308", "other": "https://arxiv.org/format/2109.11308"}, "title": "Breaking BERT: Understanding its Vulnerabilities for Named Entity Recognition through Adversarial Attack", "author_info": ["Anne Dirkson", "Suzan Verberne", "Wessel Kraaij"], "summary": "Both generic and domain-specific BERT models are widely used for natural language processing (NLP) tasks. In this paper we investigate the vulnerability of BERT models to variation in input data for Named Entity Recognition (NER) through adversarial attack. Experimental results show that BERT models are vulnerable to variation in the entity context with 20.2 to 45.0% of entities predicted completely wrong and another 29.3 to 53.3% of entities predicted wrong partially. BERT models seem most vulnerable to changes in the local context of entities and often a single change is sufficient to fool the model. The domain-specific BERT model trained from scratch (SciBERT) is more vulnerable than the original BERT model or the domain-specific model that retains the BERT vocabulary (BioBERT). We also find that BERT models are particularly vulnerable to emergent entities. Our results chart the vulnerabilities of BERT models for NER and emphasize the importance of further research into uncovering and reducing these weaknesses.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2109.11010", "id": "2109.11010", "pdf": "https://arxiv.org/pdf/2109.11010", "other": "https://arxiv.org/format/2109.11010"}, "title": "Alzheimers Dementia Detection using Acoustic & Linguistic features and Pre-Trained BERT", "author_info": ["Akshay Valsaraj", "Ithihas Madala", "Nikhil Garg", "Veeky Baths"], "summary": "Alzheimers disease is a fatal progressive brain disorder that worsens with time. It is high time we have inexpensive and quick clinical diagnostic techniques for early detection and care. In previous studies, various Machine Learning techniques and Pre-trained Deep Learning models have been used in conjunction with the extraction of various acoustic and linguistic features. Our study focuses on three models for the classification task in the ADReSS (The Alzheimers Dementia Recognition through Spontaneous Speech) 2021 Challenge. We use the well-balanced dataset provided by the ADReSS Challenge for training and validating our models. Model 1 uses various acoustic features from the eGeMAPs feature-set, Model 2 uses various linguistic features that we generated from auto-generated transcripts and Model 3 uses the auto-generated transcripts directly to extract features using a Pre-trained BERT and TF-IDF. These models are described in detail in the models section.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2109.09780", "id": "2109.09780", "pdf": "https://arxiv.org/pdf/2109.09780", "other": "https://arxiv.org/format/2109.09780"}, "title": "BERT Has Uncommon Sense: Similarity Ranking for Word Sense BERTology", "author_info": ["Luke Gessler", "Nathan Schneider"], "summary": "An important question concerning contextualized word embedding (CWE) models like BERT is how well they can represent different word senses, especially those in the long tail of uncommon senses. Rather than build a WSD system as in previous work, we investigate contextualized embedding neighborhoods directly, formulating a query-by-example nearest neighbor retrieval task and examining ranking performance for words and senses in different frequency bands. In an evaluation on two English sense-annotated corpora, we find that several popular CWE models all outperform a random baseline even for proportionally rare senses, without explicit sense supervision. However, performance varies considerably even among models with similar architectures and pretraining regimes, with especially large differences for rare word senses, revealing that CWE models are not all created equal when it comes to approximating word senses in their native representations.", "comment": " Comments: Accepted at BlackboxNLP 2021 "}]}