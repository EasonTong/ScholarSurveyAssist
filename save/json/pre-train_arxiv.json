{"data": [{"arxiv": {"page": "https://arxiv.org/abs/2202.10936", "id": "2202.10936", "pdf": "https://arxiv.org/pdf/2202.10936", "other": "https://arxiv.org/format/2202.10936"}, "title": "A Survey of Vision-Language Pre-Trained Models", "author_info": ["Yifan Du", "Zikang Liu", "Junyi Li", "Wayne Xin Zhao"], "summary": "As Transformer evolved, pre-trained models have advanced at a breakneck pace in recent years. They have dominated the mainstream techniques in natural language processing (NLP) and computer vision (CV). How to adapt pre-training to the field of Vision-and-Language (V-L) learning and improve the performance on downstream tasks becomes a focus of multimodal learning. In this paper, we review the recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the core content, we first briefly introduce several ways to encode raw images and texts to single-modal embeddings before pre-training. Then, we dive into the mainstream architectures of VL-PTMs in modeling the interaction between text and image representations. We further present widely-used pre-training tasks, after which we introduce some common downstream tasks. We finally conclude this paper and present some promising research directions. Our survey aims to provide multimodal researchers a synthesis and pointer to related research.", "comment": " Comments: Under review "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10401", "id": "2202.10401", "pdf": "https://arxiv.org/pdf/2202.10401", "other": "https://arxiv.org/format/2202.10401"}, "title": "Vision-Language Pre-Training with Triple Contrastive Learning", "author_info": ["Jinyu Yang", "Jiali Duan", "Son Tran", "Yi Xu", "Sampath Chanda", "Liqun Chen", "Belinda Zeng", "Trishul Chilimbi", "Junzhou Huang"], "summary": "Vision-language representation learning largely benefits from image-text alignment through contrastive losses (e.g., InfoNCE loss). The success of this alignment strategy is attributed to its capability in maximizing the mutual information (MI) between an image and its matched text. However, simply performing cross-modal alignment (CMA) ignores data potential within each modality, which may result in degraded representations. For instance, although CMA-based models are able to map image-text pairs close together in the embedding space, they fail to ensure that similar inputs from the same modality stay close by. This problem can get even worse when the pre-training data is noisy. In this paper, we propose triple contrastive learning (TCL) for vision-language pre-training by leveraging both cross-modal and intra-modal self-supervision. Besides CMA, TCL introduces an intra-modal contrastive objective to provide complementary benefits in representation learning. To take advantage of localized and structural information from image and text input, TCL further maximizes the average MI between local regions of image/text and their global summary. To the best of our knowledge, ours is the first work that takes into account local structure information for multi-modality representation learning. Experimental evaluations show that our approach is competitive and achieve the new state of the art on various common down-stream vision-language tasks such as image-text retrieval and visual question answering.", "comment": " Comments: code: https://github.com/uta-smile/TCL "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.10139", "id": "2202.10139", "pdf": "https://arxiv.org/pdf/2202.10139", "other": "https://arxiv.org/format/2202.10139"}, "title": "S3T: Self-Supervised Pre-training with Swin Transformer for Music Classification", "author_info": ["Hang Zhao", "Chen Zhang", "Belei Zhu", "Zejun Ma", "Kejun Zhang"], "summary": "In this paper, we propose S3T, a self-supervised pre-training method with Swin Transformer for music classification, aiming to learn meaningful music representations from massive easily accessible unlabeled music data. S3T introduces a momentum-based paradigm, MoCo, with Swin Transformer as its feature extractor to music time-frequency domain. For better music representations learning, S3T contributes a music data augmentation pipeline and two specially designed pre-processors. To our knowledge, S3T is the first method combining the Swin Transformer with a self-supervised learning method for music classification. We evaluate S3T on music genre classification and music tagging tasks with linear classifiers trained on learned representations. Experimental results show that S3T outperforms the previous self-supervised method (CLMR) by 12.5 percents top-1 accuracy and 4.8 percents PR-AUC on two tasks respectively, and also surpasses the task-specific state-of-the-art supervised methods. Besides, S3T shows advances in label efficiency using only 10% labeled data exceeding CLMR on both tasks with 100% labeled data.", "comment": " Comments: Accepted by ICASSP2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09817", "id": "2202.09817", "pdf": "https://arxiv.org/pdf/2202.09817", "other": "https://arxiv.org/format/2202.09817"}, "title": "-Tuning: An Efficient Tuning Paradigm for Large-Scale Pre-Trained Models via Label Representation Learning", "author_info": ["Yitao Liu", "Chenxin An", "Xipeng Qiu"], "summary": "With the success of large-scale pre-trained models (PTMs), how efficiently adapting PTMs to downstream tasks has attracted tremendous attention, especially for PTMs with billions of parameters. Although some parameter-efficient tuning paradigms have been proposed to address this problem, they still require large resources to compute the gradients in the training phase. In this paper, we propose Y-Tuning, an efficient yet effective paradigm to adapt frozen large-scale PTMs to specific downstream tasks. Y-tuning learns dense representations for labels Y defined in a given task and aligns them to fixed feature representation. Without tuning the features of input text and model parameters, Y-tuning is both parameter-efficient and training-efficient. For DeBERTaXXL with 1.6 billion parameters, Y-tuning achieves performance more than 96% of full fine-tuning on GLUE Benchmark with only 2% tunable parameters and much fewer training costs.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.09061", "id": "2202.09061", "pdf": "https://arxiv.org/pdf/2202.09061", "other": "https://arxiv.org/format/2202.09061"}, "title": "VLP: A Survey on Vision-Language Pre-training", "author_info": ["Feilong Chen", "Duzhen Zhang", "Minglun Han", "Xiuyi Chen", "Jing Shi", "Shuang Xu", "Bo Xu"], "summary": "In the past few years, the emergence of pre-training models has brought uni-modal fields such as computer vision (CV) and natural language processing (NLP) to a new era. Substantial works have shown they are beneficial for downstream uni-modal tasks and avoid training a new model from scratch. So can such pre-trained models be applied to multi-modal tasks? Researchers have explored this problem and made significant progress. This paper surveys recent advances and new frontiers in vision-language pre-training (VLP), including image-text and video-text pre-training. To give readers a better overall grasp of VLP, we first review its recent advances from five aspects: feature extraction, model architecture, pre-training objectives, pre-training datasets, and downstream tasks. Then, we summarize the specific VLP models in detail. Finally, we discuss the new frontiers in VLP. To the best of our knowledge, this is the first survey on VLP. We hope that this survey can shed light on future research in the VLP field.", "comment": " Comments: A Survey on Vision-Language Pre-training "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.08772", "id": "2202.08772", "pdf": "https://arxiv.org/pdf/2202.08772", "other": "https://arxiv.org/format/2202.08772"}, "title": "A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models", "author_info": ["Da Yin", "Li Dong", "Hao Cheng", "Xiaodong Liu", "Kai-Wei Chang", "Furu Wei", "Jianfeng Gao"], "summary": "With the increasing of model capacity brought by pre-trained language models, there emerges boosting needs for more knowledgeable natural language processing (NLP) models with advanced functionalities including providing and making flexible use of encyclopedic and commonsense knowledge. The mere pre-trained language models, however, lack the capacity of handling such knowledge-intensive NLP tasks alone. To address this challenge, large numbers of pre-trained language models augmented with external knowledge sources are proposed and in rapid development. In this paper, we aim to summarize the current progress of pre-trained language model-based knowledge-enhanced models (PLMKEs) by dissecting their three vital elements: knowledge sources, knowledge-intensive NLP tasks, and knowledge fusion methods. Finally, we present the challenges of PLMKEs based on the discussion regarding the three elements and attempt to provide NLP practitioners with potential directions for further research.", "comment": " Comments: Work in progress "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07816", "id": "2202.07816", "pdf": "https://arxiv.org/pdf/2202.07816", "other": "https://arxiv.org/format/2202.07816"}, "title": "ProsoSpeech: Enhancing Prosody With Quantized Vector Pre-training in Text-to-Speech", "author_info": ["Yi Ren", "Ming Lei", "Zhiying Huang", "Shiliang Zhang", "Qian Chen", "Zhijie Yan", "Zhou Zhao"], "summary": "Expressive text-to-speech (TTS) has become a hot research topic recently, mainly focusing on modeling prosody in speech. Prosody modeling has several challenges: 1) the extracted pitch used in previous prosody modeling works have inevitable errors, which hurts the prosody modeling; 2) different attributes of prosody (e.g., pitch, duration and energy) are dependent on each other and produce the natural prosody together; and 3) due to high variability of prosody and the limited amount of high-quality data for TTS training, the distribution of prosody cannot be fully shaped. To tackle these issues, we propose ProsoSpeech, which enhances the prosody using quantized latent vectors pre-trained on large-scale unpaired and low-quality text and speech data. Specifically, we first introduce a word-level prosody encoder, which quantizes the low-frequency band of the speech and compresses prosody attributes in the latent prosody vector (LPV). Then we introduce an LPV predictor, which predicts LPV given word sequence. We pre-train the LPV predictor on large-scale text and low-quality speech data and fine-tune it on the high-quality TTS dataset. Finally, our model can generate expressive speech conditioned on the predicted LPV. Experimental results show that ProsoSpeech can generate speech with richer prosody compared with baseline methods.", "comment": " Comments: Accepted by ICASSP 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.07499", "id": "2202.07499", "pdf": "https://arxiv.org/pdf/2202.07499", "other": "https://arxiv.org/format/2202.07499"}, "title": "Texture Aware Autoencoder Pre-training And Pairwise Learning Refinement For Improved Iris Recognition", "author_info": ["Manashi Chakraborty", "Aritri Chakraborty", "Prabir Kumar Biswas", "Pabitra Mitra"], "summary": "This paper presents a texture aware end-to-end trainable iris recognition system, specifically designed for datasets like iris having limited training data. We build upon our previous stagewise learning framework with certain key optimization and architectural innovations. First, we pretrain a Stage-1 encoder network with an unsupervised autoencoder learning optimized with an additional data relation loss on top of usual reconstruction loss. The data relation loss enables learning better texture representation which is pivotal for a texture rich dataset such as iris. Robustness of Stage-1 feature representation is further enhanced with an auxiliary denoising task. Such pre-training proves beneficial for effectively training deep networks on data constrained iris datasets. Next, in Stage-2 supervised refinement, we design a pairwise learning architecture for an end-to-end trainable iris recognition system. The pairwise learning includes the task of iris matching inside the training pipeline itself and results in significant improvement in recognition performance compared to usual offline matching. We validate our model across three publicly available iris datasets and the proposed model consistently outperforms both traditional and deep learning baselines for both Within-Dataset and Cross-Dataset configurations", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06884", "id": "2202.06884", "pdf": "https://arxiv.org/pdf/2202.06884", "other": "https://arxiv.org/format/2202.06884"}, "title": "COLA: COarse LAbel pre-training for 3D semantic segmentation of sparse LiDAR datasets", "author_info": ["Jules Sanchez", "Jean-Emmanuel Deschaud", "Fran\u00e7ois Goulette"], "summary": "Transfer learning is a proven technique in 2D computer vision to leverage the large amount of data available and achieve high performance with datasets limited in size due to the cost of acquisition or annotation. In 3D, annotation is known to be a costly task; nevertheless, transfer learning methods have only recently been investigated. Unsupervised pre-training has been heavily favored as no very large annotated dataset are available. In this work, we tackle the case of real-time 3D semantic segmentation of sparse outdoor LiDAR scans. Such datasets have been on the rise, but with different label sets even for the same task. In this work, we propose here an intermediate-level label set called the coarse labels, which allows all the data available to be leveraged without any manual labelization. This way, we have access to a larger dataset, alongside a simpler task of semantic segmentation. With it, we introduce a new pre-training task: the coarse label pre-training, also called COLA. We thoroughly analyze the impact of COLA on various datasets and architectures and show that it yields a noticeable performance improvement, especially when the finetuning task has access only to a small dataset.", "comment": " Comments: preprint for IROS "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06862", "id": "2202.06862", "pdf": "https://arxiv.org/pdf/2202.06862", "other": "https://arxiv.org/format/2202.06862"}, "title": "Threats to Pre-trained Language Models: Survey and Taxonomy", "author_info": ["Shangwei Guo", "Chunlong Xie", "Jiwei Li", "Lingjuan Lyu", "Tianwei Zhang"], "summary": "Pre-trained language models (PTLMs) have achieved great success and remarkable performance over a wide range of natural language processing (NLP) tasks. However, there are also growing concerns regarding the potential security issues in the adoption of PTLMs. In this survey, we comprehensively systematize recently discovered threats to PTLM systems and applications. We perform our attack characterization from three interesting perspectives. (1) We show threats can occur at different stages of the PTLM pipeline raised by different malicious entities. (2) We identify two types of model transferability (landscape, portrait) that facilitate attacks. (3) Based on the attack goals, we summarize four categories of attacks (backdoor, evasion, data privacy and model privacy). We also discuss some open problems and research directions. We believe our survey and taxonomy will inspire future studies towards secure and privacy-preserving PTLMs.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06840", "id": "2202.06840", "pdf": "https://arxiv.org/pdf/2202.06840", "other": "https://arxiv.org/format/2202.06840"}, "title": "What Do They Capture? -- A Structural Analysis of Pre-Trained Language Models for Source Code", "author_info": ["Yao Wan", "Wei Zhao", "Hongyu Zhang", "Yulei Sui", "Guandong Xu", "Hai Jin"], "summary": "Recently, many pre-trained language models for source code have been proposed to model the context of code and serve as a basis for downstream code intelligence tasks such as code completion, code search, and code summarization. These models leverage masked pre-training and Transformer and have achieved promising results. However, currently there is still little progress regarding interpretability of existing pre-trained code models. It is not clear why these models work and what feature correlations they can capture. In this paper, we conduct a thorough structural analysis aiming to provide an interpretation of pre-trained language models for source code (e.g., CodeBERT, and GraphCodeBERT) from three distinctive perspectives: (1) attention analysis, (2) probing on the word embedding, and (3) syntax tree induction. Through comprehensive analysis, this paper reveals several insightful findings that may inspire future studies: (1) Attention aligns strongly with the syntax structure of code. (2) Pre-training language models of code can preserve the syntax structure of code in the intermediate representations of each Transformer layer. (3) The pre-trained models of code have the ability of inducing syntax trees of code. Theses findings suggest that it may be helpful to incorporate the syntax structure of code into the process of pre-training for better code representations.", "comment": " Comments: Accepted by ICSE 2022 (The 44th International Conference on Software Engineering) "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06767", "id": "2202.06767", "pdf": "https://arxiv.org/pdf/2202.06767", "other": "https://arxiv.org/format/2202.06767"}, "title": "Wukong: 100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework", "author_info": ["Jiaxi Gu", "Xiaojun Meng", "Guansong Lu", "Lu Hou", "Minzhe Niu", "Hang Xu", "Xiaodan Liang", "Wei Zhang", "Xin Jiang", "Chunjing Xu"], "summary": "This paper presents a large-scale Chinese cross-modal dataset for benchmarking different multi-modal pre-training methods to facilitate the Vision-Language Pre-training (VLP) research and community development. Recent dual-stream VLP models like CLIP, ALIGN and FILIP have shown remarkable performance on various downstream tasks as well as their remarkable zero-shot ability in the open domain tasks. However, their success heavily relies on the scale of pre-trained datasets. Though there have been both small-scale vision-language English datasets like Flickr30k, CC12M as well as large-scale LAION-400M, the current community lacks large-scale Vision-Language benchmarks in Chinese, hindering the development of broader multilingual applications. On the other hand, there is very rare publicly available large-scale Chinese cross-modal pre-training dataset that has been released, making it hard to use pre-trained models as services for downstream tasks. In this work, we release a Large-Scale Chinese Cross-modal dataset named Wukong, containing 100 million Chinese image-text pairs from the web. Furthermore, we release a group of big models pre-trained with advanced image encoders (ResNet/ViT/SwinT) and different pre-training methods (CLIP/FILIP/LiT). We provide extensive experiments, a deep benchmarking of different downstream tasks, and some exciting findings. Experiments show that Wukong can serve as a promising Chinese pre-training dataset and benchmark for different cross-modal learning methods, which gives superior performance on various downstream tasks such as zero-shot image classification and image-text retrieval benchmarks. More information can refer to https://wukong-dataset.github.io/wukong-dataset/.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06600", "id": "2202.06600", "pdf": "https://arxiv.org/pdf/2202.06600"}, "title": "Research on Dual Channel News Headline Classification Based on ERNIE Pre-training Model", "author_info": ["Junjie Li", "Hui Cao"], "summary": "The classification of news headlines is an important direction in the field of NLP, and its data has the characteristics of compactness, uniqueness and various forms. Aiming at the problem that the traditional neural network model cannot adequately capture the underlying feature information of the data and cannot jointly extract key global features and deep local features, a dual-channel network model DC-EBAD based on the ERNIE pre-training model is proposed. Use ERNIE to extract the lexical, semantic and contextual feature information at the bottom of the text, generate dynamic word vector representations fused with context, and then use the BiLSTM-AT network channel to secondary extract the global features of the data and use the attention mechanism to give key parts higher The weight of the DPCNN channel is used to overcome the long-distance text dependence problem and obtain deep local features. The local and global feature vectors are spliced, and finally passed to the fully connected layer, and the final classification result is output through Softmax. The experimental results show that the proposed model improves the accuracy, precision and F1-score of news headline classification compared with the traditional neural network model and the single-channel model under the same conditions. It can be seen that it can perform well in the multi-classification application of news headline text under large data volume.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.06335", "id": "2202.06335", "pdf": "https://arxiv.org/pdf/2202.06335", "other": "https://arxiv.org/format/2202.06335"}, "title": "ET-BERT: A Contextualized Datagram Representation with Pre-training Transformers for Encrypted Traffic Classification", "author_info": ["Xinjie Lin", "Gang Xiong", "Gaopeng Gou", "Zhen Li", "Junzheng Shi", "Jing Yu"], "summary": "Encrypted traffic classification requires discriminative and robust traffic representation captured from content-invisible and imbalanced traffic data for accurate classification, which is challenging but indispensable to achieve network security and network management. The major limitation of existing solutions is that they highly rely on the deep features, which are overly dependent on data size and hard to generalize on unseen data. How to leverage the open-domain unlabeled traffic data to learn representation with strong generalization ability remains a key challenge. In this paper,we propose a new traffic representation model called Encrypted Traffic Bidirectional Encoder Representations from Transformer (ET-BERT), which pre-trains deep contextualized datagram-level representation from large-scale unlabeled data. The pre-trained model can be fine-tuned on a small number of task-specific labeled data and achieves state-of-the-art performance across five encrypted traffic classification tasks, remarkably pushing the F1 of ISCX-Tor to 99.2% (4.4% absolute improvement), ISCX-VPN-Service to 98.9% (5.2% absolute improvement), Cross-Platform (Android) to 92.5% (5.4% absolute improvement), CSTNET-TLS 1.3 to 97.4% (10.0% absolute improvement). Notably, we provide explanation of the empirically powerful pre-training model by analyzing the randomness of ciphers. It gives us insights in understanding the boundary of classification ability over encrypted traffic. The code is available at: https://github.com/linwhitehat/ET-BERT.", "comment": " Comments: This work has been accepted in Security, Privacy, and Trust track at The Web Conference 2022 (WWW'22)(see https://www2022.thewebconf.org/cfp/research/security/) "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.05269", "id": "2202.05269", "pdf": "https://arxiv.org/pdf/2202.05269", "other": "https://arxiv.org/format/2202.05269"}, "title": "A Plug-and-Play Approach to Multiparametric Quantitative MRI: Image Reconstruction using Pre-Trained Deep Denoisers", "author_info": ["Ketan Fatania", "Carolin M. Pirkl", "Marion I. Menzel", "Peter Hall", "Mohammad Golbabaee"], "summary": "Current spatiotemporal deep learning approaches to Magnetic Resonance Fingerprinting (MRF) build artefact-removal models customised to a particular k-space subsampling pattern which is used for fast (compressed) acquisition. This may not be useful when the acquisition process is unknown during training of the deep learning model and/or changes during testing time. This paper proposes an iterative deep learning plug-and-play reconstruction approach to MRF which is adaptive to the forward acquisition process. Spatiotemporal image priors are learned by an image denoiser i.e. a Convolutional Neural Network (CNN), trained to remove generic white gaussian noise (not a particular subsampling artefact) from data. This CNN denoiser is then used as a data-driven shrinkage operator within the iterative reconstruction algorithm. This algorithm with the same denoiser model is then tested on two simulated acquisition processes with distinct subsampling patterns. The results show consistent de-aliasing performance against both acquisition schemes and accurate mapping of tissues' quantitative bio-properties. Software available: https://github.com/ketanfatania/QMRI-PnP-Recon-POC", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.04639", "id": "2202.04639", "pdf": "https://arxiv.org/pdf/2202.04639", "other": "https://arxiv.org/format/2202.04639"}, "title": "Point-Level Region Contrast for Object Detection Pre-Training", "author_info": ["Yutong Bai", "Xinlei Chen", "Alexander Kirillov", "Alan Yuille", "Alexander C. Berg"], "summary": "In this work we present point-level region contrast, a self-supervised pre-training approach for the task of object detection. This approach is motivated by the two key factors in detection: localization and recognition. While accurate localization favors models that operate at the pixel- or point-level, correct recognition typically relies on a more holistic, region-level view of objects. Incorporating this perspective in pre-training, our approach performs contrastive learning by directly sampling individual point pairs from different regions. Compared to an aggregated representation per region, our approach is more robust to the change in input region quality, and further enables us to implicitly improve initial region assignments via online knowledge distillation during training. Both advantages are important when dealing with imperfect regions encountered in the unsupervised setting. Experiments show point-level region contrast improves on state-of-the-art pre-training methods for object detection and segmentation across multiple tasks and datasets, and we provide extensive ablation studies and visualizations to aid understanding. Code will be made available.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.04298", "id": "2202.04298", "pdf": "https://arxiv.org/pdf/2202.04298", "other": "https://arxiv.org/format/2202.04298"}, "title": "Image Difference Captioning with Pre-training and Contrastive Learning", "author_info": ["Linli Yao", "Weiying Wang", "Qin Jin"], "summary": "The Image Difference Captioning (IDC) task aims to describe the visual differences between two similar images with natural language. The major challenges of this task lie in two aspects: 1) fine-grained visual differences that require learning stronger vision and language association and 2) high-cost of manual annotations that leads to limited supervised data. To address these challenges, we propose a new modeling framework following the pre-training-finetuning paradigm. Specifically, we design three self-supervised tasks and contrastive learning strategies to align visual differences and text descriptions at a fine-grained level. Moreover, we propose a data expansion strategy to utilize extra cross-task supervision information, such as data for fine-grained image classification, to alleviate the limitation of available supervised IDC data. Extensive experiments on two IDC benchmark datasets, CLEVR-Change and Birds-to-Words, demonstrate the effectiveness of the proposed modeling framework. The codes and models will be released at https://github.com/yaolinli/IDC.", "comment": " Comments: Accepted to AAAI2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03612", "id": "2202.03612", "pdf": "https://arxiv.org/pdf/2202.03612", "other": "https://arxiv.org/format/2202.03612"}, "title": "HistBERT: A Pre-trained Language Model for Diachronic Lexical Semantic Analysis", "author_info": ["Wenjun Qiu", "Yang Xu"], "summary": "Contextualized word embeddings have demonstrated state-of-the-art performance in various natural language processing tasks including those that concern historical semantic change. However, language models such as BERT was trained primarily on contemporary corpus data. To investigate whether training on historical corpus data improves diachronic semantic analysis, we present a pre-trained BERT-based language model, HistBERT, trained on the balanced Corpus of Historical American English. We examine the effectiveness of our approach by comparing the performance of the original BERT and that of HistBERT, and we report promising results in word similarity and semantic shift analysis. Our work suggests that the effectiveness of contextual embeddings in diachronic semantic analysis is dependent on the temporal profile of the input text and care should be taken in applying this methodology to study historical semantic change.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.03382", "id": "2202.03382", "pdf": "https://arxiv.org/pdf/2202.03382", "other": "https://arxiv.org/format/2202.03382"}, "title": "Corrupted Image Modeling for Self-Supervised Visual Pre-Training", "author_info": ["Yuxin Fang", "Li Dong", "Hangbo Bao", "Xinggang Wang", "Furu Wei"], "summary": "We introduce Corrupted Image Modeling (CIM) for self-supervised visual pre-training. CIM uses an auxiliary generator with a small trainable BEiT to corrupt the input image instead of using artificial mask tokens, where some patches are randomly selected and replaced with plausible alternatives sampled from the BEiT output distribution. Given this corrupted image, an enhancer network learns to either recover all the original image pixels, or predict whether each visual token is replaced by a generator sample or not. The generator and the enhancer are simultaneously trained and synergistically updated. After pre-training, the enhancer can be used as a high-capacity visual encoder for downstream tasks. CIM is a general and flexible visual pre-training framework that is suitable for various network architectures. For the first time, CIM demonstrates that both ViT and CNN can learn rich visual representations using a unified, non-Siamese framework. Experimental results show that our approach achieves compelling results in vision benchmarks, such as ImageNet classification and ADE20K semantic segmentation. For example, 300-epoch CIM pre-trained vanilla ViT-Base/16 and ResNet-50 obtain 83.3 and 80.6 Top-1 fine-tuning accuracy on ImageNet-1K image classification respectively.", "comment": " Comments: Preprint. Work in progress. Code will be released at https://aka.ms/beit-cim "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.02944", "id": "2202.02944", "pdf": "https://arxiv.org/pdf/2202.02944", "other": "https://arxiv.org/format/2202.02944"}, "title": "Prompt-Guided Injection of Conformation to Pre-trained Protein Model", "author_info": ["Qiang Zhang", "Zeyuan Wang", "Yuqiang Han", "Haoran Yu", "Xurui Jin", "Huajun Chen"], "summary": "Pre-trained protein models (PTPMs) represent a protein with one fixed embedding and thus are not capable for diverse tasks. For example, protein structures can shift, namely protein folding, between several conformations in various biological processes. To enable PTPMs to produce task-aware representations, we propose to learn interpretable, pluggable and extensible protein prompts as a way of injecting task-related knowledge into PTPMs. In this regard, prior PTPM optimization with the masked language modeling task can be interpreted as learning a sequence prompt (Seq prompt) that enables PTPMs to capture the sequential dependency between amino acids. To incorporate conformational knowledge to PTPMs, we propose an interaction-conformation prompt (IC prompt) that is learned through back-propagation with the protein-protein interaction task. As an instantiation, we present a conformation-aware pre-trained protein model that learns both sequence and interaction-conformation prompts in a multi-task setting. We conduct comprehensive experiments on nine protein datasets. Results confirm our expectation that using the sequence prompt does not hurt PTPMs' performance on sequence-related tasks while incorporating the interaction-conformation prompt significantly improves PTPMs' performance on tasks where conformational knowledge counts. We also show the learned prompts can be combined and extended to deal with new complex tasks.", "comment": " Comments: Work in progress "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.02294", "id": "2202.02294", "pdf": "https://arxiv.org/pdf/2202.02294", "other": "https://arxiv.org/format/2202.02294"}, "title": "Pre-Trained Neural Language Models for Automatic Mobile App User Feedback Answer Generation", "author_info": ["Yue Cao", "Fatemeh H. Fard"], "summary": "Studies show that developers' answers to the mobile app users' feedbacks on app stores can increase the apps' star rating. To help app developers generate answers that are related to the users' issues, recent studies develop models to generate the answers automatically. Aims: The app response generation models use deep neural networks and require training data. Pre-Trained neural language Models (PTM) used in Natural Language Processing (NLP) take advantage of the information they learned from a large corpora in an unsupervised manner, and can reduce the amount of required training data. In this paper, we evaluate PTMs to generate replies to the mobile app user feedbacks. Method: We train a Transformer model from scratch and fine-tune two PTMs to evaluate the generated responses, which are compared to RRGEN, a current app response model. We also evaluate the models with different portions of the training data. Results: The results on a large dataset evaluated by automatic metrics show that PTMs obtain lower scores than the baselines. However, our human evaluation confirms that PTMs can generate more relevant and meaningful responses to the posted feedbacks. Moreover, the performance of PTMs has less drop compared to other models when the amount of training data is reduced to 1/3. Conclusion: PTMs are useful in generating responses to app reviews and are more robust models to the amount of training data provided. However, the prediction time is 19X than RRGEN. This study can provide new avenues for research in adapting the PTMs for analyzing mobile app user feedbacks. Index Terms-mobile app user feedback analysis, neural pre-trained language models, automatic answer generation", "comment": " Comments: 6 pages, published in the 2021 ASE RAISE workshop "}, {"arxiv": {"page": "https://arxiv.org/abs/2202.01771", "id": "2202.01771", "pdf": "https://arxiv.org/pdf/2202.01771", "other": "https://arxiv.org/format/2202.01771"}, "title": "Pre-Trained Language Models for Interactive Decision-Making", "author_info": ["Shuang Li", "Xavier Puig", "Yilun Du", "Clinton Wang", "Ekin Akyurek", "Antonio Torralba", "Jacob Andreas", "Igor Mordatch"], "summary": "Language model (LM) pre-training has proven useful for a wide variety of language processing tasks, but can such pre-training be leveraged for more general machine learning problems? We investigate the effectiveness of language modeling to scaffold learning and generalization in autonomous decision-making. We describe a framework for imitation learning in which goals and observations are represented as a sequence of embeddings, and translated into actions using a policy network initialized with a pre-trained transformer LM. We demonstrate that this framework enables effective combinatorial generalization across different environments, such as VirtualHome and BabyAI. In particular, for test tasks involving novel goals or novel scenes, initializing policies with language models improves task completion rates by 43.6% in VirtualHome. We hypothesize and investigate three possible factors underlying the effectiveness of LM-based policy initialization. We find that sequential representations (vs. fixed-dimensional feature vectors) and the LM objective (not just the transformer architecture) are both important for generalization. Surprisingly, however, the format of the policy inputs encoding (e.g. as a natural language string vs. an arbitrary sequential encoding) has little influence. Together, these results suggest that language modeling induces representations that are useful for modeling not just language, but also goals and plans; these representations can aid learning and generalization even outside of language processing.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.01374", "id": "2202.01374", "pdf": "https://arxiv.org/pdf/2202.01374", "other": "https://arxiv.org/format/2202.01374"}, "title": "mSLAM: Massively multilingual joint pre-training for speech and text", "author_info": ["Ankur Bapna", "Colin Cherry", "Yu Zhang", "Ye Jia", "Melvin Johnson", "Yong Cheng", "Simran Khanuja", "Jason Riesa", "Alexis Conneau"], "summary": "We present mSLAM, a multilingual Speech and LAnguage Model that learns cross-lingual cross-modal representations of speech and text by pre-training jointly on large amounts of unlabeled speech and text in multiple languages. mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on character-level text, along with Connectionist Temporal Classification (CTC) losses on paired speech and transcript data, to learn a single model capable of learning from and representing both speech and text signals in a shared representation space. We evaluate mSLAM on several downstream speech understanding tasks and find that joint pre-training with text improves quality on speech translation, speech intent classification and speech language-ID while being competitive on multilingual ASR, when compared against speech-only pre-training. Our speech translation model demonstrates zero-shot text translation without seeing any text translation data, providing evidence for cross-modal alignment of representations. mSLAM also benefits from multi-modal fine-tuning, further improving the quality of speech translation by directly leveraging text translation data during the fine-tuning process. Our empirical analysis highlights several opportunities and challenges arising from large-scale multimodal pre-training, suggesting directions for future research.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2202.01145", "id": "2202.01145", "pdf": "https://arxiv.org/pdf/2202.01145", "other": "https://arxiv.org/format/2202.01145"}, "title": "Relative Position Prediction as Pre-training for Text Encoders", "author_info": ["Rickard Br\u00fcel-Gabrielsson", "Chris Scarvelis"], "summary": "Meaning is defined by the company it keeps. However, company is two-fold: It's based on the identity of tokens and also on their position (topology). We argue that a position-centric perspective is more general and useful. The classic MLM and CLM objectives in NLP are easily phrased as position predictions over the whole vocabulary. Adapting the relative position encoding paradigm in NLP to create relative labels for self-supervised learning, we seek to show superior pre-training judged by performance on downstream tasks.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.12723", "id": "2201.12723", "pdf": "https://arxiv.org/pdf/2201.12723", "other": "https://arxiv.org/format/2201.12723"}, "title": "VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training", "author_info": ["Ziyang Luo", "Yadong Xi", "Rongsheng Zhang", "Jing Ma"], "summary": "Vision-and-language pre-trained models (VLMs) have achieved tremendous success in the cross-modal area, but most of them require a large amount of parallel image-caption data for pre-training. Collating such data is expensive and labor-intensive. In this work, we focus on reducing such need for generative vision-and-language pre-training (G-VLP) by taking advantage of the visual pre-trained model (CLIP-ViT) as encoder and language pre-trained model (GPT2) as decoder. Unfortunately, GPT2 lacks a necessary cross-attention module, which hinders the direct connection of CLIP-ViT and GPT2. To remedy such defects, we conduct extensive experiments to empirically investigate how to design and pre-train our model. Based on our experimental results, we propose a novel G-VLP framework, Visual Conditioned GPT (VC-GPT), and pre-train it with a small-scale image-caption corpus (Visual Genome, only 110k distinct images). Evaluating on the image captioning downstream tasks (MSCOCO and Flickr30k Captioning), VC-GPT achieves either the best or the second-best performance across all evaluation metrics over the previous works which consume around 30 times more distinct images during cross-modal pre-training.", "comment": " Comments: Work in progress "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.12596", "id": "2201.12596", "pdf": "https://arxiv.org/pdf/2201.12596", "other": "https://arxiv.org/format/2201.12596"}, "title": "MVP: Multi-Stage Vision-Language Pre-Training via Multi-Level Semantic Alignment", "author_info": ["Zejun Li", "Zhihao Fan", "Huaixiao Tou", "Zhongyu Wei"], "summary": "In this paper, we propose a Multi-stage Vision-language Pre-training (MVP) framework to learn cross-modality representation via multi-level semantic alignment. We introduce concepts in both modalities to construct two-level semantic representations for language and vision. Based on the multi-level input, we train the cross-modality model in two stages, namely, uni-modal learning and cross-modal learning. The former stage enforces within-modality interactions to learn multi-level semantics for each single modality. The latter stage enforces interactions across modalities via both coarse-grain and fine-grain semantic alignment tasks. Image-text matching and masked language modeling are then used to further optimize the pre-training model. Our model generates the-state-of-the-art results on several vision and language tasks.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.12469", "id": "2201.12469", "pdf": "https://arxiv.org/pdf/2201.12469", "other": "https://arxiv.org/format/2201.12469"}, "title": "ScaLA: Accelerating Adaptation of Pre-Trained Transformer-Based Language Models via Efficient Large-Batch Adversarial Noise", "author_info": ["Minjia Zhang", "Niranjan Uma Naresh", "Yuxiong He"], "summary": "In recent years, large pre-trained Transformer-based language models have led to dramatic improvements in many natural language understanding tasks. To train these models with increasing sizes, many neural network practitioners attempt to increase the batch sizes in order to leverage multiple GPUs to improve training speed. However, increasing the batch size often makes the optimization more difficult, leading to slow convergence or poor generalization that can require orders of magnitude more training time to achieve the same model quality. In this paper, we explore the steepness of the loss landscape of large-batch optimization for adapting pre-trained Transformer-based language models to domain-specific tasks and find that it tends to be highly complex and irregular, posing challenges to generalization on downstream tasks.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.12438", "id": "2201.12438", "pdf": "https://arxiv.org/pdf/2201.12438", "other": "https://arxiv.org/format/2201.12438"}, "title": "Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey", "author_info": ["Prajjwal Bhargava", "Vincent Ng"], "summary": "While commonsense knowledge acquisition and reasoning has traditionally been a core research topic in the knowledge representation and reasoning community, recent years have seen a surge of interest in the natural language processing community in developing pre-trained models and testing their ability to address a variety of newly designed commonsense knowledge reasoning and generation tasks. This paper presents a survey of these tasks, discusses the strengths and weaknesses of state-of-the-art pre-trained models for commonsense reasoning and generation as revealed by these tasks, and reflects on future research directions.", "comment": " Comments: AAAI 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.12086", "id": "2201.12086", "pdf": "https://arxiv.org/pdf/2201.12086", "other": "https://arxiv.org/format/2201.12086"}, "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "author_info": ["Junnan Li", "Dongxu Li", "Caiming Xiong", "Steven Hoi"], "summary": "Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11826", "id": "2201.11826", "pdf": "https://arxiv.org/pdf/2201.11826", "other": "https://arxiv.org/format/2201.11826"}, "title": "Sentiment-Aware Automatic Speech Recognition pre-training for enhanced Speech Emotion Recognition", "author_info": ["Ayoub Ghriss", "Bo Yang", "Viktor Rozgic", "Elizabeth Shriberg", "Chao Wang"], "summary": "We propose a novel multi-task pre-training method for Speech Emotion Recognition (SER). We pre-train SER model simultaneously on Automatic Speech Recognition (ASR) and sentiment classification tasks to make the acoustic ASR model more ``emotion aware''. We generate targets for the sentiment classification using text-to-sentiment model trained on publicly available data. Finally, we fine-tune the acoustic ASR on emotion annotated speech data. We evaluated the proposed approach on the MSP-Podcast dataset, where we achieved the best reported concordance correlation coefficient (CCC) of 0.41 for valence prediction.", "comment": " ACM Class:           I.2.7                "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11692", "id": "2201.11692", "pdf": "https://arxiv.org/pdf/2201.11692", "other": "https://arxiv.org/format/2201.11692"}, "title": "SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders", "author_info": ["Tianshuo Cong", "Xinlei He", "Yang Zhang"], "summary": "Self-supervised learning is an emerging machine learning (ML) paradigm. Compared to supervised learning that leverages high-quality labeled datasets to achieve good performance, self-supervised learning relies on unlabeled datasets to pre-train powerful encoders which can then be treated as feature extractors for various downstream tasks. The huge amount of data and computational resources consumption makes the encoders themselves become a valuable intellectual property of the model owner. Recent research has shown that the ML model's copyright is threatened by model stealing attacks, which aims to train a surrogate model to mimic the behavior of a given model. We empirically show that pre-trained encoders are highly vulnerable to model stealing attacks. However, most of the current efforts of copyright protection algorithms such as fingerprinting and watermarking concentrate on classifiers. Meanwhile, the intrinsic challenges of pre-trained encoder's copyright protection remain largely unstudied. We fill the gap by proposing SSLGuard, the first watermarking algorithm for pre-trained encoders. Given a clean pre-trained encoder, SSLGuard embeds a watermark into it and outputs a watermarked version. The shadow training technique is also applied to preserve the watermark under potential model stealing attacks. Our extensive evaluation shows that SSLGuard is effective in watermark injection and verification, and is robust against model stealing and other watermark removal attacks such as pruning and finetuning.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11664", "id": "2201.11664", "pdf": "https://arxiv.org/pdf/2201.11664", "other": "https://arxiv.org/format/2201.11664"}, "title": "Team Yao at Factify 2022: Utilizing Pre-trained Models and Co-attention Networks for Multi-Modal Fact Verification", "author_info": ["Wei-Yao Wang", "Wen-Chih Peng"], "summary": "In recent years, social media has enabled users to get exposed to a myriad of misinformation and disinformation; thus, misinformation has attracted a great deal of attention in research fields and as a social issue. To address the problem, we propose a framework, Pre-CoFact, composed of two pre-trained models for extracting features from text and images, and multiple co-attention networks for fusing the same modality but different sources and different modalities. Besides, we adopt the ensemble method by using different pre-trained models in Pre-CoFact to achieve better performance. We further illustrate the effectiveness from the ablation study and examine different pre-trained models for comparison. Our team, Yao, won the fifth prize (F1-score: 74.585\\%) in the Factify challenge hosted by De-Factify @ AAAI 2022, which demonstrates that our model achieved competitive performance without using auxiliary tasks or extra information. The source code of our work is publicly available at https://github.com/wywyWang/Multi-Modal-Fact-Verification-2021", "comment": " Comments: Accepted by AAAI 2022 De-Factify Workshop: First Workshop on Multimodal Fact-Checking and Hate Speech Detection "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11327", "id": "2201.11327", "pdf": "https://arxiv.org/pdf/2201.11327", "other": "https://arxiv.org/format/2201.11327"}, "title": "Aspect-Based API Review Classification: How Far Can Pre-Trained Transformer Model Go?", "author_info": ["chengran Yang", "Bowen Xu", "Junaed younus Khan", "Gias Uddin", "Donggyun Han", "Zhou Yang", "David Lo"], "summary": "APIs (Application Programming Interfaces) are reusable software libraries and are building blocks for modern rapid software development. Previous research shows that programmers frequently share and search for reviews of APIs on the mainstream software question and answer (Q&A) platforms like Stack Overflow, which motivates researchers to design tasks and approaches related to process API reviews automatically. Among these tasks, classifying API reviews into different aspects (e.g., performance or security), which is called the aspect-based API review classification, is of great importance. The current state-of-the-art (SOTA) solution to this task is based on the traditional machine learning algorithm. Inspired by the great success achieved by pre-trained models on many software engineering tasks, this study fine-tunes six pre-trained models for the aspect-based API review classification task and compares them with the current SOTA solution on an API review benchmark collected by Uddin et al. The investigated models include four models (BERT, RoBERTa, ALBERT and XLNet) that are pre-trained on natural languages, BERTOverflow that is pre-trained on text corpus extracted from posts on Stack Overflow, and CosSensBERT that is designed for handling imbalanced data. The results show that all the six fine-tuned models outperform the traditional machine learning-based tool. More specifically, the improvement on the F1-score ranges from 21.0% to 30.2%. We also find that BERTOverflow, a model pre-trained on the corpus from Stack Overflow, does not show better performance than BERT. The result also suggests that CosSensBERT also does not exhibit better performance than BERT in terms of F1, but it is still worthy of being considered as it achieves better performance on MCC and AUC.", "comment": " Comments: Accepted by Research Track in SANER 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11227", "id": "2201.11227", "pdf": "https://arxiv.org/pdf/2201.11227", "other": "https://arxiv.org/format/2201.11227"}, "title": "Synchromesh: Reliable code generation from pre-trained language models", "author_info": ["Gabriel Poesia", "Oleksandr Polozov", "Vu Le", "Ashish Tiwari", "Gustavo Soares", "Christopher Meek", "Sumit Gulwani"], "summary": "Large pre-trained language models have been used to generate code,providing a flexible interface for synthesizing programs from natural language specifications. However, they often violate syntactic and semantic rules of their output language, limiting their practical usability. In this paper, we propose Synchromesh: a framework for substantially improving the reliability of pre-trained models for code generation. Synchromesh comprises two components. First, it retrieves few-shot examples from a training bank using Target Similarity Tuning (TST), a novel method for semantic example selection. TST learns to recognize utterances that describe similar target programs despite differences in surface natural language features. Then, Synchromesh feeds the examples to a pre-trained language model and samples programs using Constrained Semantic Decoding (CSD): a general framework for constraining the output to a set of valid programs in the target language. CSD leverages constraints on partial outputs to sample complete correct programs, and needs neither re-training nor fine-tuning of the language model. We evaluate our methods by synthesizing code from natural language descriptions using GPT-3 and Codex in three real-world languages: SQL queries, Vega-Lite visualizations and SMCalFlow programs. These domains showcase rich constraints that CSD is able to enforce, including syntax, scope, typing rules, and contextual logic. We observe substantial complementary gains from CSD and TST in prediction accuracy and in effectively preventing run-time errors.", "comment": " Comments: 10 pages, 9 additional pages of Appendix "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.11218", "id": "2201.11218", "pdf": "https://arxiv.org/pdf/2201.11218", "other": "https://arxiv.org/format/2201.11218"}, "title": "DNNFuser: Generative Pre-Trained Transformer as a Generalized Mapper for Layer Fusion in DNN Accelerators", "author_info": ["Sheng-Chun Kao", "Xiaoyu Huang", "Tushar Krishna"], "summary": "Dataflow/mapping decides the compute and energy efficiency of DNN accelerators. Many mappers have been proposed to tackle the intra-layer map-space. However, mappers for inter-layer map-space (aka layer-fusion map-space), have been rarely discussed. In this work, we propose a mapper, DNNFuser, specifically focusing on this layer-fusion map-space. While existing SOTA DNN mapping explorations rely on search-based mappers, this is the first work, to the best of our knowledge, to propose a one-shot inference-based mapper. We leverage a famous language model GPT as our DNN architecture to learn layer-fusion optimization as a sequence modeling problem. Further, the trained DNNFuser can generalize its knowledge and infer new solutions for unseen conditions. Within one inference pass, DNNFuser can infer solutions with compatible performance to the ones found by a highly optimized search-based mapper while being 66x-127x faster.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.10716", "id": "2201.10716", "pdf": "https://arxiv.org/pdf/2201.10716", "other": "https://arxiv.org/format/2201.10716"}, "title": "Neural Grapheme-to-Phoneme Conversion with Pre-trained Grapheme Models", "author_info": ["Lu Dong", "Zhi-Qiang Guo", "Chao-Hong Tan", "Ya-Jun Hu", "Yuan Jiang", "Zhen-Hua Ling"], "summary": "Neural network models have achieved state-of-the-art performance on grapheme-to-phoneme (G2P) conversion. However, their performance relies on large-scale pronunciation dictionaries, which may not be available for a lot of languages. Inspired by the success of the pre-trained language model BERT, this paper proposes a pre-trained grapheme model called grapheme BERT (GBERT), which is built by self-supervised training on a large, language-specific word list with only grapheme information. Furthermore, two approaches are developed to incorporate GBERT into the state-of-the-art Transformer-based G2P model, i.e., fine-tuning GBERT or fusing GBERT into the Transformer model by attention. Experimental results on the Dutch, Serbo-Croatian, Bulgarian and Korean datasets of the SIGMORPHON 2021 G2P task confirm the effectiveness of our GBERT-based G2P models under both medium-resource and low-resource data conditions.", "comment": " Comments: This paper is accepted by ICASSP2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.10707", "id": "2201.10707", "pdf": "https://arxiv.org/pdf/2201.10707", "other": "https://arxiv.org/format/2201.10707"}, "title": "A Unified Strategy for Multilingual Grammatical Error Correction with Pre-trained Cross-Lingual Language Model", "author_info": ["Xin Sun", "Tao Ge", "Shuming Ma", "Jingjing Li", "Furu Wei", "Houfeng Wang"], "summary": "Synthetic data construction of Grammatical Error Correction (GEC) for non-English languages relies heavily on human-designed and language-specific rules, which produce limited error-corrected patterns. In this paper, we propose a generic and language-independent strategy for multilingual GEC, which can train a GEC system effectively for a new non-English language with only two easy-to-access resources: 1) a pretrained cross-lingual language model (PXLM) and 2) parallel translation data between English and the language. Our approach creates diverse parallel GEC data without any language-specific operations by taking the non-autoregressive translation generated by PXLM and the gold translation as error-corrected sentence pairs. Then, we reuse PXLM to initialize the GEC model and pretrain it with the synthetic data generated by itself, which yields further improvement. We evaluate our approach on three public benchmarks of GEC in different languages. It achieves the state-of-the-art results on the NLPCC 2018 Task 2 dataset (Chinese) and obtains competitive performance on Falko-Merlin (German) and RULEC-GEC (Russian). Further analysis demonstrates that our data construction method is complementary to rule-based approaches.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.10207", "id": "2201.10207", "pdf": "https://arxiv.org/pdf/2201.10207", "other": "https://arxiv.org/format/2201.10207"}, "title": "SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training", "author_info": ["Wenyong Huang", "Zhenhe Zhang", "Yu Ting Yeung", "Xin Jiang", "Qun Liu"], "summary": "We introduce a new approach for speech pre-training named SPIRAL which works by learning denoising representation of perturbed data in a teacher-student framework. Specifically, given a speech utterance, we first feed the utterance to a teacher network to obtain corresponding representation. Then the same utterance is perturbed and fed to a student network. The student network is trained to output representation resembling that of the teacher. At the same time, the teacher network is updated as moving average of student's weights over training steps. In order to prevent representation collapse, we apply an in-utterance contrastive loss as pre-training objective and impose position randomization on the input to the teacher. SPIRAL achieves competitive or better results compared to state-of-the-art speech pre-training method wav2vec 2.0, with significant reduction of training cost (80% for Base model, 65% for Large model). Furthermore, we address the problem of noise-robustness that is critical to real-world speech applications. We propose multi-condition pre-training by perturbing the student's input with various types of additive noise. We demonstrate that multi-condition pre-trained SPIRAL models are more robust to noisy speech (9.0% - 13.3% relative word error rate reduction on real noisy test data), compared to applying multi-condition training solely in the fine-tuning stage. The code will be released after publication.", "comment": " Comments: ICLR 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.10182", "id": "2201.10182", "pdf": "https://arxiv.org/pdf/2201.10182", "other": "https://arxiv.org/format/2201.10182"}, "title": "Pre-Trained Language Transformers are Universal Image Classifiers", "author_info": ["Rahul Goel", "Modar Sulaiman", "Kimia Noorbakhsh", "Mahdi Sharifi", "Rajesh Sharma", "Pooyan Jamshidi", "Kallol Roy"], "summary": "Facial images disclose many hidden personal traits such as age, gender, race, health, emotion, and psychology. Understanding these traits will help to classify the people in different attributes. In this paper, we have presented a novel method for classifying images using a pretrained transformer model. We apply the pretrained transformer for the binary classification of facial images in criminal and non-criminal classes. The pretrained transformer of GPT-2 is trained to generate text and then fine-tuned to classify facial images. During the finetuning process with images, most of the layers of GT-2 are frozen during backpropagation and the model is frozen pretrained transformer (FPT). The FPT acts as a universal image classifier, and this paper shows the application of FPT on facial images. We also use our FPT on encrypted images for classification. Our FPT shows high accuracy on both raw facial images and encrypted images. We hypothesize the meta-learning capacity FPT gained because of its large size and trained on a large size with theory and experiments. The GPT-2 trained to generate a single word token at a time, through the autoregressive process, forced to heavy-tail distribution. Then the FPT uses the heavy-tail property as its meta-learning capacity for classifying images. Our work shows one way to avoid bias during the machine classification of images.The FPT encodes worldly knowledge because of the pretraining of one text, which it uses during the classification. The statistical error of classification is reduced because of the added context gained from the text.Our paper shows the ethical dimension of using encrypted data for classification.Criminal images are sensitive to share across the boundary but encrypted largely evades ethical concern.FPT showing good classification accuracy on encrypted images shows promise for further research on privacy-preserving machine learning.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.10113", "id": "2201.10113", "pdf": "https://arxiv.org/pdf/2201.10113"}, "title": "Two heads are better than one: Enhancing medical representations by pre-training over structured and unstructured electronic health records", "author_info": ["Sicen Liu", "Xiaolong Wang", "Yongshuai Hou", "Ge Li", "Hui Wang", "Hui Xu", "Yang Xiang", "Buzhou Tang"], "summary": "The massive amount of electronic health records (EHRs) has created enormous potentials for improving healthcare, among which structured (coded) data and unstructured (clinical narratives) data are two important textual modalities. They do not exist in isolation and can complement each other in many real-life clinical scenarios. Most existing studies in medical informatics, however, either only focus on a particular modality or apply simple and na\u00efve ways to concatenate data from different modalities, which ignores the interactions between them. To address these issues, we proposed a Unified Medical Multimodal Pre-trained Language Model, named UMM-PLM, to jointly learn enhanced representations from both structured and unstructured EHRs. In UMM-PLM, an unimodal information extraction module is used to learn representative characteristics from each data modality respectively, where two Transformer-based components are adopted. A cross-modal module is then introduced to model the interactions between the two modalities. We pre-trained the model on a large EHR dataset containing both structured data and unstructured data, and verified the effectiveness of the model on three downstream clinical tasks, i.e., medication recommendation, 30-day readmission, and ICD coding, through extensive experiments. The results demonstrate the power of UMM-PLM compared with benchmark methods and state-of-the-art baselines. Further analyses show that UMM-PLM can effectively integrate multimodal textual information and potentially provide more comprehensive interpretations for clinical decision-making.", "comment": " Comments: 30 pages, 5 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.10103", "id": "2201.10103", "pdf": "https://arxiv.org/pdf/2201.10103", "other": "https://arxiv.org/format/2201.10103"}, "title": "Improving non-autoregressive end-to-end speech recognition with pre-trained acoustic and language models", "author_info": ["Keqi Deng", "Zehui Yang", "Shinji Watanabe", "Yosuke Higuchi", "Gaofeng Cheng", "Pengyuan Zhang"], "summary": "While Transformers have achieved promising results in end-to-end (E2E) automatic speech recognition (ASR), their autoregressive (AR) structure becomes a bottleneck for speeding up the decoding process. For real-world deployment, ASR systems are desired to be highly accurate while achieving fast inference. Non-autoregressive (NAR) models have become a popular alternative due to their fast inference speed, but they still fall behind AR systems in recognition accuracy. To fulfill the two demands, in this paper, we propose a NAR CTC/attention model utilizing both pre-trained acoustic and language models: wav2vec2.0 and BERT. To bridge the modality gap between speech and text representations obtained from the pre-trained models, we design a novel modality conversion mechanism, which is more suitable for logographic languages. During inference, we employ a CTC branch to generate a target length, which enables the BERT to predict tokens in parallel. We also design a cache-based CTC/attention joint decoding method to improve the recognition accuracy while keeping the decoding speed fast. Experimental results show that the proposed NAR model greatly outperforms our strong wav2vec2.0 CTC baseline (15.1% relative CER reduction on AISHELL-1). The proposed NAR model significantly surpasses previous NAR systems on the AISHELL-1 benchmark and shows a potential for English tasks.", "comment": " Comments: Accepted by ICASSP2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.10005", "id": "2201.10005", "pdf": "https://arxiv.org/pdf/2201.10005", "other": "https://arxiv.org/format/2201.10005"}, "title": "Text and Code Embeddings by Contrastive Pre-Training", "author_info": ["Arvind Neelakantan", "Tao Xu", "Raul Puri", "Alec Radford", "Jesse Michael Han", "Jerry Tworek", "Qiming Yuan", "Nikolas Tezak", "Jong Wook Kim", "Chris Hallacy", "Johannes Heidecke", "Pranav Shyam", "Boris Power", "Tyna Eloundou Nekoul", "Girish Sastry", "Gretchen Krueger", "David Schnurr", "Felipe Petroski Such", "Kenny Hsu", "Madeleine Thompson", "Tabarak Khan", "Toki Sherbakov", "Joanne Jang", "Peter Welinder", "Lilian Weng"], "summary": "Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4% and 1.8% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and 10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8% relative improvement over prior best work on code search.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.09745", "id": "2201.09745", "pdf": "https://arxiv.org/pdf/2201.09745", "other": "https://arxiv.org/format/2201.09745"}, "title": "Table Pre-training: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks", "author_info": ["Haoyu Dong", "Zhoujun Cheng", "Xinyi He", "Mengyu Zhou", "Anda Zhou", "Fan Zhou", "Ao Liu", "Shi Han", "Dongmei Zhang"], "summary": "Since a vast number of tables can be easily collected from web pages, spreadsheets, PDFs, and various other document types, a flurry of table pre-training frameworks have been proposed following the success of text and images, and they have achieved new state-of-the-arts on various tasks such as table question answering, table type recognition, column relation classification, table search, formula prediction, etc. To fully use the supervision signals in unlabeled tables, a variety of pre-training objectives have been designed and evaluated, for example, denoising cell values, predicting numerical relationships, and implicitly executing SQLs. And to best leverage the characteristics of (semi-)structured tables, various tabular language models, particularly with specially-designed attention mechanisms, have been explored. Since tables usually appear and interact with free-form text, table pre-training usually takes the form of table-text joint pre-training, which attracts significant research interests from multiple domains. This survey aims to provide a comprehensive review of different model designs, pre-training objectives, and downstream tasks for table pre-training, and we further share our thoughts and vision on existing challenges and future opportunities.", "comment": " Comments: Work in progress "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.09427", "id": "2201.09427", "pdf": "https://arxiv.org/pdf/2201.09427", "other": "https://arxiv.org/format/2201.09427"}, "title": "Polyphone disambiguation and accent prediction using pre-trained language models in Japanese TTS front-end", "author_info": ["Rem Hida", "Masaki Hamada", "Chie Kamada", "Emiru Tsunoo", "Toshiyuki Sekiya", "Toshiyuki Kumakura"], "summary": "Although end-to-end text-to-speech (TTS) models can generate natural speech, challenges still remain when it comes to estimating sentence-level phonetic and prosodic information from raw text in Japanese TTS systems. In this paper, we propose a method for polyphone disambiguation (PD) and accent prediction (AP). The proposed method incorporates explicit features extracted from morphological analysis and implicit features extracted from pre-trained language models (PLMs). We use BERT and Flair embeddings as implicit features and examine how to combine them with explicit features. Our objective evaluation results showed that the proposed method improved the accuracy by 5.7 points in PD and 6.0 points in AP. Moreover, the perceptual listening test results confirmed that a TTS system employing our proposed model as a front-end achieved a mean opinion score close to that of synthesized speech with ground-truth pronunciation and accent in terms of naturalness.", "comment": " Comments: 5 pages, 2 figures. Accepted to ICASSP2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.09165", "id": "2201.09165", "pdf": "https://arxiv.org/pdf/2201.09165", "other": "https://arxiv.org/format/2201.09165"}, "title": "A Pre-trained Audio-Visual Transformer for Emotion Recognition", "author_info": ["Minh Tran", "Mohammad Soleymani"], "summary": "In this paper, we introduce a pretrained audio-visual Transformer trained on more than 500k utterances from nearly 4000 celebrities from the VoxCeleb2 dataset for human behavior understanding. The model aims to capture and extract useful information from the interactions between human facial and auditory behaviors, with application in emotion recognition. We evaluate the model performance on two datasets, namely CREMAD-D (emotion classification) and MSP-IMPROV (continuous emotion regression). Experimental results show that fine-tuning the pre-trained model helps improving emotion classification accuracy by 5-7% and Concordance Correlation Coefficients (CCC) in continuous emotion recognition by 0.03-0.09 compared to the same model trained from scratch. We also demonstrate the robustness of finetuning the pre-trained model in a low-resource setting. With only 10% of the original training set provided, fine-tuning the pre-trained model can lead to at least 10% better emotion recognition accuracy and a CCC score improvement by at least 0.1 for continuous emotion recognition.", "comment": " Comments: Accepted by IEEE ICASSP 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.08930", "id": "2201.08930", "pdf": "https://arxiv.org/pdf/2201.08930", "other": "https://arxiv.org/format/2201.08930"}, "title": "A Noise-Robust Self-supervised Pre-training Model Based Speech Representation Learning for Automatic Speech Recognition", "author_info": ["Qiu-Shi Zhu", "Jie Zhang", "Zi-Qiang Zhang", "Ming-Hui Wu", "Xin Fang", "Li-Rong Dai"], "summary": "Wav2vec2.0 is a popular self-supervised pre-training framework for learning speech representations in the context of automatic speech recognition (ASR). It was shown that wav2vec2.0 has a good robustness against the domain shift, while the noise robustness is still unclear. In this work, we therefore first analyze the noise robustness of wav2vec2.0 via experiments. We observe that wav2vec2.0 pre-trained on noisy data can obtain good representations and thus improve the ASR performance on the noisy test set, which however brings a performance degradation on the clean test set. To avoid this issue, in this work we propose an enhanced wav2vec2.0 model. Specifically, the noisy speech and the corresponding clean version are fed into the same feature encoder, where the clean speech provides training targets for the model. Experimental results reveal that the proposed method can not only improve the ASR performance on the noisy test set which surpasses the original wav2vec2.0, but also ensure a tiny performance decrease on the clean test set. In addition, the effectiveness of the proposed method is demonstrated under different types of noise conditions.", "comment": " Comments: Accepted by ICASSP 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.08698", "id": "2201.08698", "pdf": "https://arxiv.org/pdf/2201.08698", "other": "https://arxiv.org/format/2201.08698"}, "title": "Natural Attack for Pre-trained Models of Code", "author_info": ["Zhou Yang", "Jieke Shi", "Junda He", "David Lo"], "summary": "Pre-trained models of code have achieved success in many important software engineering tasks. However, these powerful models are vulnerable to adversarial attacks that slightly perturb model inputs to make a victim model produce wrong outputs. Current works mainly attack models of code with examples that preserve operational program semantics but ignore a fundamental requirement for adversarial example generation: perturbations should be natural to human judges, which we refer to as naturalness requirement.", "comment": " Comments: Accepted to the Technical Track of ICSE 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.08531", "id": "2201.08531", "pdf": "https://arxiv.org/pdf/2201.08531", "other": "https://arxiv.org/format/2201.08531"}, "title": "Black-box Prompt Learning for Pre-trained Language Models", "author_info": ["Shizhe Diao", "Xuechun Li", "Yong Lin", "Zhichao Huang", "Tong Zhang"], "summary": "Domain-specific fine-tuning strategies for large pre-trained models received vast attention in recent years. In previously studied settings, the model architectures and parameters are tunable or at least visible, which we refer to as white-box settings. This work considers a new scenario, where we do not have access to a pre-trained model, except for its outputs given inputs, and we call this problem black-box fine-tuning. To illustrate our approach, we first introduce the black-box setting formally on text classification, where the pre-trained model is not only frozen but also invisible. We then propose our solution black-box prompt, a new technique in the prompt-learning family, which can leverage the knowledge learned by pre-trained models from the pre-training corpus. Our experiments demonstrate that the proposed method achieved the state-of-the-art performance on eight datasets. Further analyses on different human-designed objectives, prompt lengths, and intuitive explanations demonstrate the robustness and flexibility of our method.", "comment": " Comments: 10 pages, 5 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.08371", "id": "2201.08371", "pdf": "https://arxiv.org/pdf/2201.08371", "other": "https://arxiv.org/format/2201.08371"}, "title": "Revisiting Weakly Supervised Pre-Training of Visual Perception Models", "author_info": ["Mannat Singh", "Laura Gustafson", "Aaron Adcock", "Vinicius de Freitas Reis", "Bugra Gedik", "Raj Prateek Kosaraju", "Dhruv Mahajan", "Ross Girshick", "Piotr Doll\u00e1r", "Laurens van der Maaten"], "summary": "Model pre-training is a cornerstone of modern visual recognition systems. Although fully supervised pre-training on datasets like ImageNet is still the de-facto standard, recent studies suggest that large-scale weakly supervised pre-training can outperform fully supervised approaches. This paper revisits weakly-supervised pre-training of models using hashtag supervision with modern versions of residual networks and the largest-ever dataset of images and corresponding hashtags. We study the performance of the resulting models in various transfer-learning settings including zero-shot transfer. We also compare our models with those obtained via large-scale self-supervised learning. We find our weakly-supervised models to be very competitive across all settings, and find they substantially outperform their self-supervised counterparts. We also include an investigation into whether our models learned potentially troubling associations or stereotypes. Overall, our results provide a compelling argument for the use of weakly supervised learning in the development of visual recognition systems. Our models, Supervised Weakly through hashtAGs (SWAG), are available publicly.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.08217", "id": "2201.08217", "pdf": "https://arxiv.org/pdf/2201.08217", "other": "https://arxiv.org/format/2201.08217"}, "title": "Watermarking Pre-trained Encoders in Contrastive Learning", "author_info": ["Yutong Wu", "Han Qiu", "Tianwei Zhang", "Jiwei L", "Meikang Qiu"], "summary": "Contrastive learning has become a popular technique to pre-train image encoders, which could be used to build various downstream classification models in an efficient way. This process requires a large amount of data and computation resources. Hence, the pre-trained encoders are an important intellectual property that needs to be carefully protected. It is challenging to migrate existing watermarking techniques from the classification tasks to the contrastive learning scenario, as the owner of the encoder lacks the knowledge of the downstream tasks which will be developed from the encoder in the future. We propose the \\textit{first} watermarking methodology for the pre-trained encoders. We introduce a task-agnostic loss function to effectively embed into the encoder a backdoor as the watermark. This backdoor can still exist in any downstream models transferred from the encoder. Extensive evaluations over different contrastive learning algorithms, datasets, and downstream tasks indicate our watermarks exhibit high effectiveness and robustness against different adversarial operations.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.08081", "id": "2201.08081", "pdf": "https://arxiv.org/pdf/2201.08081", "other": "https://arxiv.org/format/2201.08081"}, "title": "LEMON: Language-Based Environment Manipulation via Execution-Guided Pre-training", "author_info": ["Qi Shi", "Qian Liu", "Bei Chen", "Yu Zhang", "Ting Liu", "Jian-Guang Lou"], "summary": "Language-based environment manipulation requires agents to manipulate the environment following natural language instructions, which is challenging due to the huge space of the environments. To address this challenge, various approaches have been proposed in recent work. Although these approaches work well for their intended environments, they are difficult to generalize across environments. In this work, we propose LEMON, a general framework for language-based environment manipulation tasks. Specifically, we first propose a unified approach to deal with various environments using the same generative language model. Then we propose an execution-guided pre-training strategy to inject prior knowledge of environments to the language model with a pure synthetic pre-training corpus. Experimental results on tasks including Alchemy, Scene, Tangrams and ProPara demonstrate the effectiveness of LEMON: it achieves new state-of-the-art results on Alchemy, Scene and ProPara, and the execution-guided pre-training strategy brings remarkable improvements on all experimental tasks.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.08070", "id": "2201.08070", "pdf": "https://arxiv.org/pdf/2201.08070", "other": "https://arxiv.org/format/2201.08070"}, "title": "Linguistically-driven Multi-task Pre-training for Low-resource Neural Machine Translation", "author_info": ["Zhuoyuan Mao", "Chenhui Chu", "Sadao Kurohashi"], "summary": "In the present study, we propose novel sequence-to-sequence pre-training objectives for low-resource machine translation (NMT): Japanese-specific sequence to sequence (JASS) for language pairs involving Japanese as the source or target language, and English-specific sequence to sequence (ENSS) for language pairs involving English. JASS focuses on masking and reordering Japanese linguistic units known as bunsetsu, whereas ENSS is proposed based on phrase structure masking and reordering tasks. Experiments on ASPEC Japanese--English & Japanese--Chinese, Wikipedia Japanese--Chinese, News English--Korean corpora demonstrate that JASS and ENSS outperform MASS and other existing language-agnostic pre-training methods by up to +2.9 BLEU points for the Japanese--English tasks, up to +7.0 BLEU points for the Japanese--Chinese tasks and up to +1.3 BLEU points for English--Korean tasks. Empirical analysis, which focuses on the relationship between individual parts in JASS and ENSS, reveals the complementary nature of the subtasks of JASS and ENSS. Adequacy evaluation using LASER, human evaluation, and case studies reveals that our proposed methods significantly outperform pre-training methods without injected linguistic knowledge and they have a larger positive impact on the adequacy as compared to the fluency. We release codes here: https://github.com/Mao-KU/JASS/tree/master/linguistically-driven-pretraining.", "comment": " Journal ref:         TALLIP Volume 21, Issue 4, July 2022       "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.07894", "id": "2201.07894", "pdf": "https://arxiv.org/pdf/2201.07894", "other": "https://arxiv.org/format/2201.07894"}, "title": "Enhanced Performance of Pre-Trained Networks by Matched Augmentation Distributions", "author_info": ["Touqeer Ahmad", "Mohsen Jafarzadeh", "Akshay Raj Dhamija", "Ryan Rabinowitz", "Steve Cruz", "Chunchun Li", "Terrance E. Boult"], "summary": "There exists a distribution discrepancy between training and testing, in the way images are fed to modern CNNs. Recent work tried to bridge this gap either by fine-tuning or re-training the network at different resolutions. However re-training a network is rarely cheap and not always viable. To this end, we propose a simple solution to address the train-test distributional shift and enhance the performance of pre-trained models -- which commonly ship as a package with deep learning platforms \\eg, PyTorch. Specifically, we demonstrate that running inference on the center crop of an image is not always the best as important discriminatory information may be cropped-off. Instead we propose to combine results for multiple random crops for a test image. This not only matches the train time augmentation but also provides the full coverage of the input image. We explore combining representation of random crops through averaging at different levels \\ie, deep feature level, logit level, and softmax level. We demonstrate that, for various families of modern deep networks, such averaging results in better validation accuracy compared to using a single central crop per image. The softmax averaging results in the best performance for various pre-trained networks without requiring any re-training or fine-tuning whatsoever. On modern GPUs with batch processing, the paper's approach to inference of pre-trained networks, is essentially free as all images in a batch can all be processed at once.", "comment": " MSC Class:           68T45                              ACM Class:           I.4.8                "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.07614", "id": "2201.07614", "pdf": "https://arxiv.org/pdf/2201.07614", "other": "https://arxiv.org/format/2201.07614"}, "title": "Uncovering More Shallow Heuristics: Probing the Natural Language Inference Capacities of Transformer-Based Pre-Trained Language Models Using Syllogistic Patterns", "author_info": ["Reto Gubelmann", "Siegfried Handschuh"], "summary": "In this article, we explore the shallow heuristics used by transformer-based pre-trained language models (PLMs) that are fine-tuned for natural language inference (NLI). To do so, we construct or own dataset based on syllogistic, and we evaluate a number of models' performance on our dataset. We find evidence that the models rely heavily on certain shallow heuristics, picking up on symmetries and asymmetries between premise and hypothesis. We suggest that the lack of generalization observable in our study, which is becoming a topic of lively debate in the field, means that the PLMs are currently not learning NLI, but rather spurious heuristics.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.06857", "id": "2201.06857", "pdf": "https://arxiv.org/pdf/2201.06857", "other": "https://arxiv.org/format/2201.06857"}, "title": "RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training", "author_info": ["Luya Wang", "Feng Liang", "Yangguang Li", "Honggang Zhang", "Wanli Ouyang", "Jing Shao"], "summary": "Recently, self-supervised vision transformers have attracted unprecedented attention for their impressive representation learning ability. However, the dominant method, contrastive learning, mainly relies on an instance discrimination pretext task, which learns a global understanding of the image. This paper incorporates local feature learning into self-supervised vision transformers via Reconstructive Pre-training (RePre). Our RePre extends contrastive frameworks by adding a branch for reconstructing raw image pixels in parallel with the existing contrastive objective. RePre is equipped with a lightweight convolution-based decoder that fuses the multi-hierarchy features from the transformer encoder. The multi-hierarchy features provide rich supervisions from low to high semantic information, which are crucial for our RePre. Our RePre brings decent improvements on various contrastive frameworks with different vision transformer architectures. Transfer performance in downstream tasks outperforms supervised pre-training and state-of-the-art (SOTA) self-supervised counterparts.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.06850", "id": "2201.06850", "pdf": "https://arxiv.org/pdf/2201.06850", "other": "https://arxiv.org/format/2201.06850"}, "title": "Using Pre-Trained Models to Boost Code Review Automation", "author_info": ["Rosalia Tufano", "Simone Masiero", "Antonio Mastropaolo", "Luca Pascarella", "Denys Poshyvanyk", "Gabriele Bavota"], "summary": "Code review is a practice widely adopted in open source and industrial projects. Given the non-negligible cost of such a process, researchers started investigating the possibility of automating specific code review tasks. We recently proposed Deep Learning (DL) models targeting the automation of two tasks: the first model takes as input a code submitted for review and implements in it changes likely to be recommended by a reviewer; the second takes as input the submitted code and a reviewer comment posted in natural language and automatically implements the change required by the reviewer. While the preliminary results we achieved are encouraging, both models had been tested in rather simple code review scenarios, substantially simplifying the targeted problem. This was also due to the choices we made when designing both the technique and the experiments. In this paper, we build on top of that work by demonstrating that a pre-trained Text-To-Text Transfer Transformer (T5) model can outperform previous DL models for automating code review tasks. Also, we conducted our experiments on a larger and more realistic (and challenging) dataset of code review activities.", "comment": " Comments: Accepted for publication at ICSE 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.05889", "id": "2201.05889", "pdf": "https://arxiv.org/pdf/2201.05889", "other": "https://arxiv.org/format/2201.05889"}, "title": "StolenEncoder: Stealing Pre-trained Encoders", "author_info": ["Yupei Liu", "Jinyuan Jia", "Hongbin Liu", "Neil Zhenqiang Gong"], "summary": "Pre-trained encoders are general-purpose feature extractors that can be used for many downstream tasks. Recent progress in self-supervised learning can pre-train highly effective encoders using a large volume of unlabeled data, leading to the emerging encoder as a service (EaaS). A pre-trained encoder may be deemed confidential because its training often requires lots of data and computation resources as well as its public release may facilitate misuse of AI, e.g., for deepfakes generation. In this paper, we propose the first attack called StolenEncoder to steal pre-trained image encoders. We evaluate StolenEncoder on multiple target encoders pre-trained by ourselves and three real-world target encoders including the ImageNet encoder pre-trained by Google, CLIP encoder pre-trained by OpenAI, and Clarifai's General Embedding encoder deployed as a paid EaaS. Our results show that the encoders stolen by StolenEncoder have similar functionality with the target encoders. In particular, the downstream classifiers built upon a target encoder and a stolen encoder have similar accuracy. Moreover, stealing a target encoder using StolenEncoder requires much less data and computation resources than pre-training it from scratch. We also explore three defenses that perturb feature vectors produced by a target encoder. Our evaluation shows that these defenses are not enough to mitigate StolenEncoder.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.05613", "id": "2201.05613", "pdf": "https://arxiv.org/pdf/2201.05613", "other": "https://arxiv.org/format/2201.05613"}, "title": "The Dark Side of the Language: Pre-trained Transformers in the DarkNet", "author_info": ["Leonardo Ranaldi", "Aria Nourbakhsh", "Arianna Patrizi", "Elena Sofia Ruzzetti", "Dario Onorati", "Francesca Fallucchi", "Fabio Massimo Zanzotto"], "summary": "Pre-trained Transformers are challenging human performances in many natural language processing tasks. The gigantic datasets used for pre-training seem to be the key for their success on existing tasks. In this paper, we explore how a range of pre-trained natural language understanding models perform on truly novel and unexplored data, provided by classification tasks over a DarkNet corpus. Surprisingly, results show that syntactic and lexical neural networks largely outperform pre-trained Transformers. This seems to suggest that pre-trained Transformers have serious difficulties in adapting to radically novel texts.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.05337", "id": "2201.05337", "pdf": "https://arxiv.org/pdf/2201.05337", "other": "https://arxiv.org/format/2201.05337"}, "title": "A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models", "author_info": ["Hanqing Zhang", "Haolin Song", "Shaoyu Li", "Ming Zhou", "Dawei Song"], "summary": "Controllable Text Generation (CTG) is emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that are more natural and better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the lower level of interpretability of deep neural networks, the controllability of these methods need to be guaranteed. To this end, controllable text generation using transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the recent 3-4 years, targeting different CTG tasks which may require different types of controlled constraints. In this paper, we present a systematic critical review on the common tasks, main approaches and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey paper to summarize CTG techniques from the perspective of PLMs. We hope it can help researchers in related fields to quickly track the academic frontier, providing them with a landscape of the area and a roadmap for future research.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.04843", "id": "2201.04843", "pdf": "https://arxiv.org/pdf/2201.04843", "other": "https://arxiv.org/format/2201.04843"}, "title": "LP-BERT: Multi-task Pre-training Knowledge Graph BERT for Link Prediction", "author_info": ["Da Li", "Ming Yi", "Yukai He"], "summary": "Link prediction plays an significant role in knowledge graph, which is an important resource for many artificial intelligence tasks, but it is often limited by incompleteness. In this paper, we propose knowledge graph BERT for link prediction, named LP-BERT, which contains two training stages: multi-task pre-training and knowledge graph fine-tuning. The pre-training strategy not only uses Mask Language Model (MLM) to learn the knowledge of context corpus, but also introduces Mask Entity Model (MEM) and Mask Relation Model (MRM), which can learn the relationship information from triples by predicting semantic based entity and relation elements. Structured triple relation information can be transformed into unstructured semantic information, which can be integrated into the pre-training model together with context corpus information. In the fine-tuning phase, inspired by contrastive learning, we carry out a triple-style negative sampling in sample batch, which greatly increased the proportion of negative sampling while keeping the training time almost unchanged. Furthermore, we propose a data augmentation method based on the inverse relationship of triples to further increase the sample diversity. We achieve state-of-the-art results on WN18RR and UMLS datasets, especially the Hits@10 indicator improved by 5\\% from the previous state-of-the-art result on WN18RR dataset.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.04809", "id": "2201.04809", "pdf": "https://arxiv.org/pdf/2201.04809", "other": "https://arxiv.org/format/2201.04809"}, "title": "Conditional Variational Autoencoder with Balanced Pre-training for Generative Adversarial Networks", "author_info": ["Yuchong Yao", "Xiaohui Wangr", "Yuanbang Ma", "Han Fang", "Jiaying Wei", "Liyuan Chen", "Ali Anaissi", "Ali Braytee"], "summary": "Class imbalance occurs in many real-world applications, including image classification, where the number of images in each class differs significantly. With imbalanced data, the generative adversarial networks (GANs) leans to majority class samples. The two recent methods, Balancing GAN (BAGAN) and improved BAGAN (BAGAN-GP), are proposed as an augmentation tool to handle this problem and restore the balance to the data. The former pre-trains the autoencoder weights in an unsupervised manner. However, it is unstable when the images from different categories have similar features. The latter is improved based on BAGAN by facilitating supervised autoencoder training, but the pre-training is biased towards the majority classes. In this work, we propose a novel Conditional Variational Autoencoder with Balanced Pre-training for Generative Adversarial Networks (CAPGAN) as an augmentation tool to generate realistic synthetic images. In particular, we utilize a conditional convolutional variational autoencoder with supervised and balanced pre-training for the GAN initialization and training with gradient penalty. Our proposed method presents a superior performance of other state-of-the-art methods on the highly imbalanced version of MNIST, Fashion-MNIST, CIFAR-10, and two medical imaging datasets. Our method can synthesize high-quality minority samples in terms of Fr\u00e9chet inception distance, structural similarity index measure and perceptual quality.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.04026", "id": "2201.04026", "pdf": "https://arxiv.org/pdf/2201.04026", "other": "https://arxiv.org/format/2201.04026"}, "title": "Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular Vision-Language Pre-training", "author_info": ["Yehao Li", "Jiahao Fan", "Yingwei Pan", "Ting Yao", "Weiyao Lin", "Tao Mei"], "summary": "Vision-language pre-training has been an emerging and fast-developing research topic, which transfers multi-modal knowledge from rich-resource pre-training task to limited-resource downstream tasks. Unlike existing works that predominantly learn a single generic encoder, we present a pre-trainable Universal Encoder-DEcoder Network (Uni-EDEN) to facilitate both vision-language perception (e.g., visual question answering) and generation (e.g., image captioning). Uni-EDEN is a two-stream Transformer based structure, consisting of three modules: object and sentence encoders that separately learns the representations of each modality, and sentence decoder that enables both multi-modal reasoning and sentence generation via inter-modal interaction. Considering that the linguistic representations of each image can span different granularities in this hierarchy including, from simple to comprehensive, individual label, a phrase, and a natural sentence, we pre-train Uni-EDEN through multi-granular vision-language proxy tasks: Masked Object Classification (MOC), Masked Region Phrase Generation (MRPG), Image-Sentence Matching (ISM), and Masked Sentence Generation (MSG). In this way, Uni-EDEN is endowed with the power of both multi-modal representation extraction and language modeling. Extensive experiments demonstrate the compelling generalizability of Uni-EDEN by fine-tuning it to four vision-language perception and generation downstream tasks.", "comment": " Comments: ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.03742", "id": "2201.03742", "pdf": "https://arxiv.org/pdf/2201.03742", "other": "https://arxiv.org/format/2201.03742"}, "title": "Explaining Prediction Uncertainty of Pre-trained Language Models by Detecting Uncertain Words in Inputs", "author_info": ["Hanjie Chen", "Yangfeng Ji"], "summary": "Estimating the predictive uncertainty of pre-trained language models is important for increasing their trustworthiness in NLP. Although many previous works focus on quantifying prediction uncertainty, there is little work on explaining the uncertainty. This paper pushes a step further on explaining uncertain predictions of post-calibrated pre-trained language models. We adapt two perturbation-based post-hoc interpretation methods, Leave-one-out and Sampling Shapley, to identify words in inputs that cause the uncertainty in predictions. We test the proposed methods on BERT and RoBERTa with three tasks: sentiment classification, natural language inference, and paraphrase identification, in both in-domain and out-of-domain settings. Experiments show that both methods consistently capture words in inputs that cause prediction uncertainty.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.03382", "id": "2201.03382", "pdf": "https://arxiv.org/pdf/2201.03382", "other": "https://arxiv.org/format/2201.03382"}, "title": "BERT for Sentiment Analysis: Pre-trained and Fine-Tuned Alternatives", "author_info": ["Frederico Souza", "Jo\u00e3o Filho"], "summary": "BERT has revolutionized the NLP field by enabling transfer learning with large language models that can capture complex textual patterns, reaching the state-of-the-art for an expressive number of NLP applications. For text classification tasks, BERT has already been extensively explored. However, aspects like how to better cope with the different embeddings provided by the BERT output layer and the usage of language-specific instead of multilingual models are not well studied in the literature, especially for the Brazilian Portuguese language. The purpose of this article is to conduct an extensive experimental study regarding different strategies for aggregating the features produced in the BERT output layer, with a focus on the sentiment analysis task. The experiments include BERT models trained with Brazilian Portuguese corpora and the multilingual version, contemplating multiple aggregation strategies and open-source datasets with predefined training, validation, and test partitions to facilitate the reproducibility of the results. BERT achieved the highest ROC-AUC values for the majority of cases as compared to TF-IDF. Nonetheless, TF-IDF represents a good trade-off between the predictive performance and computational cost.", "comment": " Comments: 10 pages, 1 figure, 3 tables. Accepted at International Conference on the Computational Processing of Portuguese (PROPOR 2022), but not yet published "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.02772", "id": "2201.02772", "pdf": "https://arxiv.org/pdf/2201.02772", "other": "https://arxiv.org/format/2201.02772"}, "title": "A Comprehensive Empirical Study of Vision-Language Pre-trained Model for Supervised Cross-Modal Retrieval", "author_info": ["Zhixiong Zeng", "Wenji Mao"], "summary": "Cross-Modal Retrieval (CMR) is an important research topic across multimodal computing and information retrieval, which takes one type of data as the query to retrieve relevant data of another type, and has been widely used in many real-world applications. Recently, the vision-language pre-trained model represented by CLIP has demonstrated its superiority of learning visual and textual representations and its impressive performance on various vision and language related tasks. Although CLIP as well as the previous pre-trained models have shown great performance improvement in unsupervised CMR, the performance and impact of these pre-trained models on supervised CMR were rarely explored due to the lack of multimodal class-level associations.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2201.01549", "id": "2201.01549", "pdf": "https://arxiv.org/pdf/2201.01549", "other": "https://arxiv.org/format/2201.01549"}, "title": "SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations", "author_info": ["Changan Niu", "Chuanyi Li", "Vincent Ng", "Jidong Ge", "Liguo Huang", "Bin Luo"], "summary": "Recent years have seen the successful application of large pre-trained models to code representation learning, resulting in substantial improvements on many code-related downstream tasks. But there are issues surrounding their application to SE tasks. First, the majority of the pre-trained models focus on pre-training only the encoder of the Transformer. For generation tasks that are addressed using models with the encoder-decoder architecture, however, there is no reason why the decoder should be left out during pre-training. Second, many existing pre-trained models, including state-of-the-art models such as T5-learning, simply reuse the pre-training tasks designed for natural languages. Moreover, to learn the natural language description of source code needed eventually for code-related tasks such as code summarization, existing pre-training tasks require a bilingual corpus composed of source code and the associated natural language description, which severely limits the amount of data for pre-training. To this end, we propose SPT-Code, a sequence-to-sequence pre-trained model for source code. In order to pre-train SPT-Code in a sequence-to-sequence manner and address the aforementioned weaknesses associated with existing pre-training tasks, we introduce three pre-training tasks that are specifically designed to enable SPT-Code to learn knowledge of source code, the corresponding code structure, as well as a natural language description of the code without relying on any bilingual corpus, and eventually exploit these three sources of information when it is applied to downstream tasks. Experimental results demonstrate that SPT-Code achieves state-of-the-art performance on five code-related downstream tasks after fine-tuning.", "comment": " Comments: Accepted by ICSE 2022, not the final version "}, {"arxiv": {"page": "https://arxiv.org/abs/2201.01426", "id": "2201.01426", "pdf": "https://arxiv.org/pdf/2201.01426", "other": "https://arxiv.org/format/2201.01426"}, "title": "Advancing 3D Medical Image Analysis with Variable Dimension Transform based Supervised 3D Pre-training", "author_info": ["Shu Zhang", "Zihao Li", "Hong-Yu Zhou", "Jiechao Ma", "Yizhou Yu"], "summary": "The difficulties in both data acquisition and annotation substantially restrict the sample sizes of training datasets for 3D medical imaging applications. As a result, constructing high-performance 3D convolutional neural networks from scratch remains a difficult task in the absence of a sufficient pre-training parameter. Previous efforts on 3D pre-training have frequently relied on self-supervised approaches, which use either predictive or contrastive learning on unlabeled data to build invariant 3D representations. However, because of the unavailability of large-scale supervision information, obtaining semantically invariant and discriminative representations from these learning frameworks remains problematic. In this paper, we revisit an innovative yet simple fully-supervised 3D network pre-training framework to take advantage of semantic supervisions from large-scale 2D natural image datasets. With a redesigned 3D network architecture, reformulated natural images are used to address the problem of data scarcity and develop powerful 3D representations. Comprehensive experiments on four benchmark datasets demonstrate that the proposed pre-trained models can effectively accelerate convergence while also improving accuracy for a variety of 3D medical imaging tasks such as classification, segmentation and detection. In addition, as compared to training from scratch, it can save up to 60% of annotation efforts. On the NIH DeepLesion dataset, it likewise achieves state-of-the-art detection performance, outperforming earlier self-supervised and fully-supervised pre-training approaches, as well as methods that do training from scratch. To facilitate further development of 3D medical models, our code and pre-trained model weights are publicly available at https://github.com/urmagicsmine/CSPR.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.15283", "id": "2112.15283", "pdf": "https://arxiv.org/pdf/2112.15283", "other": "https://arxiv.org/format/2112.15283"}, "title": "ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation", "author_info": ["Han Zhang", "Weichong Yin", "Yewei Fang", "Lanxin Li", "Boqiang Duan", "Zhihua Wu", "Yu Sun", "Hao Tian", "Hua Wu", "Haifeng Wang"], "summary": "Conventional methods for the image-text generation tasks mainly tackle the naturally bidirectional generation tasks separately, focusing on designing task-specific frameworks to improve the quality and fidelity of the generated samples. Recently, Vision-Language Pre-training models have greatly improved the performance of the image-to-text generation tasks, but large-scale pre-training models for text-to-image synthesis task are still under-developed. In this paper, we propose ERNIE-ViLG, a unified generative pre-training framework for bidirectional image-text generation with transformer model. Based on the image quantization models, we formulate both image generation and text generation as autoregressive generative tasks conditioned on the text/image input. The bidirectional image-text generative modeling eases the semantic alignments across vision and language. For the text-to-image generation process, we further propose an end-to-end training method to jointly learn the visual sequence generator and the image reconstructor. To explore the landscape of large-scale pre-training for bidirectional text-image generation, we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-the-art performance for both text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for text-to-image synthesis and best results on COCO-CN and AIC-ICC for image captioning.", "comment": " Comments: 15 pages, 7 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.14757", "id": "2112.14757", "pdf": "https://arxiv.org/pdf/2112.14757", "other": "https://arxiv.org/format/2112.14757"}, "title": "A Simple Baseline for Zero-shot Semantic Segmentation with Pre-trained Vision-language Model", "author_info": ["Mengde Xu", "Zheng Zhang", "Fangyun Wei", "Yutong Lin", "Yue Cao", "Han Hu", "Xiang Bai"], "summary": "Recently, zero-shot image classification by vision-language pre-training has demonstrated incredible achievements, that the model can classify arbitrary category without seeing additional annotated images of that category. However, it is still unclear how to make the zero-shot recognition working well on broader vision problems, such as object detection and semantic segmentation. In this paper, we target for zero-shot semantic segmentation, by building it on an off-the-shelf pre-trained vision-language model, i.e., CLIP. It is difficult because semantic segmentation and the CLIP model perform on different visual granularity, that semantic segmentation processes on pixels while CLIP performs on images. To remedy the discrepancy on processing granularity, we refuse the use of the prevalent one-stage FCN based framework, and advocate a two-stage semantic segmentation framework, with the first stage extracting generalizable mask proposals and the second stage leveraging an image based CLIP model to perform zero-shot classification on the masked image crops which are generated in the first stage. Our experimental results show that this simple framework surpasses previous state-of-the-arts by a large margin: +29.5 hIoU on the Pascal VOC 2012 dataset, and +8.9 hIoU on the COCO Stuff dataset. With its simplicity and strong performance, we hope this framework to serve as a baseline to facilitate the future research.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.13800", "id": "2112.13800", "pdf": "https://arxiv.org/pdf/2112.13800", "other": "https://arxiv.org/format/2112.13800"}, "title": "\"A Passage to India\": Pre-trained Word Embeddings for Indian Languages", "author_info": ["Kumar Saurav", "Kumar Saunack", "Diptesh Kanojia", "Pushpak Bhattacharyya"], "summary": "Dense word vectors or 'word embeddings' which encode semantic properties of words, have now become integral to NLP tasks like Machine Translation (MT), Question Answering (QA), Word Sense Disambiguation (WSD), and Information Retrieval (IR). In this paper, we use various existing approaches to create multiple word embeddings for 14 Indian languages. We place these embeddings for all these languages, viz., Assamese, Bengali, Gujarati, Hindi, Kannada, Konkani, Malayalam, Marathi, Nepali, Odiya, Punjabi, Sanskrit, Tamil, and Telugu in a single repository. Relatively newer approaches that emphasize catering to context (BERT, ELMo, etc.) have shown significant improvements, but require a large amount of resources to generate usable models. We release pre-trained embeddings generated using both contextual and non-contextual approaches. We also use MUSE and XLM to train cross-lingual embeddings for all pairs of the aforementioned languages. To show the efficacy of our embeddings, we evaluate our embedding models on XPOS, UPOS and NER tasks for all these languages. We release a total of 436 models using 8 different approaches. We hope they are useful for the resource-constrained Indian language NLP. The title of this paper refers to the famous novel 'A Passage to India' by E.M. Forster, published initially in 1924.", "comment": " Comments: Published at LREC 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.13512", "id": "2112.13512", "pdf": "https://arxiv.org/pdf/2112.13512"}, "title": "Event-based clinical findings extraction from radiology reports with pre-trained language model", "author_info": ["Wilson Lau", "Kevin Lybarger", "Martin L. Gunn", "Meliha Yetisgen"], "summary": "Radiology reports contain a diverse and rich set of clinical abnormalities documented by radiologists during their interpretation of the images. Comprehensive semantic representations of radiological findings would enable a wide range of secondary use applications to support diagnosis, triage, outcomes prediction, and clinical research. In this paper, we present a new corpus of radiology reports annotated with clinical findings. Our annotation schema captures detailed representations of pathologic findings that are observable on imaging (\"lesions\") and other types of clinical problems (\"medical problems\"). The schema used an event-based representation to capture fine-grained details, including assertion, anatomy, characteristics, size, count, etc. Our gold standard corpus contained a total of 500 annotated computed tomography (CT) reports. We extracted triggers and argument entities using two state-of-the-art deep learning architectures, including BERT. We then predicted the linkages between trigger and argument entities (referred to as argument roles) using a BERT-based relation extraction model. We achieved the best extraction performance using a BERT model pre-trained on 3 million radiology reports from our institution: 90.9%-93.4% F1 for finding triggers 72.0%-85.6% F1 for arguments roles. To assess model generalizability, we used an external validation set randomly sampled from the MIMIC Chest X-ray (MIMIC-CXR) database. The extraction performance on this validation set was 95.6% for finding triggers and 79.1%-89.7% for argument roles, demonstrating that the model generalized well to the cross-institutional data with a different imaging modality. We extracted the finding events from all the radiology reports in the MIMIC-CXR database and provided the extractions to the research community.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.13236", "id": "2112.13236", "pdf": "https://arxiv.org/pdf/2112.13236", "other": "https://arxiv.org/format/2112.13236"}, "title": "An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass Malware Classification", "author_info": ["Ferhat Demirk\u0131ran", "Aykut \u00c7ay\u0131r", "U\u011fur \u00dcnal", "Hasan Da\u011f"], "summary": "Classification of malware families is crucial for a comprehensive understanding of how they can infect devices, computers, or systems. Thus, malware identification enables security researchers and incident responders to take precautions against malware and accelerate mitigation. API call sequences made by malware are widely utilized features by machine and deep learning models for malware classification as these sequences represent the behavior of malware. However, traditional machine and deep learning models remain incapable of capturing sequence relationships between API calls. On the other hand, the transformer-based models process sequences as a whole and learn relationships between API calls due to multi-head attention mechanisms and positional embeddings. Our experiments demonstrate that the transformer model with one transformer block layer surpassed the widely used base architecture, LSTM. Moreover, BERT or CANINE, pre-trained transformer models, outperformed in classifying highly imbalanced malware families according to evaluation metrics, F1-score, and AUC score. Furthermore, the proposed bagging-based random transformer forest (RTF), an ensemble of BERT or CANINE, has reached the state-of-the-art evaluation scores on three out of four datasets, particularly state-of-the-art F1-score of 0.6149 on one of the commonly used benchmark dataset.", "comment": " Comments: 34 pages, 7 Figures, 11 Tables "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.12750", "id": "2112.12750", "pdf": "https://arxiv.org/pdf/2112.12750", "other": "https://arxiv.org/format/2112.12750"}, "title": "SLIP: Self-supervision meets Language-Image Pre-training", "author_info": ["Norman Mu", "Alexander Kirillov", "David Wagner", "Saining Xie"], "summary": "Recent work has shown that self-supervised pre-training leads to improvements over supervised learning on challenging visual recognition tasks. CLIP, an exciting new approach to learning with language supervision, demonstrates promising performance on a wide variety of benchmarks. In this work, we explore whether self-supervised learning can aid in the use of language supervision for visual representation learning. We introduce SLIP, a multi-task learning framework for combining self-supervised learning and CLIP pre-training. After pre-training with Vision Transformers, we thoroughly evaluate representation quality and compare performance to both CLIP and self-supervised learning under three distinct settings: zero-shot transfer, linear classification, and end-to-end finetuning. Across ImageNet and a battery of additional datasets, we find that SLIP improves accuracy by a large margin. We validate our results further with experiments on different model sizes, training schedules, and pre-training datasets. Our findings show that SLIP enjoys the best of both worlds: better performance than self-supervision (+8.1% linear accuracy) and language supervision (+5.2% zero-shot accuracy).", "comment": " Comments: Code: https://github.com/facebookresearch/SLIP "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.12731", "id": "2112.12731", "pdf": "https://arxiv.org/pdf/2112.12731", "other": "https://arxiv.org/format/2112.12731"}, "title": "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation", "author_info": ["Shuohuan Wang", "Yu Sun", "Yang Xiang", "Zhihua Wu", "Siyu Ding", "Weibao Gong", "Shikun Feng", "Junyuan Shang", "Yanbin Zhao", "Chao Pang", "Jiaxiang Liu", "Xuyi Chen", "Yuxiang Lu", "Weixin Liu", "Xi Wang", "Yangfan Bai", "Qiuliang Chen", "Li Zhao", "Shiyong Li", "Peng Sun", "Dianhai Yu", "Yanjun Ma", "Hao Tian", "Hua Wu", "Tian Wu"], "summary": "Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.", "comment": " Comments: arXiv admin note: text overlap with arXiv:2107.02137 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.12522", "id": "2112.12522", "pdf": "https://arxiv.org/pdf/2112.12522", "other": "https://arxiv.org/format/2112.12522"}, "title": "Data Augmentation based Consistency Contrastive Pre-training for Automatic Speech Recognition", "author_info": ["Changfeng Gao", "Gaofeng Cheng", "Yifan Guo", "Qingwei Zhao", "Pengyuan Zhang"], "summary": "Self-supervised acoustic pre-training has achieved amazing results on the automatic speech recognition (ASR) task. Most of the successful acoustic pre-training methods use contrastive learning to learn the acoustic representations by distinguish the representations from different time steps, ignoring the speaker and environment robustness. As a result, the pre-trained model could show poor performance when meeting out-of-domain data during fine-tuning. In this letter, we design a novel consistency contrastive learning (CCL) method by utilizing data augmentation for acoustic pre-training. Different kinds of augmentation are applied on the original audios and then the augmented audios are fed into an encoder. The encoder should not only contrast the representations within one audio but also maximize the measurement of the representations across different augmented audios. By this way, the pre-trained model can learn a text-related representation method which is more robust with the change of the speaker or the environment.Experiments show that by applying the CCL method on the Wav2Vec2.0, better results can be realized both on the in-domain data and the out-of-domain data. Especially for noisy out-of-domain data, more than 15% relative improvement can be obtained.", "comment": " Comments: 5 pages, 2 figures "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.12356", "id": "2112.12356", "pdf": "https://arxiv.org/pdf/2112.12356", "other": "https://arxiv.org/format/2112.12356"}, "title": "Do Multi-Lingual Pre-trained Language Models Reveal Consistent Token Attributions in Different Languages?", "author_info": ["Junxiang Wang", "Xuchao Zhang", "Bo Zong", "Yanchi Liu", "Wei Cheng", "Jingchao Ni", "Haifeng Chen", "Liang Zhao"], "summary": "During the past several years, a surge of multi-lingual Pre-trained Language Models (PLMs) has been proposed to achieve state-of-the-art performance in many cross-lingual downstream tasks. However, the understanding of why multi-lingual PLMs perform well is still an open domain. For example, it is unclear whether multi-Lingual PLMs reveal consistent token attributions in different languages. To address this, in this paper, we propose a Cross-lingual Consistency of Token Attributions (CCTA) evaluation framework. Extensive experiments in three downstream tasks demonstrate that multi-lingual PLMs assign significantly different attributions to multi-lingual synonyms. Moreover, we have the following observations: 1) the Spanish achieves the most consistent token attributions in different languages when it is used for training PLMs; 2) the consistency of token attributions strongly correlates with performance in downstream tasks.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.11670", "id": "2112.11670", "pdf": "https://arxiv.org/pdf/2112.11670", "other": "https://arxiv.org/format/2112.11670"}, "title": "Domain Adaptation with Pre-trained Transformers for Query Focused Abstractive Text Summarization", "author_info": ["Md Tahmid Rahman Laskar", "Enamul Hoque", "Jimmy Xiangji Huang"], "summary": "The Query Focused Text Summarization (QFTS) task aims at building systems that generate the summary of the text document(s) based on the given query. A key challenge in addressing this task is the lack of large labeled data for training the summarization model. In this paper, we address this challenge by exploring a series of domain adaptation techniques. Given the recent success of pre-trained transformer models in a wide range of natural language processing tasks, we utilize such models to generate abstractive summaries for the QFTS task for both single-document and multi-document scenarios. For domain adaptation, we apply a variety of techniques using pre-trained transformer-based summarization models including transfer learning, weakly supervised learning, and distant supervision. Extensive experiments on six datasets show that our proposed approach is very effective in generating abstractive summaries for the QFTS task while setting a new state-of-the-art result in several datasets across a set of automatic and human evaluation metrics.", "comment": " Comments: The final version will be published in the Computational Linguistics journal "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.11668", "id": "2112.11668", "pdf": "https://arxiv.org/pdf/2112.11668", "other": "https://arxiv.org/format/2112.11668"}, "title": "How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?", "author_info": ["Xinhsuai Dong", "Luu Anh Tuan", "Min Lin", "Shuicheng Yan", "Hanwang Zhang"], "summary": "The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.", "comment": " Comments: Accepted by NeurIPS-2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.11459", "id": "2112.11459", "pdf": "https://arxiv.org/pdf/2112.11459", "other": "https://arxiv.org/format/2112.11459"}, "title": "Self-Supervised Learning based Monaural Speech Enhancement with Multi-Task Pre-Training", "author_info": ["Yi Li", "Yang Sun", "Syed Mohsen Naqvi"], "summary": "In self-supervised learning, it is challenging to reduce the gap between the enhancement performance on the estimated and target speech signals with existed pre-tasks. In this paper, we propose a multi-task pre-training method to improve the speech enhancement performance with self-supervised learning. Within the pre-training autoencoder (PAE), only a limited set of clean speech signals are required to learn their latent representations. Meanwhile, to solve the limitation of single pre-task, the proposed masking module exploits the dereverberated mask and estimated ratio mask to denoise the mixture as the second pre-task. Different from the PAE, where the target speech signals are estimated, the downstream task autoencoder (DAE) utilizes a large number of unlabeled and unseen reverberant mixtures to generate the estimated mixtures. The trained DAE is shared by the learned representations and masks. Experimental results on a benchmark dataset demonstrate that the proposed method outperforms the state-of-the-art approaches.", "comment": " Comments: Submitted to ICASSP 2022. arXiv admin note: text overlap with arXiv:2112.11142 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.10740", "id": "2112.10740", "pdf": "https://arxiv.org/pdf/2112.10740", "other": "https://arxiv.org/format/2112.10740"}, "title": "Are Large-scale Datasets Necessary for Self-Supervised Pre-training?", "author_info": ["Alaaeldin El-Nouby", "Gautier Izacard", "Hugo Touvron", "Ivan Laptev", "Herv\u00e9 Jegou", "Edouard Grave"], "summary": "Pre-training models on large scale datasets, like ImageNet, is a standard practice in computer vision. This paradigm is especially effective for tasks with small training sets, for which high-capacity models tend to overfit. In this work, we consider a self-supervised pre-training scenario that only leverages the target task data. We consider datasets, like Stanford Cars, Sketch or COCO, which are order(s) of magnitude smaller than Imagenet. Our study shows that denoising autoencoders, such as BEiT or a variant that we introduce in this paper, are more robust to the type and size of the pre-training data than popular self-supervised methods trained by comparing image embeddings.We obtain competitive performance compared to ImageNet pre-training on a variety of classification datasets, from different domains. On COCO, when pre-training solely using COCO images, the detection and instance segmentation performance surpasses the supervised ImageNet pre-training in a comparable setting.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.10175", "id": "2112.10175", "pdf": "https://arxiv.org/pdf/2112.10175", "other": "https://arxiv.org/format/2112.10175"}, "title": "On Efficient Transformer and Image Pre-training for Low-level Vision", "author_info": ["Wenbo Li", "Xin Lu", "Jiangbo Lu", "Xiangyu Zhang", "Jiaya Jia"], "summary": "Pre-training has marked numerous state of the arts in high-level computer vision, but few attempts have ever been made to investigate how pre-training acts in image processing systems. In this paper, we present an in-depth study of image pre-training. To conduct this study on solid ground with practical value in mind, we first propose a generic, cost-effective Transformer-based framework for image processing. It yields highly competitive performance across a range of low-level tasks, though under constrained parameters and computational complexity. Then, based on this framework, we design a whole set of principled evaluation tools to seriously and comprehensively diagnose image pre-training in different tasks, and uncover its effects on internal network representations. We find pre-training plays strikingly different roles in low-level tasks. For example, pre-training introduces more local information to higher layers in super-resolution (SR), yielding significant performance gains, while pre-training hardly affects internal feature representations in denoising, resulting in a little gain. Further, we explore different methods of pre-training, revealing that multi-task pre-training is more effective and data-efficient. All codes and models will be released at https://github.com/fenglinglwb/EDT.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.09965", "id": "2112.09965", "pdf": "https://arxiv.org/pdf/2112.09965", "other": "https://arxiv.org/format/2112.09965"}, "title": "Pre-Training Transformers for Domain Adaptation", "author_info": ["Burhan Ul Tayyab", "Nicholas Chua"], "summary": "The Visual Domain Adaptation Challenge 2021 called for unsupervised domain adaptation methods that could improve the performance of models by transferring the knowledge obtained from source datasets to out-of-distribution target datasets. In this paper, we utilize BeiT [1] and demonstrate its capability of capturing key attributes from source datasets and apply it to target datasets in a semi-supervised manner. Our method was able to outperform current state-of-the-art (SoTA) techniques and was able to achieve 1st place on the ViSDA Domain Adaptation Challenge with ACC of 56.29% and AUROC of 69.79%.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.09583", "id": "2112.09583", "pdf": "https://arxiv.org/pdf/2112.09583", "other": "https://arxiv.org/format/2112.09583"}, "title": "Align and Prompt: Video-and-Language Pre-training with Entity Prompts", "author_info": ["Dongxu Li", "Junnan Li", "Hongdong Li", "Juan Carlos Niebles", "Steven C. H. Hoi"], "summary": "Video-and-language pre-training has shown promising improvements on various downstream tasks. Most previous methods capture cross-modal interactions with a transformer-based multimodal encoder, not fully addressing the misalignment between unimodal video and text features. Besides, learning fine-grained visual-language alignment usually requires off-the-shelf object detectors to provide object information, which is bottlenecked by the detector's limited vocabulary and expensive computation cost.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.09426", "id": "2112.09426", "pdf": "https://arxiv.org/pdf/2112.09426", "other": "https://arxiv.org/format/2112.09426"}, "title": "SiamTrans: Zero-Shot Multi-Frame Image Restoration with Pre-Trained Siamese Transformers", "author_info": ["Lin Liu", "Shanxin Yuan", "Jianzhuang Liu", "Xin Guo", "Youliang Yan", "Qi Tian"], "summary": "We propose a novel zero-shot multi-frame image restoration method for removing unwanted obstruction elements (such as rains, snow, and moire patterns) that vary in successive frames. It has three stages: transformer pre-training, zero-shot restoration, and hard patch refinement. Using the pre-trained transformers, our model is able to tell the motion difference between the true image information and the obstructing elements. For zero-shot image restoration, we design a novel model, termed SiamTrans, which is constructed by Siamese transformers, encoders, and decoders. Each transformer has a temporal attention layer and several self-attention layers, to capture both temporal and spatial information of multiple frames. Only pre-trained (self-supervised) on the denoising task, SiamTrans is tested on three different low-level vision tasks (deraining, demoireing, and desnowing). Compared with related methods, ours achieves the best performances, even outperforming those with supervised learning.", "comment": " Journal ref:         AAAI 2022       "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.09356", "id": "2112.09356", "pdf": "https://arxiv.org/pdf/2112.09356", "other": "https://arxiv.org/format/2112.09356"}, "title": "Unified 2D and 3D Pre-training for Medical Image classification and Segmentation", "author_info": ["Yutong Xie", "Jianpeng Zhang", "Yong Xia", "Qi Wu"], "summary": "Self-supervised learning (SSL) opens up huge opportunities for better utilizing unlabeled data. It is essential for medical image analysis that is generally known for its lack of annotations. However, when we attempt to use as many as possible unlabeled medical images in SSL, breaking the dimension barrier (\\ie, making it possible to jointly use both 2D and 3D images) becomes a must. In this paper, we propose a Universal Self-Supervised Transformer (USST) framework based on the student-teacher paradigm, aiming to leverage a huge of unlabeled medical data with multiple dimensions to learn rich representations. To achieve this, we design a Pyramid Transformer U-Net (PTU) as the backbone, which is composed of switchable patch embedding (SPE) layers and Transformer layers. The SPE layer switches to either 2D or 3D patch embedding depending on the input dimension. After that, the images are converted to a sequence regardless of their original dimensions. The Transformer layer then models the long-term dependencies in a sequence-to-sequence manner, thus enabling USST to learn representations from both 2D and 3D images. USST has two obvious merits compared to current dimension-specific SSL: (1) \\textbf{more effective} - can learn representations from more and diverse data; and (2) \\textbf{more versatile} - can be transferred to various downstream tasks. The results show that USST provides promising results on six 2D/3D medical image classification and segmentation tasks, outperforming the supervised ImageNet pre-training and advanced SSL counterparts substantially.", "comment": " Comments: 13 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.09153", "id": "2112.09153", "pdf": "https://arxiv.org/pdf/2112.09153", "other": "https://arxiv.org/format/2112.09153"}, "title": "An Empirical Investigation of the Role of Pre-training in Lifelong Learning", "author_info": ["Sanket Vaibhav Mehta", "Darshan Patil", "Sarath Chandar", "Emma Strubell"], "summary": "The lifelong learning paradigm in machine learning is an attractive alternative to the more prominent isolated learning scheme not only due to its resemblance to biological learning, but also its potential to reduce energy waste by obviating excessive model re-training. A key challenge to this paradigm is the phenomenon of catastrophic forgetting. With the increasing popularity and success of pre-trained models in machine learning, we pose the question: What role does pre-training play in lifelong learning, specifically with respect to catastrophic forgetting? We investigate existing methods in the context of large, pre-trained models and evaluate their performance on a variety of text and image classification tasks, including a large-scale study using a novel dataset of 15 diverse NLP tasks. Across all settings, we observe that generic pre-training implicitly alleviates the effects of catastrophic forgetting when learning multiple tasks sequentially compared to randomly initialized models. We then further investigate why pre-training alleviates forgetting in this setting. We study this phenomenon by analyzing the loss landscape, finding that pre-trained weights appear to ease forgetting by leading to wider minima. Based on this insight, we propose jointly optimizing for current task loss and loss basin sharpness in order to explicitly encourage wider basins during sequential fine-tuning. We show that this optimization approach leads to performance comparable to the state-of-the-art in task-sequential continual learning across multiple settings, without retaining a memory that scales in size with the number of tasks.", "comment": " Comments: 30 pages "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.09133", "id": "2112.09133", "pdf": "https://arxiv.org/pdf/2112.09133", "other": "https://arxiv.org/format/2112.09133"}, "title": "Masked Feature Prediction for Self-Supervised Visual Pre-Training", "author_info": ["Chen Wei", "Haoqi Fan", "Saining Xie", "Chao-Yuan Wu", "Alan Yuille", "Christoph Feichtenhofer"], "summary": "We present Masked Feature Prediction (MaskFeat) for self-supervised pre-training of video models. Our approach first randomly masks out a portion of the input sequence and then predicts the feature of the masked regions. We study five different types of features and find Histograms of Oriented Gradients (HOG), a hand-crafted feature descriptor, works particularly well in terms of both performance and efficiency. We observe that the local contrast normalization in HOG is essential for good results, which is in line with earlier work using HOG for visual recognition. Our approach can learn abundant visual knowledge and drive large-scale Transformer-based models. Without using extra model weights or supervision, MaskFeat pre-trained on unlabeled videos achieves unprecedented results of 86.7% with MViT-L on Kinetics-400, 88.3% on Kinetics-600, 80.4% on Kinetics-700, 38.8 mAP on AVA, and 75.0% on SSv2. MaskFeat further generalizes to image input, which can be interpreted as a video with a single frame and obtains competitive results on ImageNet.", "comment": " Comments: Technical report "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.08754", "id": "2112.08754", "pdf": "https://arxiv.org/pdf/2112.08754", "other": "https://arxiv.org/format/2112.08754"}, "title": "CLIN-X: pre-trained language models and a study on cross-task transfer for concept extraction in the clinical domain", "author_info": ["Lukas Lange", "Heike Adel", "Jannik Str\u00f6tgen", "Dietrich Klakow"], "summary": "The field of natural language processing (NLP) has recently seen a large change towards using pre-trained language models for solving almost any task. Despite showing great improvements in benchmark datasets for various tasks, these models often perform sub-optimal in non-standard domains like the clinical domain where a large gap between pre-training documents and target documents is observed. In this paper, we aim at closing this gap with domain-specific training of the language model and we investigate its effect on a diverse set of downstream tasks and settings. We introduce the pre-trained CLIN-X (Clinical XLM-R) language models and show how CLIN-X outperforms other pre-trained transformer models by a large margin for ten clinical concept extraction tasks from two languages. In addition, we demonstrate how the transformer model can be further improved with our proposed task- and language-agnostic model architecture based on ensembles over random splits and cross-sentence context. Our studies in low-resource and transfer settings reveal stable model performance despite a lack of annotated data with improvements of up to 47 F1 points when only 250 labeled sentences are available. Our results highlight the importance of specialized language models as CLIN-X for concept extraction in non-standard domains, but also show that our task-agnostic model architecture is robust across the tested tasks and languages so that domain- or task-specific adaptations are not required.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.08692", "id": "2112.08692", "pdf": "https://arxiv.org/pdf/2112.08692", "other": "https://arxiv.org/format/2112.08692"}, "title": "Lacuna Reconstruction: Self-supervised Pre-training for Low-Resource Historical Document Transcription", "author_info": ["Nikolai Vogler", "Jonathan Parkes Allen", "Matthew Thomas Miller", "Taylor Berg-Kirkpatrick"], "summary": "We present a self-supervised pre-training approach for learning rich visual language representations for both handwritten and printed historical document transcription. After supervised fine-tuning of our pre-trained encoder representations for low-resource document transcription on two languages, (1) a heterogeneous set of handwritten Islamicate manuscript images and (2) early modern English printed documents, we show a meaningful improvement in recognition accuracy over the same supervised model trained from scratch with as few as 30 line image transcriptions for training. Our masked language model-style pre-training strategy, where the model is trained to be able to identify the true masked visual representation from distractors sampled from within the same line, encourages learning robust contextualized language representations invariant to scribal writing style and printing noise present across documents.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.08583", "id": "2112.08583", "pdf": "https://arxiv.org/pdf/2112.08583", "other": "https://arxiv.org/format/2112.08583"}, "title": "Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge", "author_info": ["Ian Porada", "Alessandro Sordoni", "Jackie Chi Kit Cheung"], "summary": "Transformer models pre-trained with a masked-language-modeling objective (e.g., BERT) encode commonsense knowledge as evidenced by behavioral probes; however, the extent to which this knowledge is acquired by systematic inference over the semantics of the pre-training corpora is an open question. To answer this question, we selectively inject verbalized knowledge into the minibatches of a BERT model during pre-training and evaluate how well the model generalizes to supported inferences. We find generalization does not improve over the course of pre-training, suggesting that commonsense knowledge is acquired from surface-level, co-occurrence patterns rather than induced, systematic reasoning.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.08414", "id": "2112.08414", "pdf": "https://arxiv.org/pdf/2112.08414", "other": "https://arxiv.org/format/2112.08414"}, "title": "DSGPT: Domain-Specific Generative Pre-Training of Transformers for Text Generation in E-commerce Title and Review Summarization", "author_info": ["Xueying Zhang", "Yunjiang Jiang", "Yue Shang", "Zhaomeng Cheng", "Chi Zhang", "Xiaochuan Fan", "Yun Xiao", "Bo Long"], "summary": "We propose a novel domain-specific generative pre-training (DS-GPT) method for text generation and apply it to the product titleand review summarization problems on E-commerce mobile display.First, we adopt a decoder-only transformer architecture, which fitswell for fine-tuning tasks by combining input and output all to-gether. Second, we demonstrate utilizing only small amount of pre-training data in related domains is powerful. Pre-training a languagemodel from a general corpus such as Wikipedia or the CommonCrawl requires tremendous time and resource commitment, andcan be wasteful if the downstream tasks are limited in variety. OurDSGPT is pre-trained on a limited dataset, the Chinese short textsummarization dataset (LCSTS). Third, our model does not requireproduct-related human-labeled data. For title summarization task,the state of art explicitly uses additional background knowledgein training and predicting stages. In contrast, our model implic-itly captures this knowledge and achieves significant improvementover other methods, after fine-tuning on the public Taobao.comdataset. For review summarization task, we utilize JD.com in-housedataset, and observe similar improvement over standard machinetranslation methods which lack the flexibility of fine-tuning. Ourproposed work can be simply extended to other domains for a widerange of text generation tasks.", "comment": " Journal ref:         SIGIR 2021: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2021, Pages 2146-2150       "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.08227", "id": "2112.08227", "pdf": "https://arxiv.org/pdf/2112.08227", "other": "https://arxiv.org/format/2112.08227"}, "title": "An Experimental Study of the Impact of Pre-training on the Pruning of a Convolutional Neural Network", "author_info": ["Nathan Hubens", "Matei Mancas", "Bernard Gosselin", "Marius Preda", "Titus Zaharia"], "summary": "In recent years, deep neural networks have known a wide success in various application domains. However, they require important computational and memory resources, which severely hinders their deployment, notably on mobile devices or for real-time applications. Neural networks usually involve a large number of parameters, which correspond to the weights of the network. Such parameters, obtained with the help of a training process, are determinant for the performance of the network. However, they are also highly redundant. The pruning methods notably attempt to reduce the size of the parameter set, by identifying and removing the irrelevant weights. In this paper, we examine the impact of the training strategy on the pruning efficiency. Two training modalities are considered and compared: (1) fine-tuned and (2) from scratch. The experimental results obtained on four datasets (CIFAR10, CIFAR100, SVHN and Caltech101) and for two different CNNs (VGG16 and MobileNet) demonstrate that a network that has been pre-trained on a large corpus (e.g. ImageNet) and then fine-tuned on a particular dataset can be pruned much more efficiently (up to 80% of parameter reduction) than the same network trained from scratch.", "comment": " Comments: 7 pages, published at APPIS 2020 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07515", "id": "2112.07515", "pdf": "https://arxiv.org/pdf/2112.07515", "other": "https://arxiv.org/format/2112.07515"}, "title": "CoCo-BERT: Improving Video-Language Pre-training with Contrastive Cross-modal Matching and Denoising", "author_info": ["Jianjie Luo", "Yehao Li", "Yingwei Pan", "Ting Yao", "Hongyang Chao", "Tao Mei"], "summary": "BERT-type structure has led to the revolution of vision-language pre-training and the achievement of state-of-the-art results on numerous vision-language downstream tasks. Existing solutions dominantly capitalize on the multi-modal inputs with mask tokens to trigger mask-based proxy pre-training tasks (e.g., masked language modeling and masked object/frame prediction). In this work, we argue that such masked inputs would inevitably introduce noise for cross-modal matching proxy task, and thus leave the inherent vision-language association under-explored. As an alternative, we derive a particular form of cross-modal proxy objective for video-language pre-training, i.e., Contrastive Cross-modal matching and denoising (CoCo). By viewing the masked frame/word sequences as the noisy augmentation of primary unmasked ones, CoCo strengthens video-language association by simultaneously pursuing inter-modal matching and intra-modal denoising between masked and unmasked inputs in a contrastive manner. Our CoCo proxy objective can be further integrated into any BERT-type encoder-decoder structure for video-language pre-training, named as Contrastive Cross-modal BERT (CoCo-BERT). We pre-train CoCo-BERT on TV dataset and a newly collected large-scale GIF video dataset (ACTION). Through extensive experiments over a wide range of downstream tasks (e.g., cross-modal retrieval, video question answering, and video captioning), we demonstrate the superiority of CoCo-BERT as a pre-trained structure.", "comment": " Comments: ACM Multimedia 2021 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07327", "id": "2112.07327", "pdf": "https://arxiv.org/pdf/2112.07327", "other": "https://arxiv.org/format/2112.07327"}, "title": "Model Uncertainty-Aware Knowledge Amalgamation for Pre-Trained Language Models", "author_info": ["Lei Li", "Yankai Lin", "Xuancheng Ren", "Guangxiang Zhao", "Peng Li", "Jie Zhou", "Xu Sun"], "summary": "As many fine-tuned pre-trained language models~(PLMs) with promising performance are generously released, investigating better ways to reuse these models is vital as it can greatly reduce the retraining computational cost and the potential environmental side-effects. In this paper, we explore a novel model reuse paradigm, Knowledge Amalgamation~(KA) for PLMs. Without human annotations available, KA aims to merge the knowledge from different teacher-PLMs, each of which specializes in a different classification problem, into a versatile student model. The achieve this, we design a Model Uncertainty--aware Knowledge Amalgamation~(MUKA) framework, which identifies the potential adequate teacher using Monte-Carlo Dropout for approximating the golden supervision to guide the student. Experimental results demonstrate that MUKA achieves substantial improvements over baselines on benchmark datasets. Further analysis shows that MUKA can generalize well under several complicate settings with multiple teacher models, heterogeneous teachers, and even cross-dataset teachers.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07198", "id": "2112.07198", "pdf": "https://arxiv.org/pdf/2112.07198", "other": "https://arxiv.org/format/2112.07198"}, "title": "From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression", "author_info": ["Runxin Xu", "Fuli Luo", "Chengyu Wang", "Baobao Chang", "Jun Huang", "Songfang Huang", "Fei Huang"], "summary": "Pre-trained Language Models (PLMs) have achieved great success in various Natural Language Processing (NLP) tasks under the pre-training and fine-tuning paradigm. With large quantities of parameters, PLMs are computation-intensive and resource-hungry. Hence, model pruning has been introduced to compress large-scale PLMs. However, most prior approaches only consider task-specific knowledge towards downstream tasks, but ignore the essential task-agnostic knowledge during pruning, which may cause catastrophic forgetting problem and lead to poor generalization ability. To maintain both task-agnostic and task-specific knowledge in our pruned model, we propose ContrAstive Pruning (CAP) under the paradigm of pre-training and fine-tuning. It is designed as a general framework, compatible with both structured and unstructured pruning. Unified in contrastive learning, CAP enables the pruned model to learn from the pre-trained model for task-agnostic knowledge, and fine-tuned model for task-specific knowledge. Besides, to better retain the performance of the pruned model, the snapshots (i.e., the intermediate models at each pruning iteration) also serve as effective supervisions for pruning. Our extensive experiments show that adopting CAP consistently yields significant improvements, especially in extremely high sparsity scenarios. With only 3% model parameters reserved (i.e., 97% sparsity), CAP successfully achieves 99.2% and 96.3% of the original BERT performance in QQP and MNLI tasks. In addition, our probing experiments demonstrate that the model pruned by CAP tends to achieve better generalization ability.", "comment": " Comments: Accepted to AAAI 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07191", "id": "2112.07191", "pdf": "https://arxiv.org/pdf/2112.07191", "other": "https://arxiv.org/format/2112.07191"}, "title": "An Adaptive Graph Pre-training Framework for Localized Collaborative Filtering", "author_info": ["Yiqi Wang", "Chaozhuo Li", "Zheng Liu", "Mingzheng Li", "Jiliang Tang", "Xing Xie", "Lei Chen", "Philip S. Yu"], "summary": "Graph neural networks (GNNs) have been widely applied in the recommendation tasks and have obtained very appealing performance. However, most GNN-based recommendation methods suffer from the problem of data sparsity in practice. Meanwhile, pre-training techniques have achieved great success in mitigating data sparsity in various domains such as natural language processing (NLP) and computer vision (CV). Thus, graph pre-training has the great potential to alleviate data sparsity in GNN-based recommendations. However, pre-training GNNs for recommendations face unique challenges. For example, user-item interaction graphs in different recommendation tasks have distinct sets of users and items, and they often present different properties. Therefore, the successful mechanisms commonly used in NLP and CV to transfer knowledge from pre-training tasks to downstream tasks such as sharing learned embeddings or feature extractors are not directly applicable to existing GNN-based recommendations models. To tackle these challenges, we delicately design an adaptive graph pre-training framework for localized collaborative filtering (ADAPT). It does not require transferring user/item embeddings, and is able to capture both the common knowledge across different graphs and the uniqueness for each graph. Extensive experimental results have demonstrated the effectiveness and superiority of ADAPT.", "comment": ""}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07165", "id": "2112.07165", "pdf": "https://arxiv.org/pdf/2112.07165", "other": "https://arxiv.org/format/2112.07165"}, "title": "Discovering Explanatory Sentences in Legal Case Decisions Using Pre-trained Language Models", "author_info": ["Jaromir Savelka", "Kevin D. Ashley"], "summary": "Legal texts routinely use concepts that are difficult to understand. Lawyers elaborate on the meaning of such concepts by, among other things, carefully investigating how have they been used in past. Finding text snippets that mention a particular concept in a useful way is tedious, time-consuming, and, hence, expensive. We assembled a data set of 26,959 sentences, coming from legal case decisions, and labeled them in terms of their usefulness for explaining selected legal concepts. Using the dataset we study the effectiveness of transformer-based models pre-trained on large language corpora to detect which of the sentences are useful. In light of models' predictions, we analyze various linguistic properties of the explanatory sentences as well as their relationship to the legal concept that needs to be explained. We show that the transformer-based models are capable of learning surprisingly sophisticated features and outperform the prior approaches to the task.", "comment": " Journal ref:         Savelka, Jaromir, and Kevin D. Ashley. \"Discovering Explanatory Sentences in Legal Case Decisions Using Pre-trained Language Models.\" In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 4273-4283. 2021       "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.07074", "id": "2112.07074", "pdf": "https://arxiv.org/pdf/2112.07074", "other": "https://arxiv.org/format/2112.07074"}, "title": "Towards a Unified Foundation Model: Jointly Pre-Training Transformers on Unpaired Images and Text", "author_info": ["Qing Li", "Boqing Gong", "Yin Cui", "Dan Kondratyuk", "Xianzhi Du", "Ming-Hsuan Yang", "Matthew Brown"], "summary": "In this paper, we explore the possibility of building a unified foundation model that can be adapted to both vision-only and text-only tasks. Starting from BERT and ViT, we design a unified transformer consisting of modality-specific tokenizers, a shared transformer encoder, and task-specific output heads. To efficiently pre-train the proposed model jointly on unpaired images and text, we propose two novel techniques: (i) We employ the separately-trained BERT and ViT models as teachers and apply knowledge distillation to provide additional, accurate supervision signals for the joint training; (ii) We propose a novel gradient masking strategy to balance the parameter updates from the image and text pre-training losses. We evaluate the jointly pre-trained transformer by fine-tuning it on image classification tasks and natural language understanding tasks, respectively. The experiments show that the resultant unified foundation transformer works surprisingly well on both the vision-only and text-only tasks, and the proposed knowledge distillation and gradient masking strategy can effectively lift the performance to approach the level of separately-trained models.", "comment": " Comments: preliminary work "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.06637", "id": "2112.06637", "pdf": "https://arxiv.org/pdf/2112.06637", "other": "https://arxiv.org/format/2112.06637"}, "title": "Efficient Training of Volterra Series-Based Pre-distortion Filter Using Neural Networks", "author_info": ["Vinod Bajaj", "Mathieu Chagnon", "Sander Wahls", "Vahid Aref"], "summary": "We present a simple, efficient \"direct learning\" approach to train Volterra series-based digital pre-distortion filters using neural networks. We show its superior performance over conventional training methods using a 64-QAM 64-GBaud simulated transmitter with varying transmitter nonlinearity and noisy conditions.", "comment": " Comments: Accepted for presentation in OFC 2022 "}, {"arxiv": {"page": "https://arxiv.org/abs/2112.06147", "id": "2112.06147", "pdf": "https://arxiv.org/pdf/2112.06147", "other": "https://arxiv.org/format/2112.06147"}, "title": "Self-Supervised Modality-Aware Multiple Granularity Pre-Training for RGB-Infrared Person Re-Identification", "author_info": ["Lin Wan", "Qianyan Jing", "Zongyuan Sun", "Chuang Zhang", "Zhihang Li", "Yehansen Chen"], "summary": "While RGB-Infrared cross-modality person re-identification (RGB-IR ReID) has enabled great progress in 24-hour intelligent surveillance, state-of-the-arts still heavily rely on fine-tuning ImageNet pre-trained networks. Due to the single-modality nature, such large-scale pre-training may yield RGB-biased representations that hinder the performance of cross-modality image retrieval. This paper presents a self-supervised pre-training alternative, named Modality-Aware Multiple Granularity Learning (MMGL), which directly trains models from scratch on multi-modality ReID datasets, but achieving competitive results without external data and sophisticated tuning tricks. Specifically, MMGL globally maps shuffled RGB-IR images into a shared latent permutation space and further improves local discriminability by maximizing agreement between cycle-consistent RGB-IR image patches. Experiments demonstrate that MMGL learns better representations (+6.47% Rank-1) with faster training speed (converge in few hours) and solider data efficiency (<5% data size) than ImageNet pre-training. The results also suggest it generalizes well to various existing models, losses and has promising transferability across datasets. The code will be released.", "comment": " Comments: 7 pages, 2 figures "}]}